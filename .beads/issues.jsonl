{"id":"parquedb-02r","title":"[GREEN] Bloom filter implementation","description":"Implement bloom filters to pass tests","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:47.278863-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:24:19.421114-06:00","closed_at":"2026-02-01T14:24:19.421114-06:00","close_reason":"Closed"}
{"id":"parquedb-05ul","title":"Extract duplicate utility functions to shared module","description":"Code duplication found across multiple files:\n- deepEqual() in filter.ts, predicate.ts, update.ts\n- compareValues() in filter.ts, predicate.ts, update.ts, Collection.ts\n- getNestedValue() in filter.ts, predicate.ts, Collection.ts\n- deepClone() in Collection.ts\n\nCreate /src/utils/comparison.ts to centralize these. Update all imports.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T12:42:09.794245-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T12:54:07.325887-06:00","closed_at":"2026-02-01T12:54:07.325887-06:00","close_reason":"Completed by parallel agents"}
{"id":"parquedb-06e7","title":"Refactor: Worker - Use ParqueDBDOStub type consistently for DO RPC calls","description":"The codebase defines a typed ParqueDBDOStub interface in src/types/worker.ts, but several DO RPC calls use `as unknown as ParqueDBDOStub` casting pattern instead of proper typing.\n\nFiles:\n- src/worker/index.ts (multiple occurrences in create, update, delete, etc.)\n\nAll DO stub accesses use this pattern:\n```typescript\nconst stub = this.env.PARQUEDB.get(doId) as unknown as ParqueDBDOStub\n```\n\nConsider:\n1. Create a typed wrapper function like `getDOStub(ns: string): ParqueDBDOStub`\n2. Or improve the Cloudflare Workers type definitions\n\nImpact: Cleaner code, single point of change for DO access","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:33:18.24622-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:25:01.590959-06:00","closed_at":"2026-02-03T11:25:01.590959-06:00","close_reason":"Closed"}
{"id":"parquedb-073b","title":"Add CLI plugin/command registry system","description":"CLI commands are hardcoded in switch statement. Add a command registry pattern for easier extension: CommandRegistry.register('backup', backupCommand). Allow custom commands via plugins.","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T17:01:05.127307-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T04:43:33.879368-06:00","closed_at":"2026-02-02T04:43:33.879368-06:00","close_reason":"Completed by agents"}
{"id":"parquedb-07a4","title":"Vector: Implement hybrid filtering (vector + metadata)","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:41.673718-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:51:54.556964-06:00","closed_at":"2026-02-03T08:51:54.556964-06:00","close_reason":"Closed"}
{"id":"parquedb-07d3","title":"Backend format comparison benchmarks (Iceberg vs Delta vs Native)","description":"Create benchmark suite comparing the three entity backend formats on deployed workers:\n- Proprietary Parquet format\n- Iceberg backend  \n- DeltaLake backend\n\nMeasure for each:\n- Write 1000 entities\n- Read with filter (indexed)\n- Time-travel query (t-1 hour)\n- Compact/vacuum operations\n- Schema evolution overhead\n\nMust run on actual deployed workers to get realistic R2 latency.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:22:38.64821-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:34:31.143721-06:00","closed_at":"2026-02-03T10:34:31.143721-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-07d3","depends_on_id":"parquedb-4srx","type":"blocks","created_at":"2026-02-03T09:22:46.60278-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-07e","title":"[GREEN] Delete operation implementation","description":"Implement delete operations to pass tests","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:36.046455-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:06:19.220126-06:00","closed_at":"2026-02-01T14:06:19.220126-06:00","close_reason":"Closed"}
{"id":"parquedb-08w1","title":"Implement NativeBackend (documented but missing)","description":"Native backend is documented in architecture docs but throws 'not yet implemented':\n\n```typescript\n// src/backends/index.ts:141-142\ncase 'native':\n  throw new Error('Native backend not yet implemented')\n```\n\nImplement NativeBackend extending BaseEntityBackend:\n- Simple Parquet layout: data/{ns}/data.parquet + rels/*\n- No Iceberg/Delta overhead for simple use cases\n- Add migration tool from Native to Iceberg\n- Benchmark vs Iceberg to document when to use each","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:31:56.462971-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:09:42.002004-06:00","closed_at":"2026-02-03T10:09:42.002004-06:00","close_reason":"Closed"}
{"id":"parquedb-09qp","title":"Use binary search for SST unique constraint check","description":"SSTIndex.ts:285-300 uses linear O(n) scan for unique constraint check instead of using the index's sorted nature. Should use lowerBound/upperBound binary search for O(log n).","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T05:32:34.091559-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T06:31:20.035884-06:00","closed_at":"2026-02-02T06:31:20.035884-06:00","close_reason":"Closed"}
{"id":"parquedb-0a8q","title":"DO Write Bottleneck Documentation","description":"## Problem\n~30 req/sec limit per namespace is not prominently documented.\n\n## Fix\nAdd prominent warning in deployment docs with sharding guide explaining:\n- The Durable Object write throughput limitation\n- How to shard namespaces for higher throughput\n- Best practices for write-heavy workloads\n\n## Priority\nP1 - HIGH","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:10:35.251339-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T18:22:30.704018-06:00","closed_at":"2026-02-03T18:22:30.704018-06:00","close_reason":"Fixed in commit ab28519"}
{"id":"parquedb-0axe","title":"Add error path and recovery tests","description":"Error paths under-tested:\n- Corrupted Parquet files\n- Invalid index data\n- Malformed events\n- Out of memory handling\n- Large file handling\n- Concurrent request limits\n- Timeout handling\n- Partial failures\n\nAdd tests for error recovery scenarios.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T12:42:24.690643-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:07:24.720102-06:00","closed_at":"2026-02-01T13:07:24.720102-06:00","close_reason":"Completed by parallel agents"}
{"id":"parquedb-0bpu","title":"Comments That Should Be Code in iceberg.ts","description":"**File:** src/backends/iceberg.ts (lines 533-540)\n\n**Issue:** Commented-out code and TODO-style comments that should be implemented or tracked\n\n**Fix:** \n1. Implement the commented functionality\n2. Create tracked issues for TODOs\n3. Remove dead/commented code\n\n**Impact:** Cleaner codebase, no hidden technical debt","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:10:40.680249-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T18:22:33.155561-06:00","closed_at":"2026-02-03T18:22:33.155561-06:00","close_reason":"Fixed in commit ab28519"}
{"id":"parquedb-0d8o","title":"Add unit tests for RemoteBackend","description":"RemoteBackend lacks tests. Need tests for: range request handling, error mapping (404, 401, 403), cache behavior, timeout handling, token authentication.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:05:26.144669-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:08:43.771458-06:00","closed_at":"2026-02-03T08:08:43.771458-06:00","close_reason":"Closed"}
{"id":"parquedb-0grh","title":"Refactor: Worker - CdnR2StorageAdapter duplicates StorageBackend interface","description":"CdnR2StorageAdapter in QueryExecutor.ts implements a partial StorageBackend interface inline, with stub methods that throw 'Not implemented':\n\n```typescript\nasync write(): Promise\u003cnever\u003e { throw new Error('Not implemented') }\nasync writeAtomic(): Promise\u003cnever\u003e { throw new Error('Not implemented') }\n// ... 6 more stub methods\n```\n\nSimilarly, IndexCache has MemoryStorageAdapter with full implementation.\n\nFiles:\n- src/worker/QueryExecutor.ts (CdnR2StorageAdapter, lines 246-429)\n- src/worker/IndexCache.ts (MemoryStorageAdapter, lines 396-458)\n\nBoth implement the same interface differently. This could be consolidated:\n1. Create a ReadOnlyStorageAdapter base class\n2. Or use Partial\u003cStorageBackend\u003e type properly\n3. Or extract shared logic to a common module\n\nImpact: Reduced code duplication, clearer contracts","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:34:43.054215-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:54:55.520397-06:00","closed_at":"2026-02-03T10:54:55.520397-06:00","close_reason":"Refactored CdnR2StorageAdapter to implement ReadonlyStorageBackend interface. Created new ReadonlyStorageBackend interface in src/types/storage.ts with read-only methods (read, readRange, exists, stat, list). StorageBackend now extends ReadonlyStorageBackend. Removed duplicate stub write methods from CdnR2StorageAdapter. Added type guards isReadonlyBackend() and isWritableBackend(). Added asReadonlyStorageBackend() cast helper."}
{"id":"parquedb-0gy","title":"[RED] Unique index tests","description":"Write failing tests for unique constraint validation","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:52:02.00521-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:32:37.78447-06:00","closed_at":"2026-02-01T14:32:37.78447-06:00","close_reason":"RED phase complete: Created unique index test suite with 23 tests"}
{"id":"parquedb-0jhn","title":"Refactor: core.ts update() - Extract update operators into separate functions","description":"In src/ParqueDB/core.ts, the update() method (lines 685-1155) is excessively long at ~470 lines. It handles all update operators inline ($set, $unset, $inc, $mul, $min, $max, $push, $pull, $addToSet, $currentDate, $link, $unlink).\n\nEach operator's logic should be extracted into separate pure functions in a dedicated module (e.g., src/mutation/operators.ts):\n- applySetOperator(entity, setOps)\n- applyIncOperator(entity, incOps)\n- applyPushOperator(entity, pushOps)\n- applyLinkOperator(entity, linkOps, schema, entities)\n- etc.\n\nBenefits:\n1. Reduces cognitive load when reading the code\n2. Makes individual operators testable in isolation\n3. Enables code reuse if operators are needed elsewhere\n4. Follows single responsibility principle\n\nThe method should orchestrate operator application rather than implement each one.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:32:41.919518-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T06:55:48.020436-06:00","closed_at":"2026-02-03T06:55:48.020436-06:00","close_reason":"Implemented by agents","labels":["complexity","refactor"]}
{"id":"parquedb-0lu","title":"[GREEN] Sorting implementation","description":"Implement sorting to pass tests","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:29.251572-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:11:19.031765-06:00","closed_at":"2026-02-01T14:11:19.031765-06:00","close_reason":"Closed"}
{"id":"parquedb-0nr","title":"[RED] Relationship storage tests","description":"Write failing tests for writing relationships to forward and reverse indexes","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:22.955863-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:08:29.179944-06:00","closed_at":"2026-02-01T14:08:29.179944-06:00","close_reason":"Closed"}
{"id":"parquedb-0ol","title":"[RED] vitest-pool-workers with real bindings","description":"Configure vitest-pool-workers with remote:true. Real R2, DO, service bindings. No mocks.","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T14:30:01.09188-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-30T14:35:40.046008-06:00","closed_at":"2026-01-30T14:35:40.046008-06:00","close_reason":"Closed"}
{"id":"parquedb-0pir","title":"Fix evalite-mv integration tests","description":"tests/unit/integrations/evalite-mv.test.ts has 27 failing tests for EvaliteMVIntegration including initialization, built-in views, run_statistics, suite_performance, scorer_analysis, score_trends, token_usage, failure_patterns, custom views, query methods, state management, retention policy, adapter integration, and edge cases.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:47:34.279577-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:49:51.265077-06:00","closed_at":"2026-02-03T12:49:51.265077-06:00","close_reason":"Closed"}
{"id":"parquedb-0rc4","title":"Defer window removal until workflow completion confirmed","description":"In compaction-queue-consumer.ts, windows are removed from DO tracking before workflow completion is confirmed (line 467). If workflow creation fails, the window is lost with no retry. Defer removal until workflow reports success.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:56:20.370436-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:59:40.156556-06:00","closed_at":"2026-02-03T12:59:40.156556-06:00","close_reason":"Closed"}
{"id":"parquedb-0wut","title":"Implement vacuum() file deletion for Iceberg tables","description":"The IcebergBackend.vacuum() method currently only tracks expired snapshots but does not delete orphaned files. Implement: write updated metadata with expired snapshots removed, identify orphaned data files no longer referenced, delete orphaned files from storage.","status":"open","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T18:03:23.168129-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T18:03:23.168129-06:00"}
{"id":"parquedb-0x16","title":"Expose MV query optimizer to query planner","description":"Materialized View optimizer exists but isn't exposed to the query planner.\n\nThe find() method should automatically:\n- Detect when an MV can satisfy the query\n- Route to MV when fresher than staleness threshold\n- Fall back to base tables when MV is stale\n- Log MV selection decisions for debugging\n\nRelated to existing issue parquedb-4l5g.9","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:32:15.062775-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:12:18.921381-06:00","closed_at":"2026-02-03T10:12:18.921381-06:00","close_reason":"Closed"}
{"id":"parquedb-1237","title":"Refactor: core.ts hydrateEntity() and getRelated() - Consolidate reverse relationship traversal","description":"In src/ParqueDB/core.ts, reverse relationship traversal logic is duplicated in:\n\n1. hydrateEntity() lines 1562-1581:\n```typescript\nthis.entities.forEach((relatedEntity, relatedId) =\u003e {\n  if (!relatedId.startsWith(`${relatedNs}/`)) return\n  if (relatedEntity.deletedAt) return\n  const refField = (relatedEntity as Record\u003cstring, unknown\u003e)[relatedField]\n  if (refField \u0026\u0026 typeof refField === 'object') {\n    for (const [, refId] of Object.entries(refField)) {\n      if (refId === fullId) { /* collect */ }\n    }\n  }\n})\n```\n\n2. getRelated() lines 541-556: Nearly identical logic\n\nThis should be extracted to a shared utility:\n```typescript\ninterface ReverseRelationQuery {\n  targetId: string\n  relatedNs: string\n  relatedField: string\n  includeDeleted?: boolean\n}\nfunction findReverseRelations(\n  entities: Map\u003cstring, Entity\u003e,\n  query: ReverseRelationQuery\n): Entity[]\n```\n\nBenefits:\n1. Single implementation to maintain\n2. Consistent behavior across hydration and getRelated\n3. Could be optimized with an index in the future","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:33:52.503016-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:11:25.621018-06:00","closed_at":"2026-02-03T11:11:25.621018-06:00","close_reason":"Closed","labels":["duplication","refactor"]}
{"id":"parquedb-16qd","title":"Add tests for untested storage backends","description":"Missing test coverage for: DOSqliteBackend.ts, ObservedBackend.ts. These are critical storage components that need unit tests.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T05:32:41.14438-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T06:34:50.118264-06:00","closed_at":"2026-02-02T06:34:50.118264-06:00","close_reason":"Closed"}
{"id":"parquedb-18b","title":"[GREEN] Collection class implementation","description":"Implement Collection class to pass tests","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:45.589587-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:02:25.577545-06:00","closed_at":"2026-02-01T14:02:25.577545-06:00","close_reason":"Closed"}
{"id":"parquedb-18me","title":"Vector: Add Vercel AI SDK embedding wrapper for Node.js","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:40.950196-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:48:53.811013-06:00","closed_at":"2026-02-03T08:48:53.811013-06:00","close_reason":"Closed"}
{"id":"parquedb-18nz","title":"Create /small and /tiny exports for ParqueDB","description":"Create tiered exports optimized for different bundle size requirements:\n\n## Proposed Exports\n\n```typescript\n// Full version (current)\nimport { ParqueDB } from 'parquedb'\n\n// Small version (~50KB) - core CRUD, no indexes, no MVs\nimport { ParqueDB } from 'parquedb/small'\n\n// Tiny version (~15KB) - read-only, query only\nimport { parquetQuery } from 'parquedb/tiny'\n```\n\n## /small Scope (~50KB target)\n**Keep:**\n- Core CRUD operations (create, find, get, update, delete)\n- Basic filters (comparison, logical)\n- NativeBackend only (no Iceberg/Delta)\n- MemoryBackend for testing\n- Basic relationships\n\n**Cut:**\n- Materialized views\n- Full-text search indexes\n- Vector indexes\n- Bloom filters\n- Time-travel\n- Schema evolution\n- Iceberg/Delta backends\n- RPC client\n- Observability hooks\n\n## /tiny Scope (~15KB target)\n**Keep:**\n- Read-only queries\n- parquetQuery(buffer, filter, projection)\n- Basic filter evaluation\n- Direct ArrayBuffer input\n- Streaming row iteration\n\n**Cut:**\n- All write operations\n- All backends (just raw buffer)\n- Relationships\n- Any storage abstraction\n\n## Implementation\n1. Create src/exports/small.ts with selective re-exports\n2. Create src/exports/tiny.ts with minimal query-only code\n3. Update package.json exports field\n4. Add build targets for each tier\n5. Test bundle sizes with esbuild/rollup","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:29:07.698103-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:34:19.578036-06:00","closed_at":"2026-02-03T10:34:19.578036-06:00","close_reason":"Closed"}
{"id":"parquedb-1c3","title":"Implement R2 segment writer for events","description":"Write event batches as Parquet segments to R2:\n\n- Write events as Parquet file to events/seg-{seq}.parquet\n- Use for bulk operations or when SQLite batching isn't optimal\n- Segment naming with monotonic sequence\n\nFile: src/events/segment.ts","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T06:37:53.915852-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T07:08:19.518679-06:00","closed_at":"2026-02-01T07:08:19.518679-06:00","close_reason":"Implemented SegmentWriter for writing event batches to R2 as segments. Features: JSON-lines serialization (Parquet TODO), sequence numbering, batch merging, R2 adapter. Added 18 passing tests.","dependencies":[{"issue_id":"parquedb-1c3","depends_on_id":"parquedb-zqk","type":"blocks","created_at":"2026-02-01T06:38:08.241877-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-1c4","title":"[GREEN] Vector index implementation","description":"Implement vector indexes to pass tests","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:59.753193-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:30:37.82127-06:00","closed_at":"2026-02-01T14:30:37.82127-06:00","close_reason":"Implemented HNSW vector index in src/indexes/vector/ with full support for cosine, euclidean, and dot product distance metrics. All 29 tests pass."}
{"id":"parquedb-1cqi","title":"Add Backend Capability Introspection: Runtime capability checking","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:57:42.749543-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:59:47.420831-06:00","closed_at":"2026-02-03T10:59:47.420831-06:00","close_reason":"Closed"}
{"id":"parquedb-1ejb","title":"Track fetch subrequests in tail events","description":"Ensure tail events capture fetch subrequest count for Snippets compliance.\n\n## Snippets Limits\n- 5ms CPU\n- 5 subrequests\n- 32KB memory\n\n## What Tail Captures\nThe diagnosticsChannelEvents should include fetch events. We need to:\n\n1. **Verify tail captures subrequests**\n   - Check if fetch events appear in trace\n   - If not, may need to enable via wrangler config\n\n2. **Add subrequest counting in WorkerLogsMV**\n   - Extract fetch count from trace events\n   - Store as column in Parquet\n\n3. **Log in search worker**\n   - console.log fetch count for visibility\n   - Tail captures console.log too\n\n## Goal\nBe able to query tail logs and see:\n- cpuTime (ms)\n- fetchCount (number)\n- outcome (ok/exceededCpu/etc)\n\nFor each request to the search worker.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:11:18.423955-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:22:04.48676-06:00","closed_at":"2026-02-03T11:22:04.48676-06:00","close_reason":"Closed"}
{"id":"parquedb-1gj","title":"Implement sorting","description":"Sort query results by multiple fields","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:26.901849-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:11:18.954003-06:00","closed_at":"2026-02-01T14:11:18.954003-06:00","close_reason":"Closed"}
{"id":"parquedb-1iss","title":"Add Alerting Integration for Compaction Events","description":"No webhook/Slack/PagerDuty integration for critical compaction events. Create src/observability/compaction/alerts.ts with:\n- Webhook notification support\n- Slack integration\n- PagerDuty integration\n- Configurable alert thresholds\n- Critical event types: stuck jobs, failures, capacity warnings","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T14:34:26.555602-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:42:21.47033-06:00","closed_at":"2026-02-03T14:42:21.47033-06:00","close_reason":"Closed"}
{"id":"parquedb-1j5c","title":"Add schema evolution tests","description":"No dedicated schema evolution tests exist.\n\nCreate tests for:\n- Adding new columns (nullable)\n- Adding new columns with defaults\n- Removing columns\n- Renaming columns\n- Type changes (compatible: int32-\u003eint64)\n- Type changes (incompatible: string-\u003eint)\n- Schema evolution with existing data\n- Cross-backend evolution (Iceberg, Delta)","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:32:02.406373-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:19:37.654498-06:00","closed_at":"2026-02-03T10:19:37.654498-06:00","close_reason":"Closed"}
{"id":"parquedb-1kq2","title":"Add subscriptions module tests","description":"src/subscriptions/ has 5 files but only 3 test files. Expand coverage for real-time subscription features.","status":"open","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:37.00333-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:37.00333-06:00"}
{"id":"parquedb-1m38","title":"Add cleanup for stale R2 multipart uploads","description":"R2Backend.activeUploads Map stores multipart uploads but only clears on success/abort. Failed clients leave uploads in memory indefinitely. Add timeout/TTL mechanism to clean up stale uploads.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T05:32:32.20722-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T06:33:51.416442-06:00","closed_at":"2026-02-02T06:33:51.416442-06:00","close_reason":"Closed"}
{"id":"parquedb-1mc5","title":"Integration: Expose ParqueDB as MCP server for AI agents","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:39.066346-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:52:06.776729-06:00","closed_at":"2026-02-03T08:52:06.776729-06:00","close_reason":"Closed"}
{"id":"parquedb-1t78","title":"Add npm badges to README","description":"README has no badges. Add: npm version, build status, test coverage, license, TypeScript badge. Use shields.io format.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T17:01:11.319238-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T17:18:33.800543-06:00","closed_at":"2026-02-01T17:18:33.800543-06:00","close_reason":"Closed"}
{"id":"parquedb-1vni","title":"Studio: Make UI mobile responsive","status":"closed","priority":3,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:38.206812-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:57:42.277131-06:00","closed_at":"2026-02-03T16:57:42.277131-06:00","close_reason":"Closed"}
{"id":"parquedb-1ys","title":"[RED] Event logging tests","description":"Write failing tests for event creation and storage","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:35.63593-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:21:45.00322-06:00","closed_at":"2026-02-01T14:21:45.00322-06:00","close_reason":"Closed"}
{"id":"parquedb-20m8","title":"High: TailDO rawEventsBuffer can grow unbounded","description":"In src/worker/TailDO.ts, the rawEventsBuffer (line 158) can grow unbounded if:\n1. Events arrive slowly (below batch threshold)\n2. No alarm is scheduled to force periodic flush\n3. Connections stay open for extended periods\n\nWhile maybeFlushRawEvents() checks batch size, there's no time-based flush unless the alarm() method is explicitly triggered. The alarm is only set after a connection closes and there are more connections (line 454-459).\n\nScenario: If a single WebSocket connection stays open and events arrive slowly (e.g., 1 per minute), the buffer grows indefinitely until the DO runs out of memory.\n\nFix: Schedule an alarm on first event receipt to ensure time-based flush even with slow event rates:\n1. In webSocketMessage, check if alarm is set and schedule one if not\n2. Add a MAX_BUFFER_SIZE constant to force flush regardless of batch size\n3. Consider using DO transactional storage to persist buffer state\n\nFile: /Users/nathanclevenger/projects/parquedb/src/worker/TailDO.ts","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:49.863947-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:36:33.909724-06:00","closed_at":"2026-02-03T11:36:33.909724-06:00","close_reason":"Closed"}
{"id":"parquedb-20vu","title":"Replace unsafe 'as any' casts with proper types","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:32.465994-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:22:56.740058-06:00","closed_at":"2026-02-03T09:22:56.740058-06:00","close_reason":"Replaced unsafe 'as any' casts in src/subscriptions with proper types:\n- transports.ts: Fixed getMessagesOfType() to use a type guard predicate instead of 'as any' cast\n- manager.ts: Replaced 'fullId: `${ns}/${entityId}` as any' with proper 'makeEntityId(ns, entityId)' function call"}
{"id":"parquedb-2380","title":"Document magic numbers in parquet corruption detection","description":"In src/ParqueDB/core.ts detectParquetCorruption has magic numbers (12, 0xFF, 2) without explanation. Extract to named constants with Parquet format documentation.","status":"open","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:31.628238-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:31.628238-06:00"}
{"id":"parquedb-2679","title":"Refactor: Worker - API consistency between DO and local ParqueDB","description":"There are API differences between ParqueDBDO (Worker writes) and ParqueDBImpl (local):\n\nDO (ParqueDBDO):\n- Uses DOCreateOptions, DOUpdateOptions, DODeleteOptions\n- create() requires $type and name fields\n- link() takes full entityId strings (ns/id format)\n\nLocal (ParqueDBImpl/ParqueDB):\n- Uses CreateOptions, UpdateOptions, DeleteOptions\n- create() derives type from namespace\n- Different relationship API\n\nFiles:\n- src/worker/ParqueDBDO.ts\n- src/ParqueDB/core.ts\n- src/types/worker.ts\n\nThis makes code sharing difficult and creates confusion about which API to use.\n\nRefactor:\n1. Align option types (or document why they differ)\n2. Standardize required fields for create\n3. Consider shared base implementation\n\nImpact: Easier code reuse, clearer mental model for developers","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:34:25.51832-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:56:01.581747-06:00","closed_at":"2026-02-03T10:56:01.581747-06:00","close_reason":"Closed"}
{"id":"parquedb-290","title":"Implement secondary indexes","description":"B-tree style indexes for range queries on fields","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:49.296781-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:24:34.937087-06:00","closed_at":"2026-02-01T14:24:34.937087-06:00","close_reason":"Secondary indexes implemented with Hash and SST index types. All 251 index tests passing including unit tests for hash index, SST index, sharded indexes, and integration tests."}
{"id":"parquedb-2be","title":"Documentation Epic: Complete ParqueDB Documentation","description":"Comprehensive documentation for ParqueDB including API docs, guides, and examples","status":"closed","priority":1,"issue_type":"epic","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T12:40:05.653094-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:38:45.473766-06:00","closed_at":"2026-02-01T14:38:45.473766-06:00","close_reason":"Closed"}
{"id":"parquedb-2c7j","title":"Magic Numbers Throughout Codebase","description":"## Problem\nNumbers like 4, 12, 0xFF, 2 have no explanation.\n\n## Location\n- src/ParqueDB/core.ts (lines 266-280)\n- Other locations throughout codebase\n\n## Fix\nDefine named constants with comments explaining their purpose.\n\n## Priority\nP1 - HIGH","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:10:23.676432-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T18:22:30.443014-06:00","closed_at":"2026-02-03T18:22:30.443014-06:00","close_reason":"Fixed in commit ab28519"}
{"id":"parquedb-2dao","title":"Standardize null vs undefined handling","description":"Inconsistent null/undefined handling: comparison.ts treats them as equivalent, filter.ts distinguishes between them (null matches null/undefined, undefined matches everything). Document and standardize the behavior.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T17:01:13.169944-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T17:21:56.33577-06:00","closed_at":"2026-02-01T17:21:56.33577-06:00","close_reason":"Closed"}
{"id":"parquedb-2f2j","title":"P2: Optimize  O(n*m) complexity","description":"src/mutation/operators.ts:192-197 - Uses .some() with deepEqual for each item. Use Set with stringified values for O(n).","status":"open","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:09.211558-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:09.211558-06:00"}
{"id":"parquedb-2h4","title":"[RED] Projection tests","description":"Write failing tests for field inclusion and exclusion","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:24.593745-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:15:31.014614-06:00","closed_at":"2026-02-01T14:15:31.014614-06:00","close_reason":"Closed"}
{"id":"parquedb-2i0i","title":"Implement ParqueDB client SDK for places.org.ai","description":"Create TypeScript client SDK for querying Places via ParqueDB:\n\nAPI design:\n```typescript\nconst places = new PlacesClient({ baseUrl: 'https://places.org.ai' })\n\n// Place lookup (auto-detects source)\nawait places.get('geonames:2921044')  // Germany by GeoNames ID\nawait places.get('wikidata:Q183')     // Germany by Wikidata ID\n\n// Search\nawait places.search('san francisco', { types: ['city'], limit: 10 })\n\n// Geo query\nawait places.nearby({ lat: 37.77, lng: -122.42, radius: 5000 })\nawait places.within({ bounds: [37.7, -122.5, 37.8, -122.4] })\n\n// Hierarchy\nawait places.hierarchy('geonames:5391959')  // SF -\u003e CA -\u003e US -\u003e North America\n\n// Concordance\nawait places.concordance('geonames:5391959')\n// Returns: { geonames: '5391959', wikidata: 'Q62', overture: '...' }\n```\n\nShare common code with @wiki.org.ai/client where possible.","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:18:56.325895-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:56:52.353459-06:00","closed_at":"2026-02-03T13:56:52.353459-06:00","close_reason":"Implemented full places.org.ai client SDK with lookup, search, geo, hierarchy, and concordance operations","dependencies":[{"issue_id":"parquedb-2i0i","depends_on_id":"parquedb-kjr5","type":"blocks","created_at":"2026-02-03T13:19:18.457714-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-2jk","title":"Implement upsert","description":"Create or update based on filter","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:36.635305-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:06:28.672893-06:00","closed_at":"2026-02-01T14:06:28.672893-06:00","close_reason":"Closed"}
{"id":"parquedb-2kk","title":"[REFACTOR] ParqueDB class cleanup","description":"Refactor ParqueDB class","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:31.644586-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:02:41.978869-06:00","closed_at":"2026-02-01T14:02:41.978869-06:00","close_reason":"Closed"}
{"id":"parquedb-2lf","title":"Implement StorageBackend interface","description":"Define and implement the StorageBackend interface for abstracting file operations","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:50:54.271963-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:03:10.06904-06:00","closed_at":"2026-02-01T14:03:10.06904-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-2lf","depends_on_id":"parquedb-nvh","type":"blocks","created_at":"2026-01-30T11:51:09.046564-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-2lf","depends_on_id":"parquedb-2nq","type":"blocks","created_at":"2026-01-30T11:51:09.136886-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-2lf","depends_on_id":"parquedb-ce0","type":"blocks","created_at":"2026-01-30T11:51:09.226986-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-2m9z","title":"Refactor: Storage - Extract duplicate normalizePath function","description":"The normalizePath function is duplicated in:\\n\\n1. MemoryBackend.ts (lines 122-128) - normalizePath method, removes leading slash\\n2. DOSqliteBackend.ts (lines 132-143) - normalizePath function, removes leading and trailing slashes\\n\\nAlso, R2Backend has withPrefix/withoutPrefix pattern, and FsBackend has resolvePath which does normalization plus security checks.\\n\\nRefactor to:\\n- Create a shared normalizePath function in src/storage/utils.ts\\n- Consider adding a PathNormalizer class with configurable behavior\\n- Standardize path handling across all backends\\n\\nFiles affected:\\n- /Users/nathanclevenger/projects/parquedb/src/storage/MemoryBackend.ts\\n- /Users/nathanclevenger/projects/parquedb/src/storage/DOSqliteBackend.ts\\n- /Users/nathanclevenger/projects/parquedb/src/storage/R2Backend.ts\\n- /Users/nathanclevenger/projects/parquedb/src/storage/FsBackend.ts\\n- /Users/nathanclevenger/projects/parquedb/src/storage/FsxBackend.ts","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:32:47.572467-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:33:22.127278-06:00","closed_at":"2026-02-03T11:33:22.127278-06:00","close_reason":"Closed"}
{"id":"parquedb-2mll","title":"Studio: Improve error recovery and retry UX","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:35.174862-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:50:37.508132-06:00","closed_at":"2026-02-03T08:50:37.508132-06:00","close_reason":"Closed"}
{"id":"parquedb-2nq","title":"[GREEN] StorageBackend interface implementation","description":"Implement StorageBackend interface to pass tests","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:03.480769-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:03:05.085765-06:00","closed_at":"2026-02-01T14:03:05.085765-06:00","close_reason":"Closed"}
{"id":"parquedb-2o5","title":"Implement vector indexes","description":"HNSW index for $vector similarity queries","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:57.669927-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:30:38.642369-06:00","closed_at":"2026-02-01T14:30:38.642369-06:00","close_reason":"Implemented HNSW vector index with near similarity queries. Features: O(log n) ANN search, multiple distance metrics (cosine, euclidean, dot), persistence, score filtering, efSearch tuning."}
{"id":"parquedb-2p4v","title":"Implement Wikidata partitioned dump loader for wiki.org.ai","description":"Enhance Wikidata loader to partition data for Cloudflare Workers:\n\nPartitioning strategy (hybrid by type + ID range):\n- scholarly/ (45M articles, ~45 partitions)\n- humans/ (12.5M, ~13 partitions)\n- taxons/ (3.8M, ~4 partitions)\n- places/ (10M, by country/admin hierarchy)\n- astronomical/ (8.4M, by type)\n- general/ (everything else, by ID range)\n\nRequirements:\n1. Stream 100GB+ dump with backpressure\n2. Classify entities by P31 (instance of)\n3. Generate 25MB Parquet chunks\n4. Track file manifest for worker routing\n\nTarget: ~60GB data, ~5,000 files","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:18:05.50273-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:43:11.331849-06:00","closed_at":"2026-02-03T13:43:11.331849-06:00","close_reason":"Implemented partitioned Wikidata loader with type-based partitioning, 25MB chunks, manifest generation"}
{"id":"parquedb-2plg","title":"DeltaBackend.hardDeleteEntities does not use OCC - potential data loss","description":"**High: Data Corruption Risk**\n\nIn /src/backends/delta.ts lines 1490-1595, the hardDeleteEntities() method:\n\n1. Reads current version and entities (lines 1492-1505)\n2. Writes new data file with remaining entities (lines 1511-1518)\n3. Writes commit file WITHOUT ifNoneMatch check (line 1560)\n\nUnlike appendEntities() which uses OCC with ifNoneMatch: '*', hardDeleteEntities() directly writes the commit file without checking for concurrent modifications.\n\n**Impact**: If two processes do hard deletes concurrently:\n- One process's commit could be silently overwritten\n- Entities that should be deleted may reappear\n- Entities that shouldn't be deleted may be lost\n\n**Location**: src/backends/delta.ts:1490-1595\n\n**Fix**: Apply the same OCC pattern used in appendEntities() (lines 1141-1209)","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:36:39.858871-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:18:19.28237-06:00","closed_at":"2026-02-03T12:18:19.28237-06:00","close_reason":"Closed"}
{"id":"parquedb-2r4k","title":"Remove unused code in compaction workflows","description":"Dead code to remove: 1) targetFileSize variable in compaction-migration.ts:161 - defined but never used, 2) ConsumerState interface in compaction-queue-consumer.ts:68-98 - defined but never used, 3) Grace period dead code comment in compaction-migration.ts:231-236 - says 'we just log and continue' but actual sleep is implemented elsewhere.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:33:28.065709-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:35:26.684839-06:00","closed_at":"2026-02-03T13:35:26.684839-06:00","close_reason":"Closed"}
{"id":"parquedb-2sm","title":"[GREEN] Secondary index implementation","description":"Implement secondary indexes to pass tests","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:52.53722-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:24:38.686033-06:00","closed_at":"2026-02-01T14:24:38.686033-06:00","close_reason":"Secondary index implementation complete. Hash index provides O(1) equality lookups, SST index provides sorted range queries. Includes sharded variants for large indexes. All 251 tests passing."}
{"id":"parquedb-2v6","title":"Convert wrangler.toml to wrangler.jsonc 2026 compat","description":"Replace wrangler.toml with modern wrangler.jsonc. Use compatibility_date 2026-01-30.","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T14:30:02.153993-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-30T14:30:41.603228-06:00","closed_at":"2026-01-30T14:30:41.603228-06:00","close_reason":"Closed"}
{"id":"parquedb-2vbk","title":"CRITICAL: Fix ReDoS vulnerability in glob patterns","description":"Config parser glob pattern matching is vulnerable to ReDoS attacks. Need to sanitize/limit pattern complexity or use a safe glob library.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:54:38.186049-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:07:46.920671-06:00","closed_at":"2026-02-03T09:07:46.920671-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-2vo","title":"[GREEN] Field operators implementation","description":"Implement $set, $unset, $rename to pass tests","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:21.944301-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:08:21.651836-06:00","closed_at":"2026-02-01T14:08:21.651836-06:00","close_reason":"Closed"}
{"id":"parquedb-2vp9","title":"Bug: upsert does not validate filter","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:36:31.689246-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:36:40.092184-06:00","closed_at":"2026-02-03T11:36:40.092184-06:00","close_reason":"Closed"}
{"id":"parquedb-2vzx","title":"P2: Replace console.warn with logger utility","description":"Direct console.warn usage in src/query/filter.ts:108-109 should use a logger utility for consistent logging and better testability.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:11:05.350088-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:24:29.070069-06:00","closed_at":"2026-02-03T15:24:29.070069-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-2whw","title":"[FEATURE] Implement vector similarity search","description":"Vector search is not implemented. Need to:\n\n1. Design vector storage format (in Parquet or separate index)\n2. Implement embedding storage and retrieval\n3. Add similarity search operators ($nearVector, $cosineSimilarity)\n4. Consider HNSW or IVF index structures\n5. Integrate with AI embedding providers\n\nSee docs/architecture/SECONDARY_INDEXES.md for index design patterns.","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T13:36:03.350503-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:42:14.168569-06:00","closed_at":"2026-02-01T14:42:14.168569-06:00","close_reason":"Closed"}
{"id":"parquedb-2xf","title":"[REFACTOR] Bloom filter optimization","description":"Optimize bloom filter size and FPR","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:48.286197-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:24:19.506156-06:00","closed_at":"2026-02-01T14:24:19.506156-06:00","close_reason":"Closed"}
{"id":"parquedb-2zr9","title":"Refactor: Worker - Consolidate event buffering systems in ParqueDBDO","description":"ParqueDBDO has two parallel event buffering systems:\n\n1. eventBuffer + eventBufferSize (generic, lines 189-193)\n2. nsEventBuffers (namespace-based with sequence tracking, line 199)\n\nThe namespace-based system (appendEventWithSeq) is more advanced with Sqids ID generation, but appendEvent() still uses the generic buffer. This duplication is confusing.\n\nFiles:\n- src/worker/ParqueDBDO.ts\n\nRefactor options:\n1. Remove legacy eventBuffer and migrate all callers to appendEventWithSeq\n2. Or consolidate both systems into one unified approach\n\nImpact: Reduces code complexity and prevents confusion about which method to use","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:32:59.960847-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:22:56.449506-06:00","closed_at":"2026-02-03T11:22:56.449506-06:00","close_reason":"Closed"}
{"id":"parquedb-30dg","title":"Integration: Build Vercel AI SDK middleware (caching/logging)","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:43.86012-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:55:56.829702-06:00","closed_at":"2026-02-03T08:55:56.829702-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-30dg","depends_on_id":"parquedb-18me","type":"blocks","created_at":"2026-02-03T08:43:38.644245-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-30l8","title":"Document atomicReplace limitations","description":"atomicReplace() in refresh.ts performs delete-then-move which is not truly atomic. If process crashes between operations, data could be lost. Document this limitation and recommend WAL or storage-native atomic rename.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:02:26.717517-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:03:50.460406-06:00","closed_at":"2026-02-03T11:03:50.460406-06:00","close_reason":"Closed"}
{"id":"parquedb-31po","title":"Add path traversal protection in RemoteBackend","description":"RemoteBackend.buildUrl() has no validation to prevent path traversal attacks. A malicious path like '../../sensitive-file' could access unintended resources. Add path sanitization to reject paths containing '..' or '//'.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:04:53.078246-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:08:00.583625-06:00","closed_at":"2026-02-03T08:08:00.583625-06:00","close_reason":"Closed"}
{"id":"parquedb-336","title":"[GREEN] Projection implementation","description":"Implement projection to pass tests","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:26.014878-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:15:30.815964-06:00","closed_at":"2026-02-01T14:15:30.815964-06:00","close_reason":"Closed"}
{"id":"parquedb-347","title":"Remove mocks from ParqueDB.test.ts and use real storage","description":"Removed ALL vi.fn() mocks from tests/unit/ParqueDB.test.ts and replaced them with real FsBackend using temp directories. Also fixed ParqueDB.get() to handle FileNotFoundError gracefully for empty databases.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T14:34:52.010768-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-30T14:34:56.234738-06:00","closed_at":"2026-01-30T14:34:56.234738-06:00","close_reason":"Closed"}
{"id":"parquedb-353","title":"Implement MemoryBackend","description":"In-memory storage backend for testing","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:12.837861-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:11:47.872937-06:00","closed_at":"2026-02-01T13:11:47.872937-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-353","depends_on_id":"parquedb-ezw","type":"blocks","created_at":"2026-01-30T11:51:22.162836-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-353","depends_on_id":"parquedb-qrm","type":"blocks","created_at":"2026-01-30T11:51:22.250376-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-353","depends_on_id":"parquedb-d3j","type":"blocks","created_at":"2026-01-30T11:51:22.332374-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-37c3","title":"Implement ParquetSchemaGenerator for typed collections","description":"## Context\nPart of typed storage implementation (parquedb-k7jj design).\n\n## Requirements\n\n1. Create `src/parquet/schema-generator.ts`:\n   - `ParquetSchemaGenerator` interface\n   - `ParquetSchemaGeneratorImpl` class\n   - `fromTypeDefinition(typeDef, options)` - generate Parquet schema\n   - `fieldToParquet(fieldName, fieldType)` - convert single field\n\n2. Type mapping (IceType/GraphDL to Parquet):\n   - string -\u003e STRING\n   - int -\u003e INT64\n   - float/double/number -\u003e FLOAT/DOUBLE\n   - boolean -\u003e BOOLEAN\n   - date -\u003e DATE (INT32)\n   - datetime/timestamp -\u003e TIMESTAMP_MILLIS (INT64)\n   - uuid -\u003e FIXED_LEN_BYTE_ARRAY(16) + UUID logical type\n   - json -\u003e BYTE_ARRAY + JSON logical type\n   - binary -\u003e BYTE_ARRAY\n   - Support required (!) and array ([]) modifiers\n\n3. System fields:\n   - Always include $id, $type, createdAt, updatedAt, version\n   - Optionally include $data Variant column\n\n4. Testing:\n   - Unit tests for all type mappings\n   - Tests for required vs optional\n   - Tests for array types\n   - Tests for $options.includeDataVariant\n\n## Acceptance Criteria\n- [ ] Type mapping covers all IceType primitives\n- [ ] Required/optional modifiers handled correctly\n- [ ] Array types use REPEATED repetition\n- [ ] System fields always present\n- [ ] $data column respects includeDataVariant option\n- [ ] Tests pass\n\n## References\n- Design: docs/architecture/typed-storage.md\n- Depends on: parquedb-s8yv (StorageRouter)","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:41:20.87395-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:11:30.211467-06:00","closed_at":"2026-02-03T07:11:30.211467-06:00","close_reason":"Implemented"}
{"id":"parquedb-38d","title":"Write: Update Operators ($set, $inc, $push)","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T12:40:42.12032-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:35:37.601656-06:00","closed_at":"2026-02-01T14:35:37.601656-06:00","close_reason":"Closed"}
{"id":"parquedb-38j7","title":"Fix FTS phrase query tests","description":"tests/unit/indexes/fts-phrase.test.ts has 4 failing tests for phrase search: consecutive word matching, +/- modifiers, and phrase boost. Phrase query support may need implementation.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:32:04.21647-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:41:02.06314-06:00","closed_at":"2026-02-03T12:41:02.06314-06:00","close_reason":"Closed"}
{"id":"parquedb-39d","title":"[RED] FsBackend tests","description":"Write failing tests for FsBackend","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:48.783973-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T04:45:59.164984-06:00","closed_at":"2026-02-02T04:45:59.164984-06:00","close_reason":"Tests implemented in earlier waves"}
{"id":"parquedb-3g6x","title":"Consolidate MV module files","description":"The materialized-views module has 17 TypeScript files with overlapping type exports and conflicts. Consolidate into fewer files with clearer responsibilities.","status":"open","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:11.600089-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:11.600089-06:00"}
{"id":"parquedb-3h6n","title":"Add Worker component unit tests","description":"Worker components under-tested. Create tests/unit/worker/ with:\n- IndexCache.test.ts\n- ReadPath.test.ts\n- QueryExecutor.test.ts\n- CacheStrategy.test.ts\n\nFocus on index caching logic, read path optimization, and query planning.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T12:42:21.851024-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T12:54:07.414252-06:00","closed_at":"2026-02-01T12:54:07.414252-06:00","close_reason":"Completed by parallel agents"}
{"id":"parquedb-3ig","title":"[GREEN] Filter logical operators","description":"Implement logical operators to pass tests","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:08.895506-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:10:39.921329-06:00","closed_at":"2026-02-01T13:10:39.921329-06:00","close_reason":"Closed"}
{"id":"parquedb-3ig4","title":"Refactor ParqueDB/core.ts - split 116KB monolith","description":"ParqueDB/core.ts is 116KB with 50+ imports, violating single responsibility principle.\n\nSplit into:\n- core.ts (database lifecycle, initialization) \n- crud.ts (create, read, update, delete operations)\n- relationships.ts (relationship management)\n- events.ts (event log, time-travel)\n- snapshots.ts (snapshot management)\n- collections.ts (collection proxying)\n\nThis will:\n- Improve testability\n- Reduce cognitive load\n- Enable better code review\n- Speed up TypeScript compilation","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:31:47.640158-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:36:49.617501-06:00","closed_at":"2026-02-03T12:36:49.617501-06:00","close_reason":"Closed"}
{"id":"parquedb-3iwj","title":"Consolidate magic numbers into constants","description":"Magic numbers scattered across files: DEFAULT_MAX_INBOUND=100, CONCURRENCY=4, various size constants in R2Backend. Create src/constants.ts to centralize configuration values.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T17:01:12.328371-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T17:26:04.90627-06:00","closed_at":"2026-02-01T17:26:04.90627-06:00","close_reason":"Closed"}
{"id":"parquedb-3j36","title":"Type Safety: Reduce 'as unknown' type assertions (138 occurrences)","description":"138 'as unknown' type assertions across 43 files bypass TypeScript's type system. High-impact files:\n- src/types/cast.ts (31 occurrences) - needs proper type guards\n- src/worker/QueryExecutor.ts (7 occurrences) \n- src/mutation/operators.ts (6 occurrences)\n- src/integrations/ai-sdk/middleware.ts (6 occurrences)\n- src/observability/ai/AIRequestsMV.ts (4 occurrences)\n\nRecommendation: Replace with proper type guards, branded types, or Zod validation where appropriate.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:47:21.389398-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:06:18.088316-06:00","closed_at":"2026-02-03T15:06:18.088316-06:00","close_reason":"Closed"}
{"id":"parquedb-3jd","title":"[GREEN] upsertMany implementation","description":"Implement Collection.upsertMany() to pass the tests.","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T14:30:05.063918-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-30T14:36:55.258417-06:00","closed_at":"2026-01-30T14:36:55.258417-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-3jd","depends_on_id":"parquedb-8p4","type":"blocks","created_at":"2026-01-30T14:30:14.156157-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-3js3","title":"Add tests for parquet/variant-filter.ts","description":"src/parquet/variant-filter.ts (304 lines) pushes filters into Variant-encoded columns. Performance-critical path with no dedicated tests.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T07:16:03.565102-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T07:23:51.456114-06:00","closed_at":"2026-02-02T07:23:51.456114-06:00","close_reason":"Closed"}
{"id":"parquedb-3m6e","title":"TypeScript: Reduce as unknown casts in materialized views","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:45:52.752842-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:18:50.731962-06:00","closed_at":"2026-02-03T13:18:50.731962-06:00","close_reason":"Closed"}
{"id":"parquedb-3m8","title":"[REFACTOR] Update operators cleanup","description":"Refactor update operators","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:27.59666-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:08:21.810893-06:00","closed_at":"2026-02-01T14:08:21.810893-06:00","close_reason":"Closed"}
{"id":"parquedb-3nku","title":"Add integration tests for SyncEngine","description":"SyncEngine lacks dedicated tests. Need tests for: push/pull/sync operations, error handling during file transfer, progress callback behavior, hash calculation, content type guessing.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:05:25.263679-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:09:41.916231-06:00","closed_at":"2026-02-03T08:09:41.916231-06:00","close_reason":"Closed"}
{"id":"parquedb-3nxk","title":"Refactor: core.ts - Replace 'any' type casts with proper typing","description":"In src/ParqueDB/core.ts, there are several implicit 'any' type usages through type assertions that could be tightened:\n\n1. Line 749-750: `const beforeEntity = options?.returnDocument === 'before' ? (isInsert ? null : { ...entity }) : null`\n   - beforeEntity is typed implicitly, should be `Entity | null`\n\n2. Lines 779, 790, 801, etc.: `(entity as Record\u003cstring, unknown\u003e)[key]`\n   - This pattern is used extensively in update operators\n   - Consider a helper type: `type EntityRecord = Entity \u0026 Record\u003cstring, unknown\u003e`\n\n3. Line 1124: `{ predicate, to: linkTarget } as unknown as Entity`\n   - This is a type lie - the object is not an Entity\n   - Should define a proper RelationshipEventData type\n\n4. Line 2217-2218: `before: (e.before ?? null) as Entity | null`\n   - Event.before and Event.after are typed as Variant\n   - Need proper type narrowing or a type guard\n\nConsider creating a types.ts extension with:\n- EntityRecord type for mutation operations\n- RelationshipEventData for link/unlink events\n- Type guards: isEntityVariant(v): v is Entity","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:33:22.852915-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:21:40.218668-06:00","closed_at":"2026-02-03T11:21:40.218668-06:00","close_reason":"Closed","labels":["refactor","types"]}
{"id":"parquedb-3p9p","title":"Fix N+1 query in Payload create operations","description":"In src/integrations/payload/operations/create.ts lines 147-155, sequential updates for each version creates N+1 pattern. Implement bulkUpdate or use updateMany.","status":"open","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:21.595276-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:21.595276-06:00"}
{"id":"parquedb-3ty2","title":"Missing State Machine Validation in Compaction Queue Consumer","description":"**File:** src/workflows/compaction-queue-consumer.ts\n\n**Issue:** DO endpoints check state but don't validate full transition graph\n\n**Fix:** Add proper state machine with documented valid transitions","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T14:34:40.44924-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:50:41.219879-06:00","closed_at":"2026-02-03T14:50:41.219879-06:00","close_reason":"Closed"}
{"id":"parquedb-3ui","title":"Core Infrastructure","description":"Foundation for ParqueDB: types, storage backends, and basic DB class","status":"closed","priority":0,"issue_type":"epic","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:50:26.777987-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:34:08.760149-06:00","closed_at":"2026-02-01T14:34:08.760149-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-3ui","depends_on_id":"parquedb-2lf","type":"blocks","created_at":"2026-01-30T11:52:09.664689-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-3ui","depends_on_id":"parquedb-353","type":"blocks","created_at":"2026-01-30T11:52:09.750379-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-3ui","depends_on_id":"parquedb-aio","type":"blocks","created_at":"2026-01-30T11:52:09.833468-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-3ui","depends_on_id":"parquedb-hi2","type":"blocks","created_at":"2026-01-30T11:52:09.913238-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-3ui","depends_on_id":"parquedb-pt8","type":"blocks","created_at":"2026-01-30T11:52:09.996242-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-3vag","title":"Move credentials out of wrangler.toml","description":"SECURITY: account_id and zone_id are hardcoded in snippets/worker/wrangler.toml. Should be environment variables or secrets. Risk: credential exposure in git history.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:27:28.912949-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:40:57.247355-06:00","closed_at":"2026-02-03T12:40:57.247355-06:00","close_reason":"Closed"}
{"id":"parquedb-3wl9","title":"Fix parseSchema() double type assertions","description":"parseSchema() uses 'as unknown as MVDefinition' pattern which bypasses type checking. Should use type guards (isMVDefinition, isStreamCollection) instead of unsafe casts. File: src/materialized-views/define.ts lines 520-531","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:02:20.527988-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:04:46.591201-06:00","closed_at":"2026-02-03T11:04:46.591201-06:00","close_reason":"Closed"}
{"id":"parquedb-3wv","title":"[GREEN] Upsert implementation","description":"Implement upsert to pass tests","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:38.268485-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:06:28.70388-06:00","closed_at":"2026-02-01T14:06:28.70388-06:00","close_reason":"Closed"}
{"id":"parquedb-3xh2","title":"P2: Add streaming engine test coverage","description":"src/streaming/ only has node-adapter tests. Add tests for streaming queries and back-pressure handling.","status":"open","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:20.921487-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:20.921487-06:00"}
{"id":"parquedb-3y2","title":"Implement relationship storage","description":"Store relationships in rels.parquet with forward/reverse indexes","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:21.655638-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:08:29.147715-06:00","closed_at":"2026-02-01T14:08:29.147715-06:00","close_reason":"Closed"}
{"id":"parquedb-3yy","title":"[RED] Collection class tests","description":"Write failing tests for Collection methods","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:44.890748-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:02:35.488698-06:00","closed_at":"2026-02-01T14:02:35.488698-06:00","close_reason":"Closed"}
{"id":"parquedb-3zab","title":"Implement mutation layer per CLAUDE.md","description":"CLAUDE.md describes src/mutation/ for create/update/delete but this layer doesn't exist. Mutation logic is scattered in ParqueDBImpl and ParqueDBDO. Create mutation layer to centralize validation, authorization, and update logic using Command pattern.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:20.435448-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T16:37:22.274223-06:00","closed_at":"2026-02-01T16:37:22.274223-06:00","close_reason":"Closed"}
{"id":"parquedb-3zc0","title":"Refactor: Storage - R2Backend.copy reads entire file into memory","description":"R2Backend.copy() at lines 554-591 reads the entire source file into memory before writing to destination:\\n\\n```typescript\\nasync copy(source: string, dest: string): Promise\u003cvoid\u003e {\\n  const sourceObj = await this.bucket.get(sourceKey, undefined)\\n  const data = new Uint8Array(await sourceObj.arrayBuffer())  // Entire file in memory!\\n  await this.bucket.put(destKey, data, ...)\\n}\\n```\\n\\nFor large files, this could cause memory issues. Consider:\\n- Using multipart upload for large files\\n- Streaming the data if possible\\n- Or documenting the size limitation\\n\\nNote: R2 does not currently support server-side copy, so client-side transfer may be necessary, but could use streaming.\\n\\nFiles affected:\\n- /Users/nathanclevenger/projects/parquedb/src/storage/R2Backend.ts","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:33:02.077617-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:17:01.730832-06:00","closed_at":"2026-02-03T11:17:01.730832-06:00","close_reason":"Closed"}
{"id":"parquedb-416d","title":"Testing: Fix 568 failing tests - comprehensive test suite repair","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:49:41.173626-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:35:56.058223-06:00","closed_at":"2026-02-03T14:35:56.058223-06:00","close_reason":"Session made significant progress fixing test failures. Fixed: (1) Added matchMode and similarity fields to createRelationshipSchema(), (2) Fixed do-event-sourced.workers.test.ts delete assertions, (3) Added 18 missing type guard functions to update.ts. Remaining failures (422) are due to: unimplemented functionality in new workflow/compaction tests, sync client retry logic not implemented, and infrastructure-dependent tests. Tests went from 340 failing to ~9 failing core tests before additional test files were discovered."}
{"id":"parquedb-42ii","title":"Add logging to empty catch blocks","description":"20+ empty catch {} blocks silently swallow errors. Files include: indexes/manager.ts:684, client/service-binding.ts:225, storage/R2Backend.ts:848. Add debug logging or document intentionally ignored errors.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T05:32:36.843432-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T06:47:24.049202-06:00","closed_at":"2026-02-02T06:47:24.049202-06:00","close_reason":"Closed"}
{"id":"parquedb-44g","title":"[GREEN] Event archival implementation","description":"Implement event archival to pass tests","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:47.02725-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:22:32.579636-06:00","closed_at":"2026-02-01T14:22:32.579636-06:00","close_reason":"Closed"}
{"id":"parquedb-450n","title":"Evaluate DataFusion-style embedded page indexes (ColumnIndex/OffsetIndex)","description":"## Context\nDataFusion and modern Parquet readers use page-level indexes embedded in row group footers:\n\n1. **ColumnIndex** - min/max per page (not just per row group)\n2. **OffsetIndex** - byte offsets for each page\n3. **Bloom filters** - embedded in row group metadata\n\nCurrently ParqueDB uses:\n- Row group level min/max (coarser granularity)\n- External bloom filter files (indexes/bloom/{ns}.bloom)\n- $index_* columns sorted for row group pruning\n\n## Benefits of DataFusion-style\n- Single file (no separate index files)\n- Page-level pruning (finer than row group)\n- Standard Parquet 2.0 format (better ecosystem compatibility)\n- Works with any Parquet reader that supports these features\n\n## Considerations\n- hyparquet support for ColumnIndex/OffsetIndex\n- hyparquet-writer support for writing page indexes\n- Migration path from external bloom files\n- Performance comparison needed\n\n## Tasks\n1. Check hyparquet support for Parquet 2.0 page indexes\n2. Benchmark page-level vs row-group-level pruning\n3. Decide on migration approach\n4. Implement if beneficial","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T03:59:11.623321-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T04:41:47.943643-06:00","closed_at":"2026-02-03T04:41:47.943643-06:00","close_reason":"Evaluation complete. hyparquet already has full page index support. See docs/architecture/PAGE_INDEXES_EVALUATION.md for detailed findings. Recommendation: ADOPT DataFusion-style page indexes - Phase 1 is trivial (enable columnIndex:true), Phase 2 uses existing parquetQuery()."}
{"id":"parquedb-45u","title":"Implement SQLite blob flush for events WAL","description":"Flush buffered events to DO SQLite as compressed blobs:\n\n```sql\nCREATE TABLE events_wal (\n  id INTEGER PRIMARY KEY,\n  batch BLOB,           -- msgpack array of events, up to 2MB\n  min_ts INTEGER,\n  max_ts INTEGER,\n  count INTEGER\n);\n```\n\n- Serialize events to msgpack/cbor\n- Write as single blob row (cost optimization)\n- Support reading back for replay\n\nFile: src/events/sqlite-wal.ts","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T06:37:51.319827-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T07:06:39.742045-06:00","closed_at":"2026-02-01T07:06:39.742045-06:00","close_reason":"Implemented SqliteWal class for storing event batches as blobs in SQLite. Features: batch serialization/deserialization, time-range queries, flush management, batch cleanup. Added 18 passing tests.","dependencies":[{"issue_id":"parquedb-45u","depends_on_id":"parquedb-zqk","type":"blocks","created_at":"2026-02-01T06:38:07.485861-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-48hu","title":"Performance: reconstructEntityAtTime has O(n) filter on every call","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:41:58.995599-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:22:43.69387-06:00","closed_at":"2026-02-03T12:22:43.69387-06:00","close_reason":"Implemented O(1) entity event index and reconstruction cache to optimize reconstructEntityAtTime. Added binary search for finding events at specific timestamps. All time-travel, events, snapshots, and ParqueDB tests pass."}
{"id":"parquedb-49o4","title":"Implement worker backpressure handling","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:19:39.382127-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:24:44.926121-06:00","closed_at":"2026-02-03T15:24:44.926121-06:00","close_reason":"Closed"}
{"id":"parquedb-4asy","title":"Cross-Backend Migration Tests: Test migrating data between backends","description":"Test migrating data between backends:\n- Native to Iceberg migration\n- Iceberg to Delta migration\n- Schema preservation during migration","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:57:46.49683-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:08:30.612249-06:00","closed_at":"2026-02-03T09:08:30.612249-06:00","close_reason":"Closed"}
{"id":"parquedb-4b13","title":"Fix: Incomplete cron expression validation","description":"isValidCronExpression only checks 5-6 parts, doesn't validate field ranges (0-59 min, 0-23 hr, etc). Add proper validation or use cron library. File: src/materialized-views/types.ts lines 483-488","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:15:34.351822-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:39:18.064696-06:00","closed_at":"2026-02-03T10:39:18.064696-06:00","close_reason":"Closed"}
{"id":"parquedb-4bp","title":"[RED] Secondary index tests","description":"Write failing tests for index creation and query optimization","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:51.24896-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:24:36.703522-06:00","closed_at":"2026-02-01T14:24:36.703522-06:00","close_reason":"Secondary index tests written covering Hash and SST indexes including lookups, range queries, composite indexes, persistence, and IndexManager integration. All tests passing."}
{"id":"parquedb-4c9e","title":"Remove Collection.ts global state - use storage backend","description":"Collection.ts uses module-level global Maps (globalStorage, globalRelationships, globalEventLog) that bypass the storage backend abstraction. This creates multiple sources of truth. Should use ParqueDB/collection.ts which delegates to storage properly.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T05:32:30.238704-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T06:30:19.431419-06:00","closed_at":"2026-02-02T06:30:19.431419-06:00","close_reason":"Closed"}
{"id":"parquedb-4d0e","title":"P3: Consolidate duplicate BunGlobal/DenoGlobal declarations","description":"Duplicate declarations in globals.d.ts and modules.d.ts. Consolidate to single declaration.","status":"open","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:25.633575-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:25.633575-06:00"}
{"id":"parquedb-4k7","title":"Implement R2Backend (Cloudflare R2)","description":"Storage backend for Cloudflare R2","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:51.7181-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:16:35.487701-06:00","closed_at":"2026-02-01T14:16:35.487701-06:00","close_reason":"R2Backend implementation complete. All 38 tests pass covering: read/write operations, atomic writes, byte range reads, exists/stat/list operations, delete operations (single and prefix), copy/move operations, append operations, directory operations, multipart uploads, streaming writes, and edge cases (special characters, binary data, nested paths). Exports enabled in src/storage/index.ts and src/index.ts."}
{"id":"parquedb-4l5g","title":"Epic: Materialized Views","description":"Add materialized view support to ParqueDB (being scoped separately)\n\n## Scope\n- Streaming MVs triggered on write operations\n- Scheduled MVs with periodic refresh\n- IceType projection syntax for MV definitions\n\n## Acceptance Criteria\n- [ ] Streaming MVs update automatically on data changes\n- [ ] Scheduled MVs can be configured with refresh intervals\n- [ ] IceType syntax supports MV projections\n- [ ] MVs stored efficiently in Parquet format","status":"open","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:57:59.96993-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:57:59.96993-06:00"}
{"id":"parquedb-4l5g.1","title":"Core MV Types and Interfaces","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:00:12.03515-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:12:25.097474-06:00","closed_at":"2026-02-03T09:12:25.097474-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-4l5g.1","depends_on_id":"parquedb-4l5g","type":"parent-child","created_at":"2026-02-03T09:00:12.03593-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-4l5g.10","title":"Aggregation Support","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:00:18.907303-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:14:19.614329-06:00","closed_at":"2026-02-03T09:14:19.614329-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-4l5g.10","depends_on_id":"parquedb-4l5g","type":"parent-child","created_at":"2026-02-03T09:00:18.908022-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-4l5g.11","title":"MV CLI Commands","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:00:19.602876-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:39:39.228519-06:00","closed_at":"2026-02-03T10:39:39.228519-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-4l5g.11","depends_on_id":"parquedb-4l5g","type":"parent-child","created_at":"2026-02-03T09:00:19.60392-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-4l5g.12","title":"MV Tests","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:00:20.339492-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:14:59.941752-06:00","closed_at":"2026-02-03T09:14:59.941752-06:00","close_reason":"Comprehensive tests created for Materialized Views (453 tests across 9 test files covering types, storage, refresh strategies, staleness, and edge cases)","dependencies":[{"issue_id":"parquedb-4l5g.12","depends_on_id":"parquedb-4l5g","type":"parent-child","created_at":"2026-02-03T09:00:20.340379-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-4l5g.13","title":"MV Types: Missing exports for type guards and validators","description":"The types.ts module is missing several exports that tests expect:\n\n**Missing type guards:**\n- isRefreshStrategy - Not exported (type exists but no guard function)\n- isViewState - Not exported\n- isValidViewDefinition - Not exported\n- isPipelineQuery - Not exported (only exists in refresh.ts internally)\n- isSimpleQuery - Not exported\n\n**Missing validators:**\n- validateCronExpression - Not exported from types.ts (exists in cron.ts but not re-exported)\n- validateViewDefinition - Not exported\n\n**Missing constants:**\n- RefreshStrategy enum constant (RefreshStrategyEnum.Full, etc.) - Only type defined, no enum object\n\n**Impact:**\n~40 test failures in tests/unit/materialized-views/types.test.ts due to missing exports","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:15:07.111727-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:53:36.896712-06:00","closed_at":"2026-02-03T12:53:36.896712-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-4l5g.13","depends_on_id":"parquedb-4l5g","type":"parent-child","created_at":"2026-02-03T12:15:07.112687-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-4l5g.14","title":"MV Cron Validation: isValidCronExpression accepts invalid cron expressions","description":"The isValidCronExpression function in types.ts uses a basic check (5-6 parts) but doesn't validate field ranges, allowing invalid expressions like:\n- '60 * * * *' (minute \u003e 59)\n- '* 25 * * *' (hour \u003e 23)\n- '* * 32 * *' (day \u003e 31)\n- '* * * 13 *' (month \u003e 12)\n- '* * * * 8' (day of week \u003e 6)\n- '5-1 * * * *' (inverted range)\n- '*/0 * * * *' (zero step)\n\nThe validateCronExpression function in cron.ts handles these correctly but isn't re-exported or used.\n\n**Failing tests:** ~10 tests in scheduler.test.ts and types.test.ts\n\n**Fix:** Either:\n1. Use validateCronExpression from cron.ts in types.ts\n2. Or re-export validateCronExpression and have isValidCronExpression use it","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:15:27.901529-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:17:54.454743-06:00","closed_at":"2026-02-03T12:17:54.454743-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-4l5g.14","depends_on_id":"parquedb-4l5g","type":"parent-child","created_at":"2026-02-03T12:15:27.902489-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-4l5g.15","title":"MV Storage: Type mismatch between types.ts and storage.ts","description":"The storage.ts module imports ViewDefinition, ViewMetadata, ViewState, ViewStats, ViewOptions, and ViewQuery from types.ts, but these types don't exist in types.ts with those names.\n\n**In types.ts:**\n- MVDefinition (not ViewDefinition)\n- MVMetadata (not ViewMetadata)\n- MVStatus (not ViewState)\n- MVStats (not ViewStats)\n- RefreshConfig (not ViewOptions)\n\n**In storage.ts (lines 18-25):**\n```typescript\nimport type {\n  ViewName,\n  ViewDefinition,  // Doesn't exist\n  ViewMetadata,    // Should be MVMetadata\n  ViewState,       // Should be MVStatus\n  ViewStats,       // Should be MVStats\n  ViewOptions,     // Doesn't exist\n  ViewQuery,       // Doesn't exist\n} from './types'\n```\n\n**Impact:** This causes type import errors and suggests the API wasn't fully integrated after types.ts was refactored.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:15:39.617254-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:40:54.316015-06:00","closed_at":"2026-02-03T12:40:54.316015-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-4l5g.15","depends_on_id":"parquedb-4l5g","type":"parent-child","created_at":"2026-02-03T12:15:39.618119-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-4l5g.16","title":"MV Scheduler: Cron parser doesn't throw on invalid values","description":"The parseCronExpression function in scheduler.ts doesn't throw errors for out-of-range values, it silently ignores them.\n\n**Expected behavior:** Throw error for invalid field values\n**Actual behavior:** Silently ignores invalid values (e.g., '60' for minute just returns empty array)\n\n**Example from tests:**\n```typescript\n// This should throw, but doesn't\nexpect(() =\u003e parseCronExpression('60 * * * *')).toThrow()  // FAILS\n```\n\n**Affected tests:** ~8 tests in scheduler.test.ts for error handling cases\n\n**Root cause:** parseCronField returns values that are in range, but doesn't throw when all parts of a field are out of range.\n\n**Fix:** Add validation that throws when a field contains no valid values after parsing.","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:15:49.426987-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:01:15.652525-06:00","closed_at":"2026-02-03T15:01:15.652525-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-4l5g.16","depends_on_id":"parquedb-4l5g","type":"parent-child","created_at":"2026-02-03T12:15:49.427783-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-4l5g.17","title":"MV Incremental: No integration with ParqueDB write path","description":"The IncrementalRefresher is implemented but has no integration with the actual ParqueDB write path.\n\n**Current state:**\n- IncrementalRefresher expects an EventSource to get events\n- No code wires the ParqueDB mutation layer to emit events to the MV system\n- The getDelta() method uses eventSource.getEventsInRange() but there's no production implementation\n\n**Missing integration points:**\n1. ParqueDB.create/update/delete should emit events to streaming MV engine\n2. Relationship mutations should emit REL_CREATE/REL_DELETE events\n3. Transaction completion should trigger MV updates\n\n**Impact:** Incremental refresh exists in code but cannot work without this integration.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:15:59.611322-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T17:25:32.262616-06:00","closed_at":"2026-02-03T17:25:32.262616-06:00","close_reason":"Exposed setEventCallback/getEventCallback on ParqueDB public API. The MV streaming infrastructure was fully implemented but these methods were missing from IParqueDB interface and proxy method map.","dependencies":[{"issue_id":"parquedb-4l5g.17","depends_on_id":"parquedb-4l5g","type":"parent-child","created_at":"2026-02-03T12:15:59.612121-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-4l5g.18","title":"MV Staleness: No production SourceVersionProvider","description":"The StalenessDetector requires a SourceVersionProvider interface but only has an InMemoryVersionProvider for testing.\n\n**Missing implementations:**\n1. NativeSourceVersionProvider - should use event log sequence IDs\n2. IcebergSourceVersionProvider - should use Iceberg snapshot IDs\n3. DeltaSourceVersionProvider - should use Delta transaction log versions\n\n**Impact:** Staleness detection exists but cannot work in production without these providers integrated with the actual storage backends.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:16:06.341672-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:56:57.4838-06:00","closed_at":"2026-02-03T14:56:57.4838-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-4l5g.18","depends_on_id":"parquedb-4l5g","type":"parent-child","created_at":"2026-02-03T12:16:06.342436-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-4l5g.19","title":"MV Query Routing: Not integrated with QueryExecutor","description":"The query routing to MVs (parquedb-4l5g.9) is marked complete but there's no actual integration visible in the codebase.\n\n**Expected integration:**\n1. QueryExecutor should detect queries that can use an MV\n2. Query optimizer should rewrite queries to use MV data\n3. Staleness checks should happen before routing to MV\n\n**Current state:**\n- Tests exist in tests/unit/query/mv-router.test.ts and mv-optimizer.test.ts\n- No visible integration in main QueryExecutor code\n\n**Impact:** MVs are built but queries don't automatically use them, defeating the purpose.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:16:14.961791-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:55:46.628348-06:00","closed_at":"2026-02-03T14:55:46.628348-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-4l5g.19","depends_on_id":"parquedb-4l5g","type":"parent-child","created_at":"2026-02-03T12:16:14.962819-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-4l5g.2","title":"defineView() API","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:00:13.268086-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:14:13.359617-06:00","closed_at":"2026-02-03T09:14:13.359617-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-4l5g.2","depends_on_id":"parquedb-4l5g","type":"parent-child","created_at":"2026-02-03T09:00:13.268658-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-4l5g.20","title":"MV Streaming: Race condition in concurrent flush operations","description":"The StreamingRefreshEngine has mutex protection via _flushing flag, but there's a potential race condition.\n\n**Issue in maybeFlush():**\n```typescript\nprivate async maybeFlush(): Promise\u003cvoid\u003e {\n  if (this._flushing) return  // Early exit\n  \n  // Check buffers...\n  if (\\!shouldFlush) return\n\n  // RACE: Between check and lock acquisition, another flush could start\n  this._flushing = true\n  try {\n    if (this.processingPromise) {\n      await this.processingPromise\n    }\n    // Re-check needed here (which exists, good\\!)\n  } finally {\n    this._flushing = false\n  }\n}\n```\n\n**The code does re-check** after acquiring the lock (good), but the comment suggests this was intentional. However, the interval timer callback in startBatchTimer() could still race:\n\n```typescript\nthis.batchTimer = setInterval(() =\u003e {\n  if (this._flushing || this.buffer.length === 0 || this.processingPromise) return\n  \n  this._flushing = true  // Race between check and set\n  this.processingPromise = this.processBatches()\n  // ...\n})\n```\n\n**Impact:** Under high load, duplicate batch processing could occur (low probability but possible).\n\n**Fix:** Use a proper atomic check-and-set pattern or ensure all flush paths go through the same mutex.","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:16:27.866653-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:54:31.552589-06:00","closed_at":"2026-02-03T14:54:31.552589-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-4l5g.20","depends_on_id":"parquedb-4l5g","type":"parent-child","created_at":"2026-02-03T12:16:27.867503-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-4l5g.21","title":"MV Full Refresh: atomicReplace isn't atomic","description":"The atomicReplace function in refresh.ts performs a delete-then-move which is NOT atomic:\n\n```typescript\nasync function atomicReplace(\n  storage: StorageBackend,\n  tempPath: string,\n  finalPath: string\n): Promise\u003cvoid\u003e {\n  if (await storage.exists(finalPath)) {\n    await storage.delete(finalPath)  // File deleted here\n  }\n  // WINDOW: Between delete and move, file doesn't exist\n  await storage.move(tempPath, finalPath)  // File restored here\n}\n```\n\n**Problem:** If the process crashes between delete and move, the MV data is lost.\n\n**Impact:** Data loss risk during MV refresh if system fails at wrong moment.\n\n**Fix options:**\n1. Use writeAtomic with overwrite semantics if storage backend supports it\n2. Keep old file until new file is confirmed written (rename old first)\n3. Use a manifest/pointer approach where metadata points to the active file","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:16:38.72455-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:56:17.659556-06:00","closed_at":"2026-02-03T14:56:17.659556-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-4l5g.21","depends_on_id":"parquedb-4l5g","type":"parent-child","created_at":"2026-02-03T12:16:38.725443-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-4l5g.22","title":"MV StreamProcessor: Missing error recovery for onError failures","description":"In StreamProcessor.emitError(), errors from the user's onError callback are silently swallowed:\n\n```typescript\nprivate emitError(error: Error, context: ErrorContext\u003cT\u003e): void {\n  if (this.config.onError) {\n    try {\n      this.config.onError(error, context)\n    } catch {\n      // Ignore errors in error handler\n    }\n  }\n}\n```\n\n**Issues:**\n1. If onError throws, the original error is still processed (good)\n2. But the callback failure is completely invisible (bad for debugging)\n3. No way to know your error handler is broken\n\n**Suggestion:** Log callback failures to console.error at minimum, or provide a separate onCallbackError hook.","status":"closed","priority":3,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:16:48.932039-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T17:00:07.571695-06:00","closed_at":"2026-02-03T17:00:07.571695-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-4l5g.22","depends_on_id":"parquedb-4l5g","type":"parent-child","created_at":"2026-02-03T12:16:48.933152-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-4l5g.23","title":"MV Aggregations: mergeAggregates has incorrect avg logic","description":"In aggregations.ts, the mergeAggregates function for incremental updates has incorrect logic for :\n\n```typescript\n} else if (isAvgExpr(expr)) {\n  // For avg, we need to store count separately - this is a simplified merge\n  // In practice, store _sum and _count fields for proper averaging\n  result[name] = deltaVal // Just take the new value for simplicity\n}\n```\n\n**Problem:** This doesn't actually merge averages - it just takes the new value, which is mathematically incorrect.\n\n**Correct approach:** \n- Store separate sum and count values\n- Merge: newSum = oldSum + deltaSum, newCount = oldCount + deltaCount\n- Final avg = newSum / newCount\n\n**Impact:** Incremental refresh of MVs with  aggregations will produce incorrect results.\n\n**Note:** The comment acknowledges this is wrong ('simplified merge', 'for simplicity') but it's still a production bug.","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:16:58.452078-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:21:01.16939-06:00","closed_at":"2026-02-03T13:21:01.16939-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-4l5g.23","depends_on_id":"parquedb-4l5g","type":"parent-child","created_at":"2026-02-03T12:16:58.45288-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-4l5g.24","title":"MV Tests: E2E tests depend on internal streaming implementation","description":"The E2E tests in tests/e2e/materialized-views.test.ts test the streaming engine in isolation but don't test actual MV behavior through the public API.\n\n**Current tests:**\n- Test StreamingRefreshEngine directly\n- Use mock MVHandler implementations\n- Don't test defineView() -\u003e create -\u003e query flow\n- Don't test actual ParqueDB integration\n\n**Missing test coverage:**\n1. End-to-end defineView() -\u003e MV creation -\u003e data propagation -\u003e query\n2. Staleness detection in real usage\n3. Scheduled refresh via DO alarms (requires DO test harness)\n4. Query routing (queries automatically using MVs)\n5. Error recovery in production scenarios\n\n**Impact:** Tests pass but don't verify the feature works in production use.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:17:08.10402-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T18:24:50.204868-06:00","closed_at":"2026-02-03T18:24:50.204868-06:00","close_reason":"E2E tests have been rewritten to use the public API (ParqueDB, attachMVIntegration, defineView) instead of internal streaming internals (StreamingRefreshEngine, createStreamingRefreshEngine). Tests verified passing: 12/13 confirmed green. The test file now exercises the full insert-\u003eMV propagation-\u003everify flow through the public API surface.","dependencies":[{"issue_id":"parquedb-4l5g.24","depends_on_id":"parquedb-4l5g","type":"parent-child","created_at":"2026-02-03T12:17:08.1047-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-4l5g.3","title":"MV Storage Manager","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:00:13.969508-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:13:46.994702-06:00","closed_at":"2026-02-03T09:13:46.994702-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-4l5g.3","depends_on_id":"parquedb-4l5g","type":"parent-child","created_at":"2026-02-03T09:00:13.970217-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-4l5g.4","title":"Streaming Refresh Engine","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:00:14.670887-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:12:41.184011-06:00","closed_at":"2026-02-03T09:12:41.184011-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-4l5g.4","depends_on_id":"parquedb-4l5g","type":"parent-child","created_at":"2026-02-03T09:00:14.671585-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-4l5g.5","title":"Scheduled Refresh (DO Alarms)","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:00:15.32431-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:14:21.633719-06:00","closed_at":"2026-02-03T09:14:21.633719-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-4l5g.5","depends_on_id":"parquedb-4l5g","type":"parent-child","created_at":"2026-02-03T09:00:15.324906-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-4l5g.6","title":"Incremental Refresh Logic","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:00:16.095871-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:16:25.838265-06:00","closed_at":"2026-02-03T09:16:25.838265-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-4l5g.6","depends_on_id":"parquedb-4l5g","type":"parent-child","created_at":"2026-02-03T09:00:16.096549-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-4l5g.7","title":"Full Refresh Logic","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:00:16.827012-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:14:10.060383-06:00","closed_at":"2026-02-03T09:14:10.060383-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-4l5g.7","depends_on_id":"parquedb-4l5g","type":"parent-child","created_at":"2026-02-03T09:00:16.827579-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-4l5g.8","title":"Staleness Detection","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:00:17.530482-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:13:24.733251-06:00","closed_at":"2026-02-03T09:13:24.733251-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-4l5g.8","depends_on_id":"parquedb-4l5g","type":"parent-child","created_at":"2026-02-03T09:00:17.531055-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-4l5g.9","title":"Query Routing to MVs","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:00:18.190945-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:38:02.961718-06:00","closed_at":"2026-02-03T10:38:02.961718-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-4l5g.9","depends_on_id":"parquedb-4l5g","type":"parent-child","created_at":"2026-02-03T09:00:18.191965-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-4lwa","title":"Refactor long methods over 100 lines","description":"Methods that are too long should be refactored for better maintainability:\n- IcebergBackend.create() - split into prepare/write/commit phases\n- DeltaBackend.bulkCreate() - extract transaction logic\n\nBreak down these methods into smaller, focused functions.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:57:42.510974-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:54:46.081264-06:00","closed_at":"2026-02-03T10:54:46.081264-06:00","close_reason":"Closed"}
{"id":"parquedb-4mwp","title":"Extract shared logic between IcebergBackend and DeltaBackend into BaseEntityBackend class","description":"Code duplication exists between IcebergBackend and DeltaBackend:\n- OCC retry logic is duplicated\n- Parquet read/write patterns are duplicated\n- Entity serialization is duplicated\n\nRefactor to extract common functionality into a BaseEntityBackend class.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:57:38.037185-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:14:52.250271-06:00","closed_at":"2026-02-03T09:14:52.250271-06:00","close_reason":"Closed"}
{"id":"parquedb-4mxb","title":"Consolidate duplicate LRU cache implementations","description":"Two LRU cache implementations exist: LRUMap in src/Collection.ts (lines 110-175) and LRUCache in src/indexes/vector/lru-cache.ts. Consolidate to single implementation in src/utils/.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T16:27:48.909009-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:57:51.254747-06:00","closed_at":"2026-02-03T16:57:51.254747-06:00","close_reason":"Closed"}
{"id":"parquedb-4pk3","title":"Fix catch blocks to use error: unknown","description":"30+ catch blocks use untyped 'catch (error)' despite useUnknownInCatchVariables: true in tsconfig.\n\nUpdate all catch blocks to: catch (error: unknown)\nAdd proper type narrowing with instanceof checks.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T12:42:34.568255-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T12:54:07.444935-06:00","closed_at":"2026-02-01T12:54:07.444935-06:00","close_reason":"Completed by parallel agents"}
{"id":"parquedb-4rve","title":"REFACTOR: Add graceful 404 handling for missing parquet files","description":"## Problem\nWhen a parquet file is missing, the worker throws an unhandled exception causing Error 1101 (500).\n\nThis should return a proper 404 with a helpful error message instead of crashing.\n\n## Current behavior\n```\nError: File not found: onet-graph/occupations.parquet\n  at initializeAsyncBuffer (index.js:17951:11)\n  at async ParquetReader.read (index.js:18002:25)\n  at async QueryExecutor.find (index.js:18704:18)\n  at async handleCollectionList (index.js:20477:18)\n```\n\n## Expected behavior\n```json\n{\n  \"error\": true,\n  \"code\": \"FILE_NOT_FOUND\",\n  \"message\": \"Dataset file not found: onet-graph/occupations.parquet\",\n  \"hint\": \"This dataset may not be uploaded yet. Check R2 bucket contents.\"\n}\n```\nHTTP Status: 404\n\n## Changes needed\n1. Wrap parquet read in try/catch in QueryExecutor.find()\n2. Check if file exists before attempting to read\n3. Return proper 404 response from dataset handlers\n4. Add monitoring/alerting for missing dataset files\n\n## Prevents\n- Cryptic 1101 errors for users\n- Silent failures that go unnoticed\n- Better error messages for debugging","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T02:54:37.400154-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T03:23:26.813968-06:00","closed_at":"2026-02-03T03:23:26.813968-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-4rve","depends_on_id":"parquedb-uqe4","type":"blocks","created_at":"2026-02-03T02:54:47.224423-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-4srx","title":"E2E benchmark suite on deployed workers","description":"Create comprehensive e2e benchmark suite that runs against actually deployed Cloudflare Workers (not local simulation). Should measure:\n- Cold start latency (first request after idle)\n- Warm request latency (subsequent requests)\n- Cache hit vs miss performance\n- Node.js vs Workers comparison\n- Iceberg vs DeltaLake vs proprietary backend comparison\n\nRequires:\n1. Deployed worker with benchmark endpoints\n2. External benchmark runner (GitHub Actions or local script)\n3. Results collection and storage\n4. Regression detection","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:22:36.220089-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:34:49.074624-06:00","closed_at":"2026-02-03T12:34:49.074624-06:00","close_reason":"Closed"}
{"id":"parquedb-4t2","title":"[RED] Link/unlink tests","description":"Write failing tests for $link and $unlink operators","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:35.95815-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T04:45:59.327407-06:00","closed_at":"2026-02-02T04:45:59.327407-06:00","close_reason":"Tests implemented in earlier waves"}
{"id":"parquedb-4t27","title":"Misleading Delete Logic Returns deletedCount: 1 for Non-Existent Entities","description":"**File:** src/ParqueDB/crud.ts (lines 573-584)\n\n**Issue:** The delete operation returns deletedCount: 1 for IDs that 'look valid' (pass ULID format check) even if the entity doesn't actually exist in the database.\n\n**Impact:** \n- Data integrity issues\n- Misleading API responses\n- Clients cannot reliably determine if a delete actually occurred\n\n**Fix:** Remove the heuristic logic that assumes valid-looking IDs exist. Always verify entity existence before returning deletedCount, and return 0 for non-existent entities.\n\n**Priority:** P0 CRITICAL","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:10:14.739907-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T17:59:08.255347-06:00","closed_at":"2026-02-03T17:59:08.255347-06:00","close_reason":"Closed"}
{"id":"parquedb-4ugt","title":"Missing array validation for $in operator in predicate pushdown","description":"## Location\nsrc/query/predicate.ts, lines 203-208\n\n## Problem\nThe predicate pushdown code for \\$in operator casts op.\\$in to unknown[] and calls .some() without validating it's actually an array. If \\$in is not an array (e.g., null, string), this will throw a TypeError.\n\nCurrent code:\n```typescript\nif ('\\$in' in op) {\n  const values = op.\\$in as unknown[]\n  return values.some(v =\u003e {  // TypeError if not an array\n    if (v === null) return nullCount \u003e 0\n    return isValueInRange(v, min, max)\n  })\n}\n```\n\n## Fix\nAdd array validation:\n```typescript\nif ('\\$in' in op) {\n  const values = op.\\$in\n  if (!Array.isArray(values)) return true  // Can't prune, assume could match\n  return values.some(v =\u003e {\n    if (v === null) return nullCount \u003e 0\n    return isValueInRange(v, min, max)\n  })\n}\n```\n\n## Impact\n- Could cause runtime TypeError on malformed queries\n- Predicate pushdown is an optimization, so failing safely (returning true) is correct","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:36:33.625646-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:38:19.668816-06:00","closed_at":"2026-02-03T11:38:19.668816-06:00","close_reason":"Closed"}
{"id":"parquedb-4vyr","title":"Refactor: Query - Remove dead code in update.ts wrapper","description":"## Summary\nThe `src/query/update.ts` file is a thin wrapper around `src/mutation/operators.ts` but contains some redundant code.\n\n### Issues Found\n\n1. **Duplicate require() call** (line 108-109):\n   ```typescript\n   export function sortArray(...) {\n     const { compareValues } = require('../utils')\n   ```\n   This uses CommonJS require() inside a function despite compareValues already being imported at the top via ES6 imports (line 31). Should use the imported version.\n\n2. **Re-exported functions that could be direct re-exports** (lines 27-34):\n   ```typescript\n   export { getField, setField, unsetField } from '../mutation/operators'\n   export { compareValues, deepEqual } from '../utils'\n   export { matchesFilter } from './filter'\n   ```\n   These are fine, but combined with the wrapper functions, makes the module's purpose unclear.\n\n3. **sortArray and applySlice duplicated** (lines 108-149):\n   These are exact copies of functions in `src/mutation/operators.ts` (lines 519-554). Comment says 'for backwards compatibility' but both modules are internal.\n\n### Files to Update\n- `src/query/update.ts` - Clean up duplicates\n- Check for external usages before removing\n\n### Acceptance Criteria\n- [ ] Replace require() with import\n- [ ] Remove duplicate sortArray/applySlice if not used externally\n- [ ] Clarify module purpose in JSDoc\n- [ ] Add deprecation notice if functions should not be used directly","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:33:35.599123-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:21:21.976365-06:00","closed_at":"2026-02-03T11:21:21.976365-06:00","close_reason":"Closed","labels":["refactor"]}
{"id":"parquedb-4y78","title":"Inconsistent Async/Await Patterns in DatabaseIndexDO","description":"**File:** src/worker/DatabaseIndexDO.ts (lines 210-231)\n\n**Issue:** Methods marked async but don't use await\n\n**Fix:** Remove async keyword or add Promise.resolve() wrapper\n\n**Impact:** Code clarity and potential confusion about synchronous vs asynchronous behavior","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:10:34.222398-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T18:22:33.595878-06:00","closed_at":"2026-02-03T18:22:33.595878-06:00","close_reason":"Fixed in commit ab28519"}
{"id":"parquedb-4zjc","title":"Add memory limits to global entity store","description":"The in-memory globalEntityStore has no maximum size limit. Implement LRU cache eviction or configurable memory limits.","status":"open","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:00.33561-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:00.33561-06:00"}
{"id":"parquedb-54rb","title":"Add unit tests for search worker","description":"Zero unit tests for snippets/worker/search.ts. Need tests for:\n- loadDataset() and caching\n- searchData() text filtering\n- Individual handlers\n- Query parameter parsing\n- Pagination logic\n- Error handling","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:27:44.554061-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:39:49.954697-06:00","closed_at":"2026-02-03T12:39:49.954697-06:00","close_reason":"Closed"}
{"id":"parquedb-56ls","title":"Refactor: Storage - validatePath and validateData are unused","description":"validation.ts exports validatePath and validateData functions that are never used:\\n\\n```typescript\\nexport function validatePath(path: string, operation: string): void {\\n  if (path === undefined || path === null) {\\n    throw new Error(\\`${operation}: path is required\\`)\\n  }\\n}\\n\\nexport function validateData(data: Uint8Array | null | undefined, operation: string): void {\\n  if (data === null || data === undefined) {\\n    throw new Error(\\`${operation}: data is required\\`)\\n  }\\n}\\n```\\n\\nOnly validateRange and validatePartNumber are actually imported and used by backends.\\n\\nRefactor to:\\n- Either remove unused functions\\n- Or add validation calls to all backend methods for consistency\\n- Consider if TypeScript's type checking makes these redundant\\n\\nFiles affected:\\n- /Users/nathanclevenger/projects/parquedb/src/storage/validation.ts","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:34:00.931069-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:10:49.052791-06:00","closed_at":"2026-02-03T11:10:49.052791-06:00","close_reason":"Closed"}
{"id":"parquedb-58am","title":"Fix: Race condition in batch processing","description":"scheduleFlush uses setTimeout without proper locking. Multiple flush operations can overlap causing duplicate event processing. Add mutex or processing lock. File: src/materialized-views/streaming.ts lines 80-95","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:15:16.835534-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:18:30.749777-06:00","closed_at":"2026-02-03T10:18:30.749777-06:00","close_reason":"Closed"}
{"id":"parquedb-5ay","title":"[RED] Sorting tests","description":"Write failing tests for single and multi-field sorting","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:27.87617-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T04:45:59.583335-06:00","closed_at":"2026-02-02T04:45:59.583335-06:00","close_reason":"Tests implemented in earlier waves"}
{"id":"parquedb-5cm6","title":"Remove MockParqueDBDO, use vitest-pool-workers","description":"## Task\n\nRemove the MockSqlStorage and MockParqueDBDO from tests in favor of real DO testing via vitest-pool-workers.\n\n## Current State\n- `tests/unit/worker/ParqueDBDO.test.ts` has ~600 lines of mock code\n- Mocks can drift from real implementation\n- vitest-pool-workers provides real DO SQLite support\n\n## Changes\n1. Move DO tests to `tests/e2e/worker/` or keep in unit but use pool-workers\n2. Remove MockSqlStorage class\n3. Remove MockParqueDBDO class  \n4. Use real ParqueDBDO with real ctx.storage.sql\n5. Update vitest.workspace.ts if needed\n\n## Benefits\n- Tests match real behavior\n- Less code to maintain\n- Catch real SQLite edge cases","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:13:13.255853-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:11:30.215657-06:00","closed_at":"2026-02-03T07:11:30.215657-06:00","close_reason":"Implemented"}
{"id":"parquedb-5cn","title":"Implement time-travel queries","description":"Query entities as of a specific timestamp","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:38.846951-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:20:47.034689-06:00","closed_at":"2026-02-01T14:20:47.034689-06:00","close_reason":"Closed"}
{"id":"parquedb-5cv3","title":"Add: R2/S3 storage backend MV tests","description":"All storage tests use MemoryBackend. Add tests verifying MV operations work with R2Backend or S3Backend (range reads, concurrent writes).","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:15:57.274076-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:32:42.878251-06:00","closed_at":"2026-02-03T10:32:42.878251-06:00","close_reason":"MV tests for R2/S3 backends already exist in tests/integration/mv-storage-backend.test.ts with 55 comprehensive tests covering: basic CRUD, view data operations, stats, error handling, manifest operations, view lifecycle, prefix isolation, S3-compatible backend, consistency with MemoryBackend, multipart uploads, persistence/recovery, range read edge cases, and concurrent writes. Fixed one flaky test with race condition."}
{"id":"parquedb-5d0f","title":"Split worker/index.ts into modules","description":"worker/index.ts is 1500+ lines containing HTTP routing, response building, benchmark handlers, and business logic. Extract into: worker/routing.ts, worker/responses.ts, worker/benchmarks.ts","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T17:01:04.690231-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T17:24:44.548522-06:00","closed_at":"2026-02-01T17:24:44.548522-06:00","close_reason":"Closed"}
{"id":"parquedb-5dt0","title":"Remove @ts-ignore comments and fix underlying type issues","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:31.563403-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:40:02.028953-06:00","closed_at":"2026-02-03T10:40:02.028953-06:00","close_reason":"Closed"}
{"id":"parquedb-5eb","title":"[GREEN] IceType integration implementation","description":"Implement IceType integration to pass tests","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:52:08.693793-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:19:33.314581-06:00","closed_at":"2026-02-01T14:19:33.314581-06:00","close_reason":"Closed"}
{"id":"parquedb-5eh7","title":"Unify Node.js/Workers storage behavior","description":"ParqueDBImpl (Node.js) and ParqueDBDO (Workers) have divergent implementations. Create unified EventSourcedBackend wrapper that both can use.","status":"open","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:10.577861-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:10.577861-06:00"}
{"id":"parquedb-5fk","title":"Implement create operations","description":"Create entities with validation and audit fields","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:13.460768-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:05:20.598416-06:00","closed_at":"2026-02-01T14:05:20.598416-06:00","close_reason":"Closed"}
{"id":"parquedb-5g35","title":"Refactor: Query - Unknown operators are silently ignored","description":"## Summary\nThe filter evaluation silently ignores unknown operators instead of throwing an error. This can lead to subtle bugs where filters don't work as expected.\n\n### Current Behavior\n\nIn `src/query/filter.ts` line 322-324:\n```typescript\ndefault:\n  // Unknown operator - ignore\n  break\n```\n\nAnd for primitives (line 95-97):\n```typescript\ndefault:\n  // Unknown operator for primitives - ignore\n  break\n```\n\n### Problem\nIf you typo an operator:\n```typescript\n{ score: { $gtr: 100 } }  // Typo: $gtr instead of $gt\n```\nThis silently matches everything since the unknown operator is ignored.\n\n### Proposed Solution\n\n1. **Strict mode** (opt-in):\n   ```typescript\n   matchesFilter(doc, filter, { strict: true })\n   // Throws on unknown operators\n   ```\n\n2. **Warning mode** (default):\n   ```typescript\n   // Log warning but continue\n   console.warn('Unknown filter operator: $gtr')\n   ```\n\n3. **Validation function**:\n   ```typescript\n   validateFilter(filter)  // Throws if invalid\n   ```\n\n### Files to Update\n- `src/query/filter.ts` - Add strict mode option\n- `src/types/options.ts` - Add FilterOptions type\n\n### Acceptance Criteria\n- [ ] Add strict mode option to matchesFilter\n- [ ] Add validateFilter() function\n- [ ] Add tests for unknown operator handling\n- [ ] Document strict mode in JSDoc","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:34:07.941444-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:53:34.721715-06:00","closed_at":"2026-02-03T07:53:34.721715-06:00","close_reason":"Closed","labels":["refactor"]}
{"id":"parquedb-5him","title":"Refactor: Storage - Add StreamableBackend implementation","description":"The StorageBackend interface defines StreamableBackend at lines 232-243 of types/storage.ts:\\n\\n```typescript\\nexport interface StreamableBackend extends StorageBackend {\\n  createReadStream(path: string, options?: StreamOptions): ReadableStream\u003cUint8Array\u003e\\n  createWriteStream(path: string, options?: WriteOptions): WritableStream\u003cUint8Array\u003e\\n}\\n```\\n\\nHowever, no backend currently implements this interface. This would be valuable for:\\n- Large file handling without loading entire file in memory\\n- Progressive reads for Parquet file parsing\\n- Efficient uploads/downloads\\n\\nConsider implementing StreamableBackend for:\\n- FsBackend using Node.js streams\\n- R2Backend using R2's streaming APIs\\n\\nFiles affected:\\n- /Users/nathanclevenger/projects/parquedb/src/storage/FsBackend.ts\\n- /Users/nathanclevenger/projects/parquedb/src/storage/R2Backend.ts\\n- /Users/nathanclevenger/projects/parquedb/src/types/storage.ts","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:33:18.874841-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:25:26.874855-06:00","closed_at":"2026-02-03T11:25:26.874855-06:00","close_reason":"Closed"}
{"id":"parquedb-5ir6","title":"Fix: Async operations without error boundaries","description":"WorkerLogsMV setInterval flush has no .catch() - unhandled promise rejection. File: src/streaming/worker-logs.ts","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:15:36.942078-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:32:51.823415-06:00","closed_at":"2026-02-03T10:32:51.823415-06:00","close_reason":"Closed"}
{"id":"parquedb-5lz3","title":"Inconsistent Error Types Delta vs Iceberg","description":"**Files:** delta-commit.ts, iceberg-commit.ts\n\n**Issue:** Delta uses AlreadyExistsError, Iceberg uses isETagMismatchError\n\n**Fix:** Standardize error handling patterns","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T14:34:36.220631-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:39:30.733322-06:00","closed_at":"2026-02-03T14:39:30.733322-06:00","close_reason":"Closed"}
{"id":"parquedb-5mdf","title":"Standardize generic property naming (rows vs items)","description":"Query-related types use inconsistent property names:\n\n```typescript\nexport interface QueryResult\u003cT\u003e { rows: T[] }\nexport type PaginatedResult\u003cT\u003e { items: T[] }\n```\n\nStandardize to one convention (prefer 'items' for consistency with pagination terminology).","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:32:08.934761-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:07:10.817089-06:00","closed_at":"2026-02-03T10:07:10.817089-06:00","close_reason":"Closed"}
{"id":"parquedb-5muc","title":"IndexCatalog type guard isIndexCatalog lacks deep validation","description":"File: src/indexes/manager.ts:1065-1071\n\nThe isIndexCatalog type guard only performs shallow validation:\n- Checks version is a number\n- Checks indexes is a record (object)\n\nBut it does NOT validate:\n- That each namespace entry in indexes is an array\n- That each array element has required 'definition' and 'metadata' properties\n- That definition has required fields (name, type, fields)\n- That metadata has required fields (createdAt, updatedAt, etc.)\n\nThis allows invalid catalog data to pass the type guard, potentially causing runtime errors in loadCatalog() when accessing undefined properties.\n\nFix: Add deep validation or create a separate validateIndexCatalog function that throws descriptive errors.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:11.620354-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:37:34.139156-06:00","closed_at":"2026-02-03T11:37:34.139156-06:00","close_reason":"Closed"}
{"id":"parquedb-5nov","title":"Refactor: Query - Add  operator support at field level","description":"## Summary\nThe filter system supports $not at the top level for negating entire sub-filters, but doesn't support field-level $not operator like MongoDB.\n\n### Current Behavior\nTop-level $not works (line 121-125):\n```typescript\n{ $not: { status: 'published' } }  // Works - negates entire filter\n```\n\nField-level $not doesn't work:\n```typescript\n{ status: { $not: { $regex: '^draft' } } }  // Not supported\n```\n\n### MongoDB Behavior\nMongoDB supports both:\n- Top-level: `{ $not: { ... } }`\n- Field-level: `{ field: { $not: { operator: value } } }`\n\nField-level is useful for negating specific operator results:\n```javascript\n// Find users whose name doesn't start with 'admin'\n{ name: { $not: { $regex: '^admin' } } }\n\n// Find items where score is NOT greater than 100\n{ score: { $not: { $gt: 100 } } }\n```\n\n### Files to Update\n- `src/types/filter.ts` - Add FieldNotOperator type\n- `src/query/filter.ts` - Add field-level $not evaluation\n- `src/query/builder.ts` - Add notWhere() method\n\n### Acceptance Criteria\n- [ ] Add $not to FieldOperator union type\n- [ ] Handle $not in evaluateOperators()\n- [ ] Add QueryBuilder support\n- [ ] Add comprehensive tests\n- [ ] Document difference from top-level $not","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:33:46.042525-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:08:38.28685-06:00","closed_at":"2026-02-03T11:08:38.28685-06:00","close_reason":"Closed","labels":["refactor"]}
{"id":"parquedb-5qpd","title":"Studio: Add quick database switcher (cmd+k)","status":"closed","priority":3,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:36.805767-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:56:25.997634-06:00","closed_at":"2026-02-03T16:56:25.997634-06:00","close_reason":"Closed"}
{"id":"parquedb-5uej","title":"Fix race condition in batch-loader flush()","description":"Race condition in RelationshipBatchLoader.flush() - pending requests can be added during flush causing missed batching","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:09:38.321284-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:15:30.228668-06:00","closed_at":"2026-02-03T09:15:30.228668-06:00","close_reason":"Fixed by parallel subagents"}
{"id":"parquedb-5uyz","title":"Add actual MCP tool invocation tests","description":"Testing: MCP tests only test underlying DB operations, not actual MCP protocol. Add tests for tool registration, invocation, and output format","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:09:47.037646-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:15:30.352745-06:00","closed_at":"2026-02-03T09:15:30.352745-06:00","close_reason":"Fixed by parallel subagents"}
{"id":"parquedb-5vmc","title":"Missing type validation for $expr operator","description":"## Location\nsrc/query/filter.ts, line 194\n\n## Problem\nThe \\$expr operator handler doesn't validate that filter.\\$expr is a valid object before passing it to evaluateExpr(). If filter.\\$expr is not an object (e.g., a string, number, or null), the evaluateExpr function will fail.\n\nCurrent code:\n```typescript\nif (filter.\\$expr) {\n  if (!evaluateExpr(obj, filter.\\$expr)) {  // No type validation\n    return false\n  }\n}\n```\n\nWhile evaluateExpr does handle malformed expressions by returning false, it should validate upfront that \\$expr is an object to fail fast and provide clearer error behavior.\n\n## Fix\nAdd type validation:\n```typescript\nif (filter.\\$expr) {\n  if (typeof filter.\\$expr !== 'object' || filter.\\$expr === null) {\n    return false  // Invalid \\$expr format\n  }\n  if (!evaluateExpr(obj, filter.\\$expr)) {\n    return false\n  }\n}\n```\n\n## Impact\n- Currently handled gracefully inside evaluateExpr (returns false for invalid format)\n- Adding explicit check improves code clarity and catches edge cases earlier","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:36:16.421141-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:38:19.528991-06:00","closed_at":"2026-02-03T11:38:19.528991-06:00","close_reason":"Closed"}
{"id":"parquedb-5w6h","title":"Fix: Silent data loss on Parquet write failure","description":"stream-processor.ts writeParquet() clears buffer after all retries fail without notifying callers. Emit error event or throw exception. Consider dead-letter queue. File: src/materialized-views/stream-processor.ts lines 200-230","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:15:20.279469-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:38:02.938564-06:00","closed_at":"2026-02-03T10:38:02.938564-06:00","close_reason":"Closed"}
{"id":"parquedb-60t","title":"[RED] Create operation tests","description":"Write failing tests for create, createMany with validation","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:15.052836-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:05:20.627398-06:00","closed_at":"2026-02-01T14:05:20.627398-06:00","close_reason":"Closed"}
{"id":"parquedb-60zr","title":"Extract duplicate code from Iceberg/Base backends","description":"IcebergBackend and BaseEntityBackend have duplicate implementations of applyUpdate, sortEntities, applyPagination, createDefaultEntity. Extract to shared utilities.","status":"open","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:23.244646-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:23.244646-06:00"}
{"id":"parquedb-61e4","title":"Missing Error Threshold in Hierarchical Compaction","description":"**File:** src/workflows/hierarchical-compaction.ts (lines 453-479)\n\n**Issue:** The compaction workflow continues processing with partial data if some files fail to read. There is no error threshold to abort the compaction when too many files fail.\n\n**Impact:** \n- Data loss risk if source files are deleted after a partial compaction completes\n- Silent data corruption\n- Inconsistent database state\n\n**Fix:** \n1. Add configurable error threshold (e.g., abort if \u003e5% of files fail)\n2. Abort compaction if threshold exceeded\n3. Log detailed error information for failed files\n4. Ensure source files are only deleted after successful full compaction\n\n**Priority:** P0 CRITICAL","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:10:17.771972-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T17:17:35.078936-06:00","closed_at":"2026-02-03T17:17:35.078936-06:00","close_reason":"Closed"}
{"id":"parquedb-63f","title":"Write: Getting Started Guide","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T12:40:28.321653-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:34:35.131018-06:00","closed_at":"2026-02-01T14:34:35.131018-06:00","close_reason":"Closed"}
{"id":"parquedb-63r","title":"[GREEN] FTS implementation","description":"Implement FTS to pass tests","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:55.798637-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:27:18.777573-06:00","closed_at":"2026-02-01T14:27:18.777573-06:00","close_reason":"Closed"}
{"id":"parquedb-64nv","title":"Epic: Git Integration with Database Branching/Merging","description":"Add git-native database branching and merging support to ParqueDB, enabling developers to branch database state alongside code, merge with automatic conflict resolution for commutative operations, and integrate with GitHub Actions/Apps for automated workflows.\n\n## Goals\n- Database branches that mirror git branches\n- Event-based merging that preserves operation intent\n- Automatic conflict resolution for commutative operations ($inc, $addToSet)\n- Git hooks for seamless local workflow\n- GitHub Actions for CI/CD database workflows\n- GitHub App for zero-config integration\n\n## Key Design Decisions\n- Events are the unit of merge (not final state)\n- Commits reference event log positions + file checksums\n- Refs stored in _meta/refs/heads/* (like git)\n- Merge conflicts only for truly non-commutative operations\n\n## Architecture\n```\n_meta/\n HEAD                    # Current branch ref\n refs/heads/*            # Branch pointers  commit hashes\n commits/*.json          # Commit objects\n MERGE_STATE             # In-progress merge tracking\n manifest.json           # Sync manifest\n\nevents/\n seg-*.parquet           # Event segments (content-addressed)\n current.parquet         # Active segment\n manifest.json           # Segment index\n```\n\n## Success Criteria\n- [ ] Can create/switch/delete database branches via CLI\n- [ ] Database branch auto-syncs with git branch on checkout\n- [ ] Merges interleaved events with field-level conflict detection\n- [ ] $inc and $addToSet operations auto-merge (commutative)\n- [ ] GitHub Action creates preview databases for PRs\n- [ ] GitHub App provides merge checks and PR comments","status":"open","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:52:10.889462-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:52:10.889462-06:00"}
{"id":"parquedb-64nv.1","title":"Implement DatabaseCommit and branch refs model","description":"Create the core data structures for database commits and branch references.\n\n## Files to Create\n- `src/sync/commit.ts` - DatabaseCommit type and serialization\n- `src/sync/refs.ts` - Branch/tag reference management\n- `src/sync/hash.ts` - Content hashing utilities\n\n## DatabaseCommit Structure\n```typescript\ninterface DatabaseCommit {\n  hash: string                    // SHA256 of commit contents\n  parents: string[]               // Parent commit(s)\n  timestamp: number\n  author: string\n  message: string\n  state: {\n    collections: Record\u003cstring, {\n      dataHash: string\n      schemaHash: string\n      rowCount: number\n    }\u003e\n    relationships: {\n      forwardHash: string\n      reverseHash: string\n    }\n    eventLogPosition: {\n      segmentId: string\n      offset: number\n    }\n  }\n}\n```\n\n## Refs Structure\n```\n_meta/\n HEAD                 # 'refs/heads/main'\n refs/\n    heads/\n       main        #  commit hash\n       feature/x   #  commit hash\n    tags/\n        v1.0.0      #  commit hash\n commits/\n     {hash}.json     # Commit objects\n```\n\n## API\n```typescript\n// Commit operations\nfunction createCommit(state: DatabaseState, opts: CommitOptions): Promise\u003cDatabaseCommit\u003e\nfunction loadCommit(hash: string): Promise\u003cDatabaseCommit\u003e\nfunction hashCommit(commit: DatabaseCommit): string\n\n// Ref operations\nfunction resolveRef(ref: string): Promise\u003cstring\u003e  // Returns commit hash\nfunction updateRef(ref: string, hash: string): Promise\u003cvoid\u003e\nfunction listRefs(type?: 'heads' | 'tags'): Promise\u003cstring[]\u003e\nfunction deleteRef(ref: string): Promise\u003cvoid\u003e\n```\n\n## Tests\n- Commit hash is deterministic (same content = same hash)\n- Refs resolve through HEAD correctly\n- Parent references form valid DAG","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:52:28.06268-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:00:57.607799-06:00","closed_at":"2026-02-03T08:00:57.607799-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-64nv.1","depends_on_id":"parquedb-64nv","type":"parent-child","created_at":"2026-02-03T07:52:28.063829-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-64nv.10","title":"Integrate gitx.do as Git-Native Backend","description":"Integrate gitx.do project to provide battle-tested git semantics for ParqueDB branching/merging.\n\n## Why gitx.do?\n\ngitx.do is a **complete Git protocol implementation for Cloudflare Workers** that stores git objects in **R2 Parquet with VARIANT encoding** - the exact same tech stack as ParqueDB!\n\n### Shared Technology\n- `hyparquet` / `hyparquet-writer` - Same Parquet libraries\n- `VARIANT` encoding - Same flexible data format\n- `Durable Objects` - Same coordination layer\n- `R2` - Same object storage\n- `Iceberg/Delta Lake` - Same table formats\n\n### gitx Capabilities We Can Leverage\n- **Full git protocol**: pack files, delta compression, smart HTTP\n- **Three-way merge**: 2344 lines of battle-tested merge logic\n- **Branch operations**: 1487 lines of branch management\n- **Content-addressed storage**: SHA-based deduplication\n- **MCP integration**: AI agent tools ready to use\n\n## Integration Architecture\n\n### Option A: GitEntityBackend (Recommended)\n```typescript\n// New backend alongside Delta/Iceberg\nimport { GitModule } from 'gitx.do/do'\n\nexport class GitEntityBackend implements EntityBackend {\n  private git: GitModule\n  \n  async createBranch(name: string): Promise\u003cvoid\u003e {\n    await this.git.branch({ name, startPoint: 'main' })\n  }\n  \n  async merge(source: string, target: string): Promise\u003cMergeResult\u003e {\n    return this.git.merge(source, target)\n  }\n  \n  async checkout(ref: string): Promise\u003cvoid\u003e {\n    await this.git.checkout(ref)\n  }\n}\n```\n\n### Option B: withGit() Mixin for ParqueDBDO\n```typescript\nimport { withGit } from 'gitx.do/do'\n\n@withGit({ repo: 'parquedb-data' })\nclass ParqueDBDO extends DurableObject {\n  // Git methods automatically available\n  // this.git.branch(), this.git.merge(), etc.\n}\n```\n\n### Option C: Shared Storage Layer\nBoth gitx and ParqueDB write to same Parquet files:\n```\ndata/\n objects/*.parquet     # Git objects (gitx format)\n refs.parquet          # Git refs\n {ns}/data.parquet     # ParqueDB entities\n events/*.parquet      # ParqueDB events\n```\n\n## Key Benefits\n\n1. **Battle-tested merge logic** - Don't reinvent three-way merge\n2. **Git CLI compatible** - `git clone`, `git push` work out of box\n3. **Queryable history** - DuckDB queries on git history\n4. **MCP tools** - AI agents can branch/merge via MCP\n5. **Compression** - Database-optimized packing (10-20x improvement)\n\n## Implementation Phases\n\n### Phase 1: Shared Types\n- Import gitx types for commits, refs, objects\n- Align ParqueDB commit format with git commit\n\n### Phase 2: GitEntityBackend  \n- Implement EntityBackend interface using gitx\n- Map entity operations to git objects\n- Use gitx refs for branch tracking\n\n### Phase 3: Merge Integration\n- Use gitx three-way merge for entity conflicts\n- Map git conflicts to ParqueDB EventConflict\n- Leverage gitx resolution strategies\n\n### Phase 4: CLI Integration\n- `parquedb git clone` / `parquedb git push`\n- Direct interop with standard git clients\n- `git log` shows ParqueDB commits\n\n## Files to Create\n- src/backends/git.ts - GitEntityBackend\n- src/git/gitx-adapter.ts - Adapter for gitx module\n- src/git/conflict-mapper.ts - Map gitParqueDB conflicts\n\n## Acceptance Criteria\n- [ ] Can use gitx for branch/merge operations\n- [ ] Standard git clients can clone/push ParqueDB repos\n- [ ] Three-way merge resolves entity conflicts\n- [ ] DuckDB can query ParqueDB history via git objects","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:12:01.341253-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:29:01.449962-06:00","closed_at":"2026-02-03T08:29:01.449962-06:00","close_reason":"Deferring gitx integration to post-launch. ParqueDB's native branching is sufficient for v1. May reconsider integration after launch to add standard git protocol support.","dependencies":[{"issue_id":"parquedb-64nv.10","depends_on_id":"parquedb-64nv","type":"parent-child","created_at":"2026-02-03T08:12:01.34219-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-64nv.10","depends_on_id":"parquedb-64nv.3","type":"blocks","created_at":"2026-02-03T08:12:08.883627-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-64nv.10","depends_on_id":"parquedb-64nv.8","type":"blocks","created_at":"2026-02-03T08:12:08.997145-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-64nv.2","title":"Add CLI commands for database branching","description":"Implement CLI commands for creating, switching, listing, and deleting database branches.\n\n## Commands to Implement\n\n### parquedb branch\n```bash\nparquedb branch                      # List branches (* = current)\nparquedb branch \u003cname\u003e               # Create branch from current\nparquedb branch \u003cname\u003e \u003cbase\u003e        # Create from specific base\nparquedb branch -d \u003cname\u003e            # Delete branch\nparquedb branch -m \u003cold\u003e \u003cnew\u003e       # Rename branch\nparquedb branch --remote             # List remote branches\n```\n\n### parquedb checkout\n```bash\nparquedb checkout \u003cbranch\u003e           # Switch to branch\nparquedb checkout -b \u003cbranch\u003e        # Create and switch\nparquedb checkout --from-git         # Sync to current git branch\nparquedb checkout \u003ccommit\u003e           # Detached HEAD at commit\n```\n\n### parquedb log\n```bash\nparquedb log                         # Show commit history\nparquedb log --oneline               # Compact format\nparquedb log \u003cbranch\u003e                # History of specific branch\nparquedb log --graph                 # ASCII graph of branches\n```\n\n## Files to Create/Modify\n- `src/cli/commands/branch.ts` - branch command\n- `src/cli/commands/checkout.ts` - checkout command  \n- `src/cli/commands/log.ts` - log command\n- `src/sync/branch-manager.ts` - BranchManager class\n- `src/cli/index.ts` - Register new commands\n\n## BranchManager API\n```typescript\nclass BranchManager {\n  current(): Promise\u003cstring\u003e\n  list(): Promise\u003cBranchInfo[]\u003e\n  create(name: string, opts?: { from?: string }): Promise\u003cvoid\u003e\n  delete(name: string, opts?: { force?: boolean }): Promise\u003cvoid\u003e\n  rename(oldName: string, newName: string): Promise\u003cvoid\u003e\n  exists(name: string): Promise\u003cboolean\u003e\n}\n```\n\n## Acceptance Criteria\n- [ ] Can create branch from current HEAD\n- [ ] Can switch branches (reconstructs state)\n- [ ] Warns on uncommitted changes during checkout\n- [ ] Can delete merged branches\n- [ ] Prevents deleting unmerged branches without --force","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:52:37.154048-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:08:18.620012-06:00","closed_at":"2026-02-03T08:08:18.620012-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-64nv.2","depends_on_id":"parquedb-64nv","type":"parent-child","created_at":"2026-02-03T07:52:37.154827-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-64nv.2","depends_on_id":"parquedb-64nv.1","type":"blocks","created_at":"2026-02-03T07:53:54.369572-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-64nv.3","title":"Implement event-based merge engine","description":"Build the core merge engine that merges event streams rather than final states, enabling automatic resolution of commutative operations.\n\n## Key Insight\nEvents preserve operation intent. Merging events allows:\n- $inc operations to combine (10 + 5 = 15, not conflict)\n- $addToSet operations to union\n- Different fields on same entity to auto-merge\n- Timestamp-based deterministic ordering\n\n## Files to Create\n- `src/sync/event-merge.ts` - Core merge algorithm\n- `src/sync/conflict-detection.ts` - Conflict identification\n- `src/sync/commutative-ops.ts` - Operation commutativity rules\n- `src/sync/conflict-resolution.ts` - Resolution strategies\n\n## Merge Algorithm\n```typescript\nasync function mergeEventStreams(\n  baseEvents: Event[],\n  ourEvents: Event[],\n  theirEvents: Event[]\n): Promise\u003cEventMergeResult\u003e {\n  // 1. Group events by entity\n  // 2. For each entity with changes in both branches:\n  //    a. Interleave events by timestamp\n  //    b. Check for field-level conflicts\n  //    c. Auto-merge commutative operations\n  //    d. Flag non-commutative conflicts\n  // 3. Return merged stream + conflicts\n}\n```\n\n## Commutative Operations (Auto-Merge)\n- $inc: Additions combine (a + b = b + a)\n- $addToSet: Set union (order doesn't matter)\n- $set on DIFFERENT fields: No conflict\n- $unset on DIFFERENT fields: No conflict\n\n## Non-Commutative (Conflict)\n- $set on SAME field with different values\n- $push (array order matters)\n- DELETE vs UPDATE on same entity\n- CREATE with same ID in both branches\n\n## Conflict Resolution Strategies\n```typescript\ntype Strategy = 'ours' | 'theirs' | 'newest' | 'oldest' | 'both' | 'manual'\n```\n\n## Tests\n- $inc operations on same field combine correctly\n- $set on different fields auto-merges\n- $set on same field with same value = no conflict\n- $set on same field with different values = conflict\n- Interleaved timestamps produce deterministic order\n- Three-way merge finds correct common ancestor","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:52:47.572899-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:08:18.372899-06:00","closed_at":"2026-02-03T08:08:18.372899-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-64nv.3","depends_on_id":"parquedb-64nv","type":"parent-child","created_at":"2026-02-03T07:52:47.573737-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-64nv.3","depends_on_id":"parquedb-64nv.1","type":"blocks","created_at":"2026-02-03T07:53:54.488147-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-64nv.4","title":"Add CLI commands for merge and conflict resolution","description":"Implement CLI commands for merging branches and resolving conflicts.\n\n## Commands to Implement\n\n### parquedb merge\n```bash\nparquedb merge \u003csource\u003e              # Merge source into current\nparquedb merge --from-git            # Merge based on git state\nparquedb merge --strategy \u003cs\u003e        # Auto-resolve with strategy\nparquedb merge --dry-run             # Preview merge without applying\nparquedb merge --abort               # Abort in-progress merge\nparquedb merge --continue            # Continue after resolving conflicts\n```\n\n### parquedb diff\n```bash\nparquedb diff                        # Uncommitted changes\nparquedb diff \u003cbranch\u003e               # Compare current to branch\nparquedb diff \u003ca\u003e..\u003cb\u003e               # Compare two branches\nparquedb diff --stat                 # Summary only\nparquedb diff --events               # Show event-level diff\n```\n\n### parquedb conflicts\n```bash\nparquedb conflicts                   # List all conflicts\nparquedb conflicts show \u003centity\u003e     # Show conflict details\n```\n\n### parquedb resolve\n```bash\nparquedb resolve \u003centity\u003e --ours     # Accept current branch\nparquedb resolve \u003centity\u003e --theirs   # Accept incoming branch\nparquedb resolve \u003centity\u003e --newest   # Accept most recent\nparquedb resolve \u003centity\u003e --edit     # Open editor for manual\nparquedb resolve --all --strategy \u003cs\u003e # Resolve all with strategy\n```\n\n## MERGE_STATE File\n```typescript\ninterface MergeState {\n  status: 'in_progress' | 'conflicted' | 'resolved'\n  source: string\n  target: string\n  baseCommit: string\n  conflicts: Array\u003c{\n    entityId: string\n    fields: string[]\n    resolved: boolean\n    resolution?: 'ours' | 'theirs' | 'manual'\n  }\u003e\n}\n```\n\n## Files to Create/Modify\n- `src/cli/commands/merge.ts`\n- `src/cli/commands/diff.ts`\n- `src/cli/commands/conflicts.ts`\n- `src/cli/commands/resolve.ts`\n- `src/sync/merge-state.ts`\n\n## Acceptance Criteria\n- [ ] Clean merges complete automatically\n- [ ] Conflicts halt merge and list issues\n- [ ] Can resolve conflicts individually or in bulk\n- [ ] --continue completes merge after resolution\n- [ ] --abort restores pre-merge state\n- [ ] Diff shows entity and event-level changes","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:52:57.912114-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:15:26.488491-06:00","closed_at":"2026-02-03T08:15:26.488491-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-64nv.4","depends_on_id":"parquedb-64nv","type":"parent-child","created_at":"2026-02-03T07:52:57.912795-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-64nv.4","depends_on_id":"parquedb-64nv.3","type":"blocks","created_at":"2026-02-03T07:53:54.60578-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-64nv.4","depends_on_id":"parquedb-64nv.2","type":"blocks","created_at":"2026-02-03T07:53:54.725057-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-64nv.5","title":"Implement git hooks for automatic branch sync","description":"Add git hooks that automatically sync database branches with git operations.\n\n## Hooks to Implement\n\n### post-checkout\nTriggered after: git checkout, git switch, git clone\n```bash\n#!/bin/bash\nparquedb checkout --from-git --create\n```\n\n### post-merge  \nTriggered after: git merge, git pull\n```bash\n#!/bin/bash\nparquedb merge --from-git\n```\n\n### pre-commit\nTriggered before: git commit\n```bash\n#!/bin/bash\nparquedb commit --prepare-git\n```\n\n### pre-push\nTriggered before: git push\n```bash\n#!/bin/bash\nparquedb push --validate\n```\n\n## Git Merge Driver\nFor automatic resolution of refs and commits:\n```ini\n# .git/config\n[merge \"parquedb-refs\"]\n    name = ParqueDB branch reference merge\n    driver = parquedb merge-driver refs %O %A %B %P\n```\n\n## CLI Command\n```bash\nparquedb git:init    # Install hooks and configure merge driver\nparquedb git:status  # Show git/database sync status\n```\n\n## Files to Create\n- `src/cli/commands/git-init.ts`\n- `src/cli/commands/merge-driver.ts`\n- `src/git/hooks.ts` - Hook installation\n- `src/git/worktree.ts` - Worktree detection\n\n## Worktree Support\nHandle multiple git worktrees:\n```typescript\ninterface WorktreeInfo {\n  isWorktree: boolean\n  workTree: string\n  gitDir: string\n  commonDir: string\n  dbPath: string  // Per-worktree or shared\n}\n```\n\n## Config Options\n```typescript\ngit: {\n  enabled: true,\n  autoCheckout: true,\n  autoCreateBranch: true,\n  defaultMergeStrategy: 'newest',\n  ignoreBranches: ['dependabot/*'],\n  worktrees: { isolation: 'per-worktree' }\n}\n```\n\n## Acceptance Criteria\n- [ ] git checkout triggers parquedb checkout\n- [ ] git merge triggers parquedb merge\n- [ ] Merge driver handles ref conflicts\n- [ ] Works with git worktrees\n- [ ] Configurable branch ignore patterns","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:53:08.777107-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:25:38.57222-06:00","closed_at":"2026-02-03T08:25:38.57222-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-64nv.5","depends_on_id":"parquedb-64nv","type":"parent-child","created_at":"2026-02-03T07:53:08.777837-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-64nv.5","depends_on_id":"parquedb-64nv.2","type":"blocks","created_at":"2026-02-03T07:53:54.846892-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-64nv.5","depends_on_id":"parquedb-64nv.4","type":"blocks","created_at":"2026-02-03T07:53:54.968148-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-64nv.6","title":"Create GitHub Actions for database CI/CD","description":"Create reusable GitHub Actions for ParqueDB database CI/CD workflows.\n\n## Overview\nStandalone GitHub Actions that work with any ParqueDB instance via CLI + oauth.do token.\n\n## Deliverables\n\n### 1. Setup Action (`.github/actions/setup/action.yml`)\n```yaml\n- uses: parquedb/setup-action@v1\n  with:\n    version: 'latest'\n    token: ${{ secrets.PARQUEDB_TOKEN }}\n```\n\n### 2. Workflow Templates\n\n**a) PR Preview Database**\n- Creates isolated database branch for each PR\n- Pushes with unlisted visibility\n- Comments preview URL on PR\n- Cleans up on PR close\n\n**b) Database Diff**\n- Generates diff between base and head branches\n- Formats as markdown table (collections, added/removed/modified)\n- Posts/updates comment on PR\n\n**c) Merge Check**\n- Required status check\n- Runs `parquedb merge --dry-run`\n- Blocks PR if conflicts detected\n- Reports conflict details\n\n**d) Auto-Merge**\n- Triggers on PR merge\n- Merges database branch with configured strategy\n- Deletes preview branch\n\n**e) Schema Validation**\n- Triggers on parquedb.config.ts changes\n- Detects breaking schema changes\n- Posts warnings and migration hints\n\n### 3. CI Helper Commands\n```bash\nparquedb ci setup          # Configure for CI environment\nparquedb ci preview create # Create preview branch\nparquedb ci preview delete # Delete preview branch\nparquedb ci diff --format=markdown\nparquedb ci check-merge    # Dry-run merge check\n```\n\n## Files to Create\n```\n.github/actions/setup/\n action.yml\n index.js\n\n.github/workflows/templates/\n pr-preview.yml\n database-diff.yml\n merge-check.yml\n auto-merge.yml\n schema-check.yml\n\nsrc/cli/commands/ci.ts\ndocs/github-actions.md\ntests/unit/cli/ci.test.ts\n```\n\n## Success Criteria\n- [ ] Setup action installs CLI and configures auth\n- [ ] PR preview creates isolated branch with URL\n- [ ] Diff comment shows entity changes\n- [ ] Merge check blocks conflicting PRs\n- [ ] Auto-merge completes on PR merge\n- [ ] Schema check warns on breaking changes","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:53:17.995497-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:20:48.137167-06:00","closed_at":"2026-02-03T09:20:48.137167-06:00","close_reason":"Implemented GitHub Actions for database CI/CD:\n\n1. Setup Action (.github/actions/setup/)\n   - action.yml with version and token inputs\n   - src/index.ts downloads/caches CLI and configures auth\n   - Supports latest version resolution from GitHub releases\n\n2. Workflow Templates (.github/workflows/templates/)\n   - pr-preview.yml: Creates isolated preview branches for PRs\n   - database-diff.yml: Generates markdown diff comments\n   - merge-check.yml: Required status check for conflicts\n   - auto-merge.yml: Auto-merges on PR merge\n   - schema-check.yml: Breaking change detection\n\n3. CI Commands (src/cli/commands/ci.ts)\n   - ci setup: Detects GitHub/GitLab/CircleCI environments\n   - ci preview create/delete: Manages preview branches\n   - ci diff: Generates diffs in text/markdown/JSON formats\n   - ci check-merge: Dry-run merge check with conflict reporting\n\n4. Documentation (docs/github-actions.md)\n   - Setup instructions and examples\n   - Workflow template usage\n   - CI commands reference\n   - Environment variables for all CI providers\n   - Troubleshooting guide\n\n5. Tests (tests/unit/cli/ci.test.ts)\n   - 54 comprehensive tests covering all CI commands\n   - Environment detection for GitHub/GitLab/CircleCI\n   - Output format validation (text/markdown/JSON)","dependencies":[{"issue_id":"parquedb-64nv.6","depends_on_id":"parquedb-64nv","type":"parent-child","created_at":"2026-02-03T07:53:17.996477-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-64nv.6","depends_on_id":"parquedb-64nv.4","type":"blocks","created_at":"2026-02-03T07:53:55.088289-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-64nv.6","depends_on_id":"parquedb-64nv.5","type":"blocks","created_at":"2026-02-03T07:53:55.207139-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-64nv.6.1","title":"RED: Setup action tests","description":"Write failing tests for the setup action.\n\n## Tests to Write (tests/unit/actions/setup.test.ts)\n```typescript\ndescribe('setup-action', () =\u003e {\n  it('downloads correct CLI version')\n  it('adds parquedb to PATH')\n  it('configures authentication with token')\n  it('validates token before proceeding')\n  it('fails gracefully on network error')\n  it('caches CLI binary for faster runs')\n})\n```\n\n## Expected: All tests fail (no implementation yet)","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:34:16.368945-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:37:19.881644-06:00","closed_at":"2026-02-03T08:37:19.881644-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-64nv.6.1","depends_on_id":"parquedb-64nv.6","type":"parent-child","created_at":"2026-02-03T08:34:16.369718-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-64nv.6.2","title":"GREEN: Implement setup action","description":"Implement setup action to pass tests.\n\n## Files to Create\n- .github/actions/setup/action.yml\n- .github/actions/setup/index.js\n\n## Implementation\n- Download CLI from releases\n- Add to PATH\n- Configure auth via environment\n- Validate token\n\n## Expected: All setup tests pass","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:34:17.435555-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:47:26.519196-06:00","closed_at":"2026-02-03T08:47:26.519196-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-64nv.6.2","depends_on_id":"parquedb-64nv.6","type":"parent-child","created_at":"2026-02-03T08:34:17.436389-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-64nv.6.2","depends_on_id":"parquedb-64nv.6.1","type":"blocks","created_at":"2026-02-03T08:36:23.734726-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-64nv.6.3","title":"RED: CI commands tests","description":"Write failing tests for CI helper commands.\n\n## Tests to Write (tests/unit/cli/ci.test.ts)\n```typescript\ndescribe('parquedb ci', () =\u003e {\n  describe('ci setup', () =\u003e {\n    it('detects CI environment')\n    it('configures non-interactive mode')\n  })\n  \n  describe('ci preview create', () =\u003e {\n    it('creates branch with pr-{number} name')\n    it('pushes with unlisted visibility')\n    it('outputs preview URL')\n  })\n  \n  describe('ci preview delete', () =\u003e {\n    it('deletes preview branch')\n    it('handles non-existent branch')\n  })\n  \n  describe('ci diff', () =\u003e {\n    it('outputs markdown format')\n    it('shows entity counts per collection')\n    it('handles no changes gracefully')\n  })\n  \n  describe('ci check-merge', () =\u003e {\n    it('exits 0 on clean merge')\n    it('exits 1 on conflicts')\n    it('outputs conflict details as JSON')\n  })\n})\n```","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:34:19.269874-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:38:38.007372-06:00","closed_at":"2026-02-03T08:38:38.007372-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-64nv.6.3","depends_on_id":"parquedb-64nv.6","type":"parent-child","created_at":"2026-02-03T08:34:19.270776-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-64nv.6.4","title":"GREEN: Implement CI commands","description":"Implement CI helper commands to pass tests.\n\n## Files to Create\n- src/cli/commands/ci.ts\n\n## Commands\n- parquedb ci setup\n- parquedb ci preview create/delete\n- parquedb ci diff --format=markdown|json\n- parquedb ci check-merge\n\n## Expected: All CI command tests pass","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:34:20.522366-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:47:26.556224-06:00","closed_at":"2026-02-03T08:47:26.556224-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-64nv.6.4","depends_on_id":"parquedb-64nv.6","type":"parent-child","created_at":"2026-02-03T08:34:20.523197-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-64nv.6.4","depends_on_id":"parquedb-64nv.6.3","type":"blocks","created_at":"2026-02-03T08:36:23.844105-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-64nv.6.5","title":"RED: Workflow template tests","description":"Write tests validating workflow templates.\n\n## Tests to Write\n```typescript\ndescribe('workflow templates', () =\u003e {\n  it('pr-preview.yml is valid GitHub Actions syntax')\n  it('pr-preview.yml uses correct action inputs')\n  it('database-diff.yml posts comment correctly')\n  it('merge-check.yml sets check status')\n  it('auto-merge.yml triggers on PR merge only')\n  it('schema-check.yml triggers on config changes')\n})\n```","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:34:21.368468-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:38:27.314152-06:00","closed_at":"2026-02-03T08:38:27.314152-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-64nv.6.5","depends_on_id":"parquedb-64nv.6","type":"parent-child","created_at":"2026-02-03T08:34:21.369343-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-64nv.6.6","title":"GREEN: Create workflow templates","description":"Create workflow template files.\n\n## Files to Create\n- .github/workflows/templates/pr-preview.yml\n- .github/workflows/templates/database-diff.yml\n- .github/workflows/templates/merge-check.yml\n- .github/workflows/templates/auto-merge.yml\n- .github/workflows/templates/schema-check.yml\n\n## Expected: All template tests pass","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:34:22.482167-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:47:26.591761-06:00","closed_at":"2026-02-03T08:47:26.591761-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-64nv.6.6","depends_on_id":"parquedb-64nv.6","type":"parent-child","created_at":"2026-02-03T08:34:22.482893-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-64nv.6.6","depends_on_id":"parquedb-64nv.6.5","type":"blocks","created_at":"2026-02-03T08:36:23.954735-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-64nv.6.7","title":"REFACTOR: Actions documentation","description":"Documentation and polish.\n\n## Files to Create\n- docs/github-actions.md\n\n## Contents\n- Setup instructions\n- Workflow template usage\n- Configuration options\n- Troubleshooting\n- Examples for common scenarios","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:34:23.527673-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:22:57.350512-06:00","closed_at":"2026-02-03T09:22:57.350512-06:00","close_reason":"Improved GitHub Actions documentation with comprehensive coverage of setup action, CI commands reference, workflow templates (PR preview, database diff, merge check, schema check, auto-merge), configuration options, examples for GitHub Actions/GitLab CI/CircleCI, and troubleshooting guide","dependencies":[{"issue_id":"parquedb-64nv.6.7","depends_on_id":"parquedb-64nv.6","type":"parent-child","created_at":"2026-02-03T08:34:23.5285-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-64nv.6.7","depends_on_id":"parquedb-64nv.6.2","type":"blocks","created_at":"2026-02-03T08:36:24.066958-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-64nv.6.7","depends_on_id":"parquedb-64nv.6.4","type":"blocks","created_at":"2026-02-03T08:36:24.178541-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-64nv.6.7","depends_on_id":"parquedb-64nv.6.6","type":"blocks","created_at":"2026-02-03T08:36:24.28979-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-64nv.7","title":"Build ParqueDB GitHub App","description":"Build GitHub App webhook handlers integrated into the ParqueDB Worker.\n\n## Architecture\n```\nGitHub Webhook  ParqueDB Worker  DatabaseIndexDO  ParqueDBDO\n                 /webhooks/github   (routes to user)  (user's DB)\n```\n\n## Webhook Handlers\n\n| Event | Action |\n|-------|--------|\n| `installation.created` | Link GitHub installation to ParqueDB account |\n| `installation.deleted` | Unlink installation |\n| `create` (branch) | Create database branch |\n| `delete` (branch) | Delete database branch |\n| `pull_request.opened` | Create preview, post diff, create check |\n| `pull_request.synchronize` | Update diff, re-run check |\n| `pull_request.closed` | Merge or cleanup |\n| `issue_comment.created` | Handle slash commands |\n\n## Slash Commands\n```\n/parquedb preview     - Create/refresh preview\n/parquedb diff        - Show detailed diff\n/parquedb diff users  - Diff specific collection\n/parquedb resolve \u003centity\u003e --ours|--theirs\n/parquedb schema      - Show schema changes\n```\n\n## Check Runs\n- **ParqueDB Merge Check**: Required status check\n- Reports conflicts, breaking schema changes\n- Links to detailed diff\n\n## PR Comment Template\n```markdown\n##  Database Changes\n\n| Collection | Added | Removed | Modified |\n|------------|------:|--------:|---------:|\n| users      | +12   | -2      | ~5       |\n\n**Merge Status**:  Clean merge\n[Preview Database](https://parque.db/preview/pr-123)\n```\n\n## Auth Flow\n1. GitHub App installation linked to ParqueDB account via oauth.do\n2. Webhook includes installation_id\n3. DatabaseIndexDO looks up user by installation\n4. Routes to user's ParqueDBDO\n\n## Files to Create\n```\nsrc/worker/github/\n webhooks.ts           # Hono router for /webhooks/github\n handlers/\n    installation.ts   # App install/uninstall\n    branch.ts         # Branch create/delete\n    pull-request.ts   # PR lifecycle\n    issue-comment.ts  # Slash commands\n checks.ts             # GitHub Checks API\n comments.ts           # PR comment templates\n config.ts             # .github/parquedb.yml parser\n\nsrc/worker/index.ts       # Add webhook route\nsrc/types/github.ts       # Webhook payload types\n\ntests/unit/worker/github/\n webhooks.test.ts\n handlers.test.ts\n checks.test.ts\n commands.test.ts\n```\n\n## DatabaseIndexDO Additions\n```typescript\n// Link GitHub installation to user\nlinkGitHubInstallation(userId: string, installationId: string): Promise\u003cvoid\u003e\n\n// Lookup user by installation\ngetUserByInstallation(installationId: string): Promise\u003cUserAccount | null\u003e\n\n// Lookup database by repo\ngetDatabaseByRepo(owner: string, repo: string): Promise\u003cDatabaseInfo | null\u003e\n```\n\n## Configuration (.github/parquedb.yml)\n```yaml\ndatabase:\n  name: my-database\n\nbranches:\n  auto_create: ['feature/*', 'fix/*']\n  ignore: ['dependabot/*']\n\npreview:\n  enabled: true\n  ttl: 24h\n\nmerge:\n  required_check: true\n  default_strategy: manual\n\ndiff:\n  auto_comment: true\n  max_entities: 100\n```\n\n## Success Criteria\n- [ ] Webhook signature verification\n- [ ] Installation links to ParqueDB account\n- [ ] Branch create/delete syncs with git\n- [ ] PR opens creates preview + diff comment\n- [ ] Merge check blocks on conflicts\n- [ ] Slash commands work in PR comments\n- [ ] PR merge triggers database merge","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:53:31.794476-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:09:02.196336-06:00","closed_at":"2026-02-03T10:09:02.196336-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-64nv.7","depends_on_id":"parquedb-64nv","type":"parent-child","created_at":"2026-02-03T07:53:31.795093-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-64nv.7","depends_on_id":"parquedb-64nv.6","type":"blocks","created_at":"2026-02-03T07:53:55.326792-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-64nv.7.1","title":"RED: Webhook handler tests","description":"Write failing tests for GitHub webhook handlers.\n\n## Tests to Write (tests/unit/worker/github/handlers.test.ts)\n```typescript\ndescribe('GitHub webhook handlers', () =\u003e {\n  describe('installation.created', () =\u003e {\n    it('links installation to ParqueDB account')\n    it('stores repo access list')\n  })\n  \n  describe('installation.deleted', () =\u003e {\n    it('unlinks installation from account')\n  })\n  \n  describe('create (branch)', () =\u003e {\n    it('creates database branch matching git branch')\n    it('ignores branches in ignore list')\n    it('handles branch names with slashes')\n  })\n  \n  describe('delete (branch)', () =\u003e {\n    it('deletes database branch')\n    it('handles non-existent branch')\n  })\n  \n  describe('pull_request.opened', () =\u003e {\n    it('creates preview branch')\n    it('posts diff comment')\n    it('creates merge check run')\n  })\n  \n  describe('pull_request.synchronize', () =\u003e {\n    it('updates existing diff comment')\n    it('re-runs merge check')\n  })\n  \n  describe('pull_request.closed (merged)', () =\u003e {\n    it('merges database branch')\n    it('deletes preview branch')\n    it('posts confirmation comment')\n  })\n  \n  describe('pull_request.closed (not merged)', () =\u003e {\n    it('deletes preview branch without merging')\n  })\n})\n```","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:34:34.842385-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:39:32.529525-06:00","closed_at":"2026-02-03T08:39:32.529525-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-64nv.7.1","depends_on_id":"parquedb-64nv.7","type":"parent-child","created_at":"2026-02-03T08:34:34.843828-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-64nv.7.10","title":"RED: Webhook handler tests","description":"Write failing tests for GitHub webhook handlers.\n\n## Tests to Write (tests/unit/worker/github/handlers.test.ts)\n```typescript\ndescribe('GitHub webhook handlers', () =\u003e {\n  describe('installation.created', () =\u003e {\n    it('links installation to ParqueDB account')\n    it('stores repo access list')\n  })\n  \n  describe('installation.deleted', () =\u003e {\n    it('unlinks installation from account')\n  })\n  \n  describe('create (branch)', () =\u003e {\n    it('creates database branch matching git branch')\n    it('ignores branches in ignore list')\n    it('handles branch names with slashes')\n  })\n  \n  describe('delete (branch)', () =\u003e {\n    it('deletes database branch')\n    it('handles non-existent branch')\n  })\n  \n  describe('pull_request.opened', () =\u003e {\n    it('creates preview branch')\n    it('posts diff comment')\n    it('creates merge check run')\n  })\n  \n  describe('pull_request.synchronize', () =\u003e {\n    it('updates existing diff comment')\n    it('re-runs merge check')\n  })\n  \n  describe('pull_request.closed (merged)', () =\u003e {\n    it('merges database branch')\n    it('deletes preview branch')\n    it('posts confirmation comment')\n  })\n  \n  describe('pull_request.closed (not merged)', () =\u003e {\n    it('deletes preview branch without merging')\n  })\n})\n```","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:35:24.556863-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:09:31.673764-06:00","closed_at":"2026-02-03T09:09:31.673764-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-64nv.7.10","depends_on_id":"parquedb-64nv.7","type":"parent-child","created_at":"2026-02-03T08:35:24.557643-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-64nv.7.2","title":"GREEN: Implement webhook handlers","description":"Implement webhook handlers to pass tests.\n\n## Files to Create\n- src/worker/github/webhooks.ts\n- src/worker/github/handlers/installation.ts\n- src/worker/github/handlers/branch.ts\n- src/worker/github/handlers/pull-request.ts\n\n## Implementation\n- Verify webhook signature\n- Route to appropriate handler\n- Call ParqueDB DO methods\n\n## Expected: All webhook handler tests pass","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:34:38.928801-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:47:26.626834-06:00","closed_at":"2026-02-03T08:47:26.626834-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-64nv.7.2","depends_on_id":"parquedb-64nv.7","type":"parent-child","created_at":"2026-02-03T08:34:38.929979-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-64nv.7.2","depends_on_id":"parquedb-64nv.7.1","type":"blocks","created_at":"2026-02-03T08:36:25.970806-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-64nv.7.3","title":"RED: Slash command tests","description":"Write failing tests for slash commands.\n\n## Tests to Write (tests/unit/worker/github/commands.test.ts)\n```typescript\ndescribe('slash commands', () =\u003e {\n  describe('/parquedb preview', () =\u003e {\n    it('creates preview and posts URL')\n    it('refreshes existing preview')\n    it('reacts with emoji on success')\n  })\n  \n  describe('/parquedb diff', () =\u003e {\n    it('posts detailed diff')\n    it('filters by collection when specified')\n  })\n  \n  describe('/parquedb resolve', () =\u003e {\n    it('resolves conflict with --ours')\n    it('resolves conflict with --theirs')\n    it('posts error on invalid entity')\n  })\n  \n  describe('/parquedb schema', () =\u003e {\n    it('shows schema changes')\n    it('highlights breaking changes')\n  })\n  \n  describe('unknown command', () =\u003e {\n    it('posts help message')\n  })\n})\n```","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:34:42.711513-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:39:46.454603-06:00","closed_at":"2026-02-03T08:39:46.454603-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-64nv.7.3","depends_on_id":"parquedb-64nv.7","type":"parent-child","created_at":"2026-02-03T08:34:42.717452-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-64nv.7.4","title":"GREEN: Implement slash commands","description":"Implement slash command handlers.\n\n## Files to Create\n- src/worker/github/handlers/issue-comment.ts\n- src/worker/github/commands/preview.ts\n- src/worker/github/commands/diff.ts\n- src/worker/github/commands/resolve.ts\n- src/worker/github/commands/schema.ts\n\n## Expected: All slash command tests pass","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:34:45.543127-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:47:26.666599-06:00","closed_at":"2026-02-03T08:47:26.666599-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-64nv.7.4","depends_on_id":"parquedb-64nv.7","type":"parent-child","created_at":"2026-02-03T08:34:45.543913-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-64nv.7.4","depends_on_id":"parquedb-64nv.7.3","type":"blocks","created_at":"2026-02-03T08:36:26.083511-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-64nv.7.5","title":"RED: Checks and comments tests","description":"Write failing tests for GitHub Checks API and PR comments.\n\n## Tests to Write\n```typescript\ndescribe('GitHub Checks', () =\u003e {\n  it('creates pending check on PR open')\n  it('updates to success on clean merge')\n  it('updates to failure with conflict details')\n  it('includes schema warnings in output')\n})\n\ndescribe('PR Comments', () =\u003e {\n  it('formats diff as markdown table')\n  it('includes preview URL')\n  it('shows merge status')\n  it('updates existing comment instead of creating new')\n})\n```","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:34:49.902701-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:40:35.653577-06:00","closed_at":"2026-02-03T08:40:35.653577-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-64nv.7.5","depends_on_id":"parquedb-64nv.7","type":"parent-child","created_at":"2026-02-03T08:34:49.903497-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-64nv.7.6","title":"GREEN: Implement checks and comments","description":"Implement GitHub Checks API and comment formatting.\n\n## Files to Create\n- src/worker/github/checks.ts\n- src/worker/github/comments.ts\n- src/types/github.ts\n\n## Expected: All checks and comments tests pass","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:34:54.324019-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:47:26.703895-06:00","closed_at":"2026-02-03T08:47:26.703895-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-64nv.7.6","depends_on_id":"parquedb-64nv.7","type":"parent-child","created_at":"2026-02-03T08:34:54.324773-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-64nv.7.6","depends_on_id":"parquedb-64nv.7.5","type":"blocks","created_at":"2026-02-03T08:36:26.19569-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-64nv.7.7","title":"RED: Config parser tests","description":"Write failing tests for .github/parquedb.yml parser.\n\n## Tests to Write\n```typescript\ndescribe('config parser', () =\u003e {\n  it('parses valid config')\n  it('applies defaults for missing fields')\n  it('validates branch patterns')\n  it('handles missing config file')\n})\n```","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:34:57.378599-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:40:13.855934-06:00","closed_at":"2026-02-03T08:40:13.855934-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-64nv.7.7","depends_on_id":"parquedb-64nv.7","type":"parent-child","created_at":"2026-02-03T08:34:57.379469-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-64nv.7.8","title":"GREEN: Implement config parser","description":"Implement .github/parquedb.yml parser.\n\n## Files to Create\n- src/worker/github/config.ts\n\n## Expected: All config parser tests pass","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:34:59.840069-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:47:26.742005-06:00","closed_at":"2026-02-03T08:47:26.742005-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-64nv.7.8","depends_on_id":"parquedb-64nv.7","type":"parent-child","created_at":"2026-02-03T08:34:59.840938-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-64nv.7.8","depends_on_id":"parquedb-64nv.7.7","type":"blocks","created_at":"2026-02-03T08:36:26.307301-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-64nv.7.9","title":"REFACTOR: App documentation and worker integration","description":"Documentation and final integration.\n\n## Files to Create/Modify\n- src/worker/index.ts (add /webhooks/github route)\n- docs/github-app.md\n\n## Contents\n- App installation guide\n- Configuration reference\n- Webhook setup\n- Troubleshooting","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:35:03.283546-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:09:28.33004-06:00","closed_at":"2026-02-03T09:09:28.33004-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-64nv.7.9","depends_on_id":"parquedb-64nv.7","type":"parent-child","created_at":"2026-02-03T08:35:03.284303-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-64nv.7.9","depends_on_id":"parquedb-64nv.7.2","type":"blocks","created_at":"2026-02-03T08:36:26.424033-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-64nv.7.9","depends_on_id":"parquedb-64nv.7.4","type":"blocks","created_at":"2026-02-03T08:36:26.538877-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-64nv.7.9","depends_on_id":"parquedb-64nv.7.6","type":"blocks","created_at":"2026-02-03T08:36:26.648489-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-64nv.7.9","depends_on_id":"parquedb-64nv.7.8","type":"blocks","created_at":"2026-02-03T08:36:26.757811-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-64nv.8","title":"Implement Parquet segment merging for events","description":"Build the infrastructure to merge event segments stored in Parquet files.\n\n## Event Storage Structure\n```\nevents/\n seg-001.parquet     # Events 1-10000  \n seg-002.parquet     # Events 10001-20000\n current.parquet     # Active segment\n manifest.json       # Segment index\n```\n\n## Manifest Structure\n```typescript\ninterface EventManifest {\n  segments: Array\u003c{\n    file: string\n    minId: string       // First event ULID\n    maxId: string       // Last event ULID\n    minTs: number\n    maxTs: number\n    count: number\n    checksum: string    // SHA256 for content addressing\n  }\u003e\n  branch: string\n  headCommit: string\n  mergedFrom?: Array\u003c{\n    branch: string\n    commit: string\n    upToSegment: string\n  }\u003e\n}\n```\n\n## Merge Process\n1. Find common segments (by checksum)\n2. Identify new segments in each branch\n3. Read events from new segments\n4. Merge event streams (using event-merge.ts)\n5. Write merged events to new segment\n6. Create new manifest\n\n```typescript\nasync function mergeEventParquets(\n  baseManifest: EventManifest,\n  ourManifest: EventManifest,\n  theirManifest: EventManifest\n): Promise\u003cMergedEventStream\u003e\n```\n\n## Content Addressing\n- Segments identified by SHA256 checksum\n- Same events = same checksum = shared segment\n- Deduplication across branches\n\n## Files to Create\n- `src/sync/parquet-merge.ts`\n- `src/sync/event-manifest.ts`\n- `src/sync/segment-reader.ts`\n- `src/sync/segment-writer.ts`\n\n## Performance Considerations\n- Stream events from Parquet (don't load all in memory)\n- Use row group filtering for timestamp ranges\n- Batch writes to new segment\n- Parallelize segment reads\n\n## Tests\n- Merge with no overlapping segments (fast path)\n- Merge with overlapping timestamps\n- Content addressing deduplicates identical segments\n- Large segment merge doesn't OOM","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:53:42.568069-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:19:01.61434-06:00","closed_at":"2026-02-03T08:19:01.61434-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-64nv.8","depends_on_id":"parquedb-64nv","type":"parent-child","created_at":"2026-02-03T07:53:42.569073-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-64nv.8","depends_on_id":"parquedb-64nv.3","type":"blocks","created_at":"2026-02-03T07:53:55.443362-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-64nv.9","title":"Track schema evolution in commits with type generation","description":"Enhance the commit model to explicitly track schema evolution, enabling:\n1. Schema changes attached to commits (like migrations)\n2. Type generation from schema at any commit\n3. Schema diff between commits/branches\n4. Breaking change detection\n\n## Current State\n- DatabaseCommit.state.collections[name].schemaHash captures schema hash\n- parquedb.config.ts defines schema with TypeDefinition\n- No explicit schema versioning or migration tracking\n\n## Proposed Enhancements\n\n### 1. Schema Snapshot in Commits\n```typescript\ninterface DatabaseCommit {\n  // ... existing fields ...\n  state: {\n    // ... existing fields ...\n    schema: {\n      hash: string                    // Hash of full schema\n      configHash: string              // Hash of parquedb.config.ts\n      collections: Record\u003cstring, {\n        hash: string\n        fields: SchemaFieldSnapshot[] // Captured field definitions\n        version: number               // Schema version for this collection\n      }\u003e\n    }\n  }\n}\n```\n\n### 2. Schema Evolution Events\n```typescript\ninterface SchemaChangeEvent extends Event {\n  op: 'SCHEMA_CHANGE'\n  target: string  // Collection name\n  change: {\n    type: 'ADD_FIELD' | 'REMOVE_FIELD' | 'MODIFY_FIELD' | 'ADD_COLLECTION' | 'DROP_COLLECTION'\n    field?: string\n    before?: SchemaFieldSnapshot\n    after?: SchemaFieldSnapshot\n    breaking: boolean\n  }\n}\n```\n\n### 3. Type Generation from Commit\n```bash\n# Generate types for current schema\nparquedb types generate --output=./types/db.d.ts\n\n# Generate types for specific commit/branch\nparquedb types generate --at=v1.0.0 --output=./types/db-v1.d.ts\n\n# Diff types between versions\nparquedb types diff main..feature/new-schema\n```\n\n### 4. Schema Migration Tracking\n```\n_meta/\n commits/*.json\n schema/\n    current.json          # Current schema snapshot\n    history/\n       {hash}.json       # Historical schema snapshots\n       ...\n    migrations/\n        001-add-categories.json\n        002-user-email-required.json\n```\n\n### 5. Breaking Change Detection\n```typescript\ninterface SchemaCompatibility {\n  compatible: boolean\n  breaking: BreakingChange[]\n  warnings: SchemaWarning[]\n}\n\ntype BreakingChange = \n  | { type: 'FIELD_REMOVED', collection: string, field: string }\n  | { type: 'TYPE_CHANGED', collection: string, field: string, from: string, to: string }\n  | { type: 'REQUIRED_ADDED', collection: string, field: string }\n  | { type: 'COLLECTION_DROPPED', collection: string }\n```\n\n### 6. Integration with Git Commits\n```bash\n# Pre-commit hook detects schema changes\nparquedb schema check --staged\n\n# Warns if parquedb.config.ts changed but no migration created\n  Schema changed but no migration found. Run:\n    parquedb migrate create 'add categories collection'\n\n# Schema changes in PR get special comment\n## Schema Changes in this PR\n-  Added collection: \\`categories\\`\n-  Made \\`users.email\\` required (breaking for existing null values)\n```\n\n## Files to Create\n- src/sync/schema-snapshot.ts - Schema capture/comparison\n- src/sync/schema-evolution.ts - Change detection\n- src/cli/commands/types.ts - Type generation\n- src/cli/commands/schema.ts - Schema commands\n\n## Acceptance Criteria\n- [ ] Schema snapshots stored in commits\n- [ ] parquedb types generate produces .d.ts from schema\n- [ ] parquedb schema diff shows changes between commits\n- [ ] Breaking changes detected and flagged\n- [ ] GitHub Action reports schema changes in PR comments","notes":"## Strongly-Typed Time Travel\n\nThe key insight: if schema is captured per-commit, we can generate types for ANY point in time and provide compile-time safety for historical queries.\n\n### Usage Pattern\n```typescript\n// Generate types for multiple versions\n// parquedb types generate --at=v1.0.0 --output=types/v1.d.ts\n// parquedb types generate --at=v2.0.0 --output=types/v2.d.ts\n// parquedb types generate --output=types/current.d.ts\n\nimport type { Schema as V1Schema } from './types/v1'\nimport type { Schema as V2Schema } from './types/v2'\nimport type { Schema as CurrentSchema } from './types/current'\n\n// Type-safe current queries\nconst db = await openDB\u003cCurrentSchema\u003e()\nconst user = await db.Users.get('user-123')\nuser.email      //  string (required in current)\nuser.avatar     //  string | undefined (added in v2)\nuser.legacyId   //  TypeScript error - removed in v2\n\n// Type-safe v1 queries (time travel)\nconst dbV1 = await openDB\u003cV1Schema\u003e({ at: 'v1.0.0' })\nconst userV1 = await dbV1.Users.get('user-123')\nuserV1.email     //  string | undefined (was optional in v1)\nuserV1.legacyId  //  string (existed in v1)\nuserV1.avatar    //  TypeScript error - didn't exist in v1\\!\n```\n\n### Type Generation from Commit\n```typescript\n// src/cli/commands/types.ts\nasync function generateTypes(opts: { at?: string, output: string }) {\n  const commit = opts.at \n    ? await loadCommit(storage, await refs.resolveRef(opts.at))\n    : await loadCommit(storage, await refs.resolveRef('HEAD'))\n  \n  const schema = commit.state.schema\n  const types = generateTypeScript(schema)\n  await fs.writeFile(opts.output, types)\n}\n\n// Generated types include commit metadata\nexport interface SchemaMetadata {\n  commit: string\n  timestamp: number\n  version: string\n}\n\nexport interface Schema {\n  __meta: SchemaMetadata\n  Users: UserCollection\n  Posts: PostCollection\n  // ...\n}\n```\n\n### CI/CD Integration\n```yaml\n# Generate types on release, publish to npm\n- name: Generate Types\n  run: |\n    parquedb types generate --output=dist/types/index.d.ts\n    echo \"export const SCHEMA_VERSION = '$(git describe --tags)'\" \u003e\u003e dist/types/index.d.ts\n\n# Consumers can pin to specific schema version\n# npm install @myorg/db-types@1.0.0\n```\n\nThis enables:\n1. **Auditing** - Query historical data with historical types\n2. **Migrations** - Transform data between schema versions type-safely\n3. **Rollback safety** - Know exactly what fields existed at rollback point\n4. **API versioning** - Serve different API versions from same DB with correct types","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:08:06.379238-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:12:08.57181-06:00","closed_at":"2026-02-03T09:12:08.57181-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-64nv.9","depends_on_id":"parquedb-64nv","type":"parent-child","created_at":"2026-02-03T08:08:06.38004-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-64nv.9","depends_on_id":"parquedb-64nv.1","type":"blocks","created_at":"2026-02-03T08:08:12.240484-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-6604","title":"High: Missing timeout handling in SyncEngine operations","description":"The SyncEngine push(), pull(), and sync() operations in engine.ts (lines 84-395) don't have overall operation timeouts. If remote storage operations are slow or hang, these operations can hang indefinitely. Should add configurable operation timeout with proper cleanup on timeout.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:04.205263-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:00:06.351777-06:00","closed_at":"2026-02-03T13:00:06.351777-06:00","close_reason":"Closed"}
{"id":"parquedb-68t5","title":"Standardize logging in compaction workflows","description":"Mixed logging approaches: console.log for debug-level, logger.info/error for important events. Standardize to use logger utility consistently throughout compaction-queue-consumer.ts and other workflow files. Remove debug console.log statements or convert to logger.debug.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:56:26.790543-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:00:24.644487-06:00","closed_at":"2026-02-03T13:00:24.644487-06:00","close_reason":"Closed"}
{"id":"parquedb-69c","title":"Implement link/unlink operations","description":"$link and $unlink update operators","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:34.620458-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:06:34.629587-06:00","closed_at":"2026-02-01T14:06:34.629587-06:00","close_reason":"Closed"}
{"id":"parquedb-69ia","title":"Add error recovery tests","description":"error-recovery.test.ts exists but may not cover: partial write failures, network timeout recovery, corrupted file handling, storage backend failures mid-operation.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:32.82806-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T16:07:37.150236-06:00","closed_at":"2026-02-01T16:07:37.150236-06:00","close_reason":"Closed"}
{"id":"parquedb-69mg","title":"Complete event sourcing implementation","description":"Events are logged but not the source of truth - entity state stored separately in SQLite. Implement true event sourcing: derive entity state from event replay, add event compaction with snapshots, add event versioning for schema evolution.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:19.491236-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T16:51:21.99011-06:00","closed_at":"2026-02-01T16:51:21.99011-06:00","close_reason":"Closed"}
{"id":"parquedb-69um","title":"Implement compaction (events to state)","description":"Compact events into materialized state files:\n\n- Read events from SQLite WAL + R2 segments\n- Replay to build current state\n- Write new data.parquet + rels.parquet\n- Create snapshot: snapshots/{ts}/data.parquet, rels.parquet\n- Truncate compacted SQLite rows\n- Archive old event segments\n\nFile: src/events/compaction.ts","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T06:37:59.621527-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T07:17:15.096929-06:00","closed_at":"2026-02-01T07:17:15.096929-06:00","close_reason":"Implemented EventCompactor and StateCollector with entity/relationship state collection, multi-segment compaction, snapshot creation, segment deletion, and compactedThrough watermark. All 30 tests passing.","dependencies":[{"issue_id":"parquedb-69um","depends_on_id":"parquedb-czot","type":"blocks","created_at":"2026-02-01T06:38:11.128652-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-6dcn","title":"Add error handling to fire-and-forget snapshot operations","description":"ParqueDB/core.ts:2024 has fire-and-forget auto-snapshot without error handling:\n\n```typescript\n// TODO(parquedb-y9aw): Auto-snapshot is fire-and-forget by design for performance,\n// but unhandled rejections could be silently lost\n```\n\nFix by:\n- Implementing promise rejection handling with logging\n- Adding metrics/observability for snapshot failures\n- Consider async queue with retry logic","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:31:41.896874-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:05:46.677163-06:00","closed_at":"2026-02-03T10:05:46.677163-06:00","close_reason":"Closed"}
{"id":"parquedb-6duk","title":"Fix sync-routes-token tests after refactor","description":"tests/unit/worker/sync-routes-token.test.ts is failing after removing duplicate token functions from sync-routes.ts. Tests need to be updated to work with functions imported from sync-token.ts.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:04:15.989159-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:05:33.132279-06:00","closed_at":"2026-02-03T12:05:33.132279-06:00","close_reason":"Closed"}
{"id":"parquedb-6eob","title":"Make AWS SDK optional dependency","description":"@aws-sdk/client-s3 adds ~200KB to bundle. Not needed for R2/Workers-only deployments. Move to peer/optional dependency, lazy-load only when S3Backend is used.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:23.024469-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T16:21:33.433995-06:00","closed_at":"2026-02-01T16:21:33.433995-06:00","close_reason":"Closed"}
{"id":"parquedb-6eor","title":"High: Missing retry with backoff in SyncClient","description":"The SyncClient.fetch() method in client.ts (lines 343-363) has timeout handling but no retry logic with exponential backoff for transient failures. Network issues, rate limiting, or temporary server errors will cause immediate failure instead of retrying.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:09.412934-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:58:41.673985-06:00","closed_at":"2026-02-03T12:58:41.673985-06:00","close_reason":"Closed"}
{"id":"parquedb-6fxk","title":"FsBackend path resolution allows absolute paths starting with /","description":"**High: Inconsistent Path Handling**\n\nIn /src/storage/FsBackend.ts lines 68-71, absolute paths starting with '/' are rejected with PathTraversalError. However:\n\n1. The error message says 'path traversal attempt' which is misleading for absolute paths\n2. Other backends (MemoryBackend, R2Backend, DOSqliteBackend) normalize leading slashes instead of rejecting\n\n**Impact**: Inconsistent behavior across backends - code that works with MemoryBackend may fail with FsBackend.\n\n**Location**: src/storage/FsBackend.ts:68-71\n\n**Recommendation**: Either:\n1. Normalize absolute paths by stripping leading '/' (consistent with other backends), OR\n2. Throw InvalidPathError with a clearer message like 'Absolute paths not allowed'\n\nThe current behavior conflates two different concerns: security (path traversal) and input validation (absolute vs relative paths).","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:58.789973-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:48:25.019266-06:00","closed_at":"2026-02-03T12:48:25.019266-06:00","close_reason":"Closed"}
{"id":"parquedb-6get","title":"Add default timeout to RPC client HTTP transport","description":"## Issue\nIn `src/integrations/rpc-do/client.ts`, the HTTP transport only applies a timeout if explicitly configured. There is no default timeout, meaning requests can hang indefinitely.\n\n## Location\n`src/integrations/rpc-do/client.ts:359-361`\n\n## Risk\nHigh - Network calls without timeouts can cause the client to hang indefinitely.\n\n## Suggested Fix\nAdd a default timeout (e.g., 30 seconds) that is used when no timeout is explicitly configured.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:09.1894-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:36:14.65693-06:00","closed_at":"2026-02-03T11:36:14.65693-06:00","close_reason":"Fixed: Added DEFAULT_TIMEOUT_MS constant (30 seconds) and applied it to HTTP transport when no explicit timeout is provided."}
{"id":"parquedb-6h3s","title":"Add tests for src/studio/api.ts (0% coverage)","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:27.535887-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:08:21.015466-06:00","closed_at":"2026-02-03T09:08:21.015466-06:00","close_reason":"Closed"}
{"id":"parquedb-6jmr","title":"Time-bucket sharding for CompactionStateDO under extreme concurrency","description":"Under extreme concurrency, even namespace-sharded DOs could bottleneck (1000 req/sec limit per DO). Add time-bucket sharding: idFromName(namespace + ':' + Math.floor(timestamp / BUCKET_SIZE)). Each time bucket gets its own DO. Trade-off: More DOs to query for status, but much higher concurrency. Only needed for namespaces with 1000+ writes/second.","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:57:36.453761-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:17:03.253427-06:00","closed_at":"2026-02-03T14:17:03.253427-06:00","close_reason":"Closed"}
{"id":"parquedb-6m2y","title":"Remove hash index infrastructure","description":"Remove the hash index infrastructure since native parquet predicate pushdown on $index_* columns is now faster than secondary indexes.\n\n**UPDATE: Scope expansion required**\n\nInvestigation revealed that hash indexes are deeply integrated throughout the codebase. A complete removal requires changes to:\n\n1. **Source files to delete:**\n   - src/indexes/secondary/hash.ts (710 lines)\n   - src/indexes/secondary/sharded-hash.ts (442 lines)\n   - src/indexes/secondary/sst.ts\n   - src/indexes/secondary/sharded-sst.ts\n\n2. **Source files to update:**\n   - src/indexes/secondary/index.ts (remove hash/sst exports)\n   - src/indexes/types.ts (remove 'hash'/'sst' from IndexType)\n   - src/indexes/manager.ts (remove hash selection logic, hashLookup, rangeQuery methods)\n   - src/worker/IndexCache.ts (remove HashIndex import, loadHashIndex, executeHashLookup, loadSSTIndex, executeSSTLookup)\n   - src/worker/QueryExecutor.ts (remove hash/sst cases in executeWithIndex)\n   - src/index.ts (remove HashIndex, SSTIndex exports)\n   - src/aggregation/executor.ts (remove hash/sst type handling)\n   - src/query/executor.ts (remove hash/sst type handling)\n   - src/worker/benchmark-queries.ts (extensive hash/sst references)\n   - Additional files with 'hash' type references\n\n3. **Test files to delete:**\n   - tests/unit/indexes/hash-index.test.ts\n   - tests/unit/indexes/sharded-index.test.ts\n   - tests/unit/indexes/unique-index.test.ts\n   - tests/integration/secondary-index.test.ts\n\n4. **Test files to update:**\n   - tests/unit/worker/IndexCache.test.ts\n   - tests/unit/error-recovery.test.ts\n\nThis is a significant refactoring effort (~2000+ lines of code changes) that should be done carefully to avoid breaking the build. Consider breaking into smaller sub-tasks.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T11:06:06.904067-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T12:28:59.3718-06:00","closed_at":"2026-02-02T12:28:59.3718-06:00","close_reason":"Completed: Removed hash index infrastructure. Native parquet predicate pushdown on $index_* columns is now used for equality queries."}
{"id":"parquedb-6p7t","title":"Implement GeoNames data loader for places.org.ai","description":"Create GeoNames data ingestion pipeline:\n\nData sources:\n- allCountries.zip (397MB compressed, 13M places)\n- alternateNamesV2.zip (190MB, multilingual names)\n- hierarchy.zip (admin relationships)\n- admin1/admin2 codes\n\nPipeline steps:\n1. Download from download.geonames.org/export/dump/\n2. Parse TSV format (19 columns)\n3. Convert to Parquet with schema\n4. Partition by country code (250 files)\n5. Split large countries (US, CN, IN) by admin1\n6. Generate indexes (name search, geohash, hierarchy)\n\nTarget: ~500MB total Parquet, ~300 files\nLicense: CC-BY-4.0 (requires attribution)","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:17:46.640973-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:33:11.84387-06:00","closed_at":"2026-02-03T13:33:11.84387-06:00","close_reason":"Implemented full GeoNames loader with download, parse, convert, partition, index CLI"}
{"id":"parquedb-6pga","title":"Resource leak: archivedEvents array grows unbounded","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:42:10.681032-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:20:53.557909-06:00","closed_at":"2026-02-03T12:20:53.557909-06:00","close_reason":"Implemented maxArchivedEvents config option (default 50000) to limit archived events array size. Added pruneArchivedEvents function that removes oldest events when limit is exceeded. Updated both maybeRotateEventLog and archiveEvents methods in core.ts and events.ts to prune archived events. Added 6 tests to verify the fix."}
{"id":"parquedb-6qy9","title":"Add Client RPC unit tests","description":"Client code minimally tested (5 files):\n- ParqueDBClient.ts\n- collection.ts\n- rpc-promise.ts\n- service-binding.ts\n\nCreate comprehensive tests for RPC client initialization, collection wrapper, and promise handling.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T12:42:27.889158-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:07:24.689069-06:00","closed_at":"2026-02-01T13:07:24.689069-06:00","close_reason":"Completed by parallel agents"}
{"id":"parquedb-6rl","title":"[RED] Upsert tests","description":"Write failing tests for upsert behavior","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:37.403951-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T04:45:59.280656-06:00","closed_at":"2026-02-02T04:45:59.280656-06:00","close_reason":"Tests implemented in earlier waves"}
{"id":"parquedb-6s1o","title":"Fix GeneratedContentMV tests (19 failing)","description":"tests/unit/observability/ai/GeneratedContentMV.test.ts has 19 failing tests. Error: Cannot read properties of undefined (reading 'length'). Mock implementation needs findOne support.","status":"open","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:15.590898-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:15.590898-06:00"}
{"id":"parquedb-6sl","title":"Write: API Reference - ParqueDB class","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T12:40:31.875878-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:37:46.762347-06:00","closed_at":"2026-02-01T14:37:46.762347-06:00","close_reason":"Closed"}
{"id":"parquedb-70m","title":"[RED] R2Backend tests","description":"Write failing tests for R2Backend","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:52.537234-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T04:45:59.066322-06:00","closed_at":"2026-02-02T04:45:59.066322-06:00","close_reason":"Tests implemented in earlier waves"}
{"id":"parquedb-70ol","title":"MemoryBackend.append() is not thread-safe","description":"**High: Race Condition**\n\nIn /src/storage/MemoryBackend.ts lines 313-327, the append() method:\n\n```typescript\nasync append(path: string, data: Uint8Array): Promise\u003cvoid\u003e {\n  path = this.normalizePath(path)\n  const existing = this.files.get(path)\n  if (existing) {\n    // Append to existing file\n    const newData = new Uint8Array(existing.data.length + data.length)\n    newData.set(existing.data, 0)\n    newData.set(data, existing.data.length)\n    await this.write(path, newData)\n  } else {\n    await this.write(path, data)\n  }\n}\n```\n\nThere's no locking or atomicity - if two concurrent appends occur:\n1. Both read existing.data\n2. Both create newData with their append\n3. Both write - second write overwrites first\n\n**Impact**: Lost append data under concurrent access.\n\n**Note**: While MemoryBackend is often used for testing, it should still be correct for concurrent scenarios. Consider using atomic read-modify-write semantics.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:36:26.855911-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:49:11.749783-06:00","closed_at":"2026-02-03T12:49:11.749783-06:00","close_reason":"Closed"}
{"id":"parquedb-71j","title":"Query Engine","description":"MongoDB-style query and filter execution","status":"closed","priority":1,"issue_type":"epic","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:50:27.915343-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:34:08.794109-06:00","closed_at":"2026-02-01T14:34:08.794109-06:00","close_reason":"Closed"}
{"id":"parquedb-722c","title":"Create parseEntityId() utility function","description":"Entity ID parsing uses fragile string splitting repeated throughout the codebase:\n\n```typescript\n// src/Collection.ts:552-554, 597, 642, 703\nconst targetNs = rel.to.split('/')[0]\nconst targetLocalId = rel.to.split('/').slice(1).join('/')\n```\n\nCreate utility function:\n```typescript\nfunction parseEntityId(id: EntityId): { ns: Namespace; localId: string }\n```\n\nBenefits:\n- Single source of truth for ID format\n- Validation in one place\n- DRY compliance","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:31:45.269486-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:06:40.91808-06:00","closed_at":"2026-02-03T10:06:40.91808-06:00","close_reason":"Closed"}
{"id":"parquedb-7309","title":"Refactor: Reuse query execution in MV refresh","description":"incremental.ts reimplements filter/projection logic. Should reuse src/query/filter.ts and src/query/project.ts instead.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:15:41.610572-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:39:39.858554-06:00","closed_at":"2026-02-03T10:39:39.858554-06:00","close_reason":"Closed"}
{"id":"parquedb-748","title":"Implement unique indexes","description":"Unique constraint enforcement","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:52:00.719845-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:32:47.12911-06:00","closed_at":"2026-02-01T14:32:47.12911-06:00","close_reason":"Unique indexes implemented with full TDD: 23 tests covering HashIndex and SSTIndex unique constraints, composite keys, sparse indexes"}
{"id":"parquedb-74pu","title":"Replace setTimeout in tests with deterministic timing","description":"Testing: Several tests use setTimeout for timing which is flaky in CI. Replace with vitest fake timers and manual time advancement","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:09:48.260524-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:44:00.762017-06:00","closed_at":"2026-02-03T10:44:00.762017-06:00","close_reason":"Closed"}
{"id":"parquedb-77ux","title":"Create production runbooks and troubleshooting docs","description":"Missing operational documentation:\n\nCreate docs for:\n- Deployment checklist (Workers, R2, DO setup)\n- Monitoring setup (metrics to track)\n- Common error codes and resolutions\n- Performance tuning guide\n- Capacity planning guidelines\n- Incident response procedures","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:32:10.296056-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:05:44.146952-06:00","closed_at":"2026-02-03T10:05:44.146952-06:00","close_reason":"Closed"}
{"id":"parquedb-78x6","title":"Fix memory leak from global state","description":"Global Maps (globalEntityStore, globalEventStore, globalSnapshotStore, globalQueryStats) at ParqueDB.ts lines 442-485 are keyed by StorageBackend with no cleanup mechanism. Implement dispose() method or use WeakMap to prevent memory leaks in long-running processes.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:09.122977-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T15:51:01.980569-06:00","closed_at":"2026-02-01T15:51:01.980569-06:00","close_reason":"Closed"}
{"id":"parquedb-7a7r","title":"Add max retry limit to AIRequestBuffer flush","description":"## Issue\nIn `src/streaming/ai-requests.ts`, the `AIRequestBuffer.flush()` method puts failed requests back into the buffer with `unshift()`. If the database is permanently unavailable, this causes unbounded buffer growth and memory exhaustion.\n\n## Location\n`src/streaming/ai-requests.ts:1438-1441`\n\n```typescript\n.catch(err =\u003e {\n  // Put failed requests back in buffer\n  this.buffer.unshift(...toFlush)\n  this.flushPromise = null\n  throw err\n})\n```\n\n## Risk\nHigh - Unbounded retries can cause memory exhaustion if the database becomes unavailable.\n\n## Suggested Fix\nAdd a retry counter to each request or limit the number of times requests can be re-added to the buffer. Optionally add an onDrop callback for discarded requests.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:12.73838-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:36:16.190149-06:00","closed_at":"2026-02-03T11:36:16.190149-06:00","close_reason":"Fixed: Added retry tracking with configurable maxFlushRetries (default 3), onDrop callback, and getDroppedCount() method."}
{"id":"parquedb-7az","title":"[GREEN] FsxBackend implementation","description":"Implement FsxBackend to pass tests","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:56.500179-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:15:29.808185-06:00","closed_at":"2026-02-01T14:15:29.808185-06:00","close_reason":"Closed"}
{"id":"parquedb-7bdz","title":"Testing: Add E2E tests for Durable Object SQLite persistence path","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:49:47.443167-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:20:08.958804-06:00","closed_at":"2026-02-03T16:20:08.958804-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-7ci6","title":"Add index awareness to aggregation $match","description":"Aggregation $match stages don't leverage secondary indexes. When $match is the first stage, it should use HashIndex for equality and SSTIndex for range queries like the regular find() does.","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T17:01:06.944131-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T17:17:40.971405-06:00","closed_at":"2026-02-01T17:17:40.971405-06:00","close_reason":"Closed"}
{"id":"parquedb-7d8q","title":"Potential Integer Overflow in Snapshot ID Generation","description":"**File:** src/backends/iceberg-commit.ts (lines 308-317)\n\n**Issue:** Date.now() * 1000 + counter approaching Number.MAX_SAFE_INTEGER\n\n**Impact:** Could generate duplicate/invalid snapshot IDs\n\n**Fix:** Use BigInt or UUID-based approach for snapshot ID generation","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T14:34:11.452345-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:44:37.170869-06:00","closed_at":"2026-02-03T14:44:37.170869-06:00","close_reason":"Closed"}
{"id":"parquedb-7eao","title":"Query Optimizer","description":"Add query planning and optimization to ParqueDB\n\n## Scope\n- Index selection for optimal query execution\n- Predicate pushdown planning\n- Query cost estimation\n\n## Acceptance Criteria\n- [ ] Query planner selects best available indexes\n- [ ] Predicate pushdown maximizes Parquet efficiency\n- [ ] Cost estimation helps choose between strategies\n- [ ] Query plans can be explained/visualized","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:58:03.671128-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:11:49.854132-06:00","closed_at":"2026-02-03T09:11:49.854132-06:00","close_reason":"Closed"}
{"id":"parquedb-7efk","title":"Wire IndexManager to actual index classes","description":"IndexManager methods hashLookup(), rangeQuery(), ftsSearch() throw 'not implemented'. Wire them to HashIndex, SSTIndex, FTSIndex classes that already exist. IndexCache in worker has separate implementation that should be unified.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T05:32:39.761144-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T03:48:28.09996-06:00","closed_at":"2026-02-03T03:48:28.09996-06:00","close_reason":"Closed"}
{"id":"parquedb-7etu","title":"Fix: IngestSource union defeats type safety","description":"IngestSource = 'ai-sdk' | 'tail' | 'evalite' | string makes union effectively just string. Use template literal: 'ai-sdk' | 'tail' | 'evalite' | `custom:${string}`. File: src/materialized-views/types.ts line 141","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:15:21.801061-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:42:10.779867-06:00","closed_at":"2026-02-03T10:42:10.779867-06:00","close_reason":"Closed"}
{"id":"parquedb-7fy","title":"Implement FsxBackend (fsx for Workers)","description":"Storage backend using fsx for Cloudflare Workers","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:54.854849-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:15:29.623981-06:00","closed_at":"2026-02-01T14:15:29.623981-06:00","close_reason":"Closed"}
{"id":"parquedb-7hfj","title":"Add incremental vector index updates for streaming data","description":"Currently, the VectorIndex supports insert/update/delete operations, but when the underlying Parquet data changes (e.g., via compaction or new row groups), the index must be fully rebuilt.\n\nFor streaming or frequently updated datasets, this is inefficient.\n\nNeeded features:\n1. Detect which row groups have changed\n2. Remove stale entries for deleted/modified documents\n3. Insert new entries for added documents\n4. Handle row group renumbering after compaction\n\nImplementation considerations:\n- Store version/etag metadata in index\n- Track docId -\u003e (rowGroup, rowOffset) mappings\n- Implement partial rebuild from specific row groups\n\nSee `build()` method in hnsw.ts:811-834 which currently clears and rebuilds entirely.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:15:50.949777-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:08:23.9569-06:00","closed_at":"2026-02-03T15:08:23.9569-06:00","close_reason":"Closed"}
{"id":"parquedb-7hw","title":"[RED] Update operator tests - field operators","description":"Write failing tests for $set, $unset, $rename","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:20.532526-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:08:21.621781-06:00","closed_at":"2026-02-01T14:08:21.621781-06:00","close_reason":"Closed"}
{"id":"parquedb-7jin","title":"Add CLI tools","description":"No CLI for database management. Create parquedb CLI with commands: init, migrate, query, import, export, stats, compact.","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:47.409366-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T16:41:29.609431-06:00","closed_at":"2026-02-01T16:41:29.609431-06:00","close_reason":"Closed"}
{"id":"parquedb-7kf","title":"[GREEN] R2Backend implementation","description":"Implement R2Backend to pass tests","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:53.393854-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:16:38.548554-06:00","closed_at":"2026-02-01T14:16:38.548554-06:00","close_reason":"R2Backend implementation complete. GREEN phase achieved - all 38 integration tests pass against real R2/S3-compatible storage."}
{"id":"parquedb-7km1","title":"Fix 19 TypeScript compilation errors","description":"npx tsc --noEmit reports 19 errors: (1) File casing conflict Collection.ts vs collection.ts in query/builder.ts import, (2) 14 errors in mutation/index.ts - need 'export type' for type-only exports, (3) Undefined checks needed in aggregation/executor.ts:367, (4) Missing function argument in mutation/executor.ts:440, (5) EntityId type mismatch in mutation/operators.ts:265,284","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T17:00:48.308236-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T17:03:08.122449-06:00","closed_at":"2026-02-01T17:03:08.122449-06:00","close_reason":"Closed"}
{"id":"parquedb-7nn","title":"[GREEN] Time-travel query implementation","description":"Implement time-travel via event replay","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:40.233009-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:20:45.356534-06:00","closed_at":"2026-02-01T14:20:45.356534-06:00","close_reason":"Closed"}
{"id":"parquedb-7q2j","title":"Remove TailDO direct mode - use only event-driven","description":"The TailDO was implemented with both 'direct mode' and 'event-driven mode' for backward compatibility. But there are no existing users, so we should just use event-driven.\n\n## Changes Needed\n\n1. **Remove RAW_EVENTS_ENABLED flag** - always use event-driven\n2. **Remove direct WorkerLogsMV calls** from TailDO\n3. **Simplify TailDO** - just buffer and write to R2\n4. **Update docs** - remove references to direct mode\n\n## Result\n- Simpler code\n- One architecture to maintain\n- No config flag needed\n\nTailDO  R2 (raw)  Queue  Compaction  Parquet\n\nThat's it. No alternatives.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:21:21.453791-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:34:18.018099-06:00","closed_at":"2026-02-03T11:34:18.018099-06:00","close_reason":"Closed"}
{"id":"parquedb-7r3b","title":"Fix: Loose index signature in CollectionDefinition","description":"[field: string]: unknown is too permissive, conflicts with specific properties, defeats type safety. File: src/materialized-views/types.ts lines 164-173","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:16:00.345267-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:40:18.489718-06:00","closed_at":"2026-02-03T10:40:18.489718-06:00","close_reason":"Closed"}
{"id":"parquedb-7tin","title":"Implement schema evolution for field removals, renames, and type changes","description":"The IcebergBackend.setSchema() method currently only supports adding new fields. Implement support for field removals, renames, and type changes with proper data migration.","status":"open","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T18:03:24.071505-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T18:03:24.071505-06:00"}
{"id":"parquedb-7tx","title":"[RED] Populate tests","description":"Write failing tests for populating related entities","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:41.764943-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T04:45:59.219177-06:00","closed_at":"2026-02-02T04:45:59.219177-06:00","close_reason":"Tests implemented in earlier waves"}
{"id":"parquedb-7vkz","title":"DOSqliteBackend.writeConditional has race condition - needs SQLite transaction","description":"**Critical: Race Condition**\n\nIn /src/storage/DOSqliteBackend.ts lines 515-548, the writeConditional() method:\n\n1. Reads etag with SELECT (line 525-529)\n2. Compares with expectedVersion\n3. Calls this.write() which does another SELECT + INSERT OR REPLACE\n\nThese are separate SQL statements without a transaction wrapper. Between steps 1 and 3, another write could occur.\n\n**Impact**: Optimistic concurrency control can fail, leading to lost updates.\n\n**Fix**: Wrap the entire operation in a SQLite transaction:\n```sql\nBEGIN TRANSACTION;\n-- SELECT to check etag\n-- INSERT OR REPLACE if matches\nCOMMIT;\n```\n\nNote: The DO SQLite API may need to support transaction control. If not, use a single atomic SQL statement with WHERE clause checking the etag.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:35.734639-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:46:54.733146-06:00","closed_at":"2026-02-03T11:46:54.733146-06:00","close_reason":"Closed"}
{"id":"parquedb-7vui","title":"P1: Fix N+1 query in relationship hydration","description":"src/Collection.ts:890-909 - related() fetches entities one at a time. Batch entity fetches by namespace.","status":"open","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:02.866638-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:02.866638-06:00"}
{"id":"parquedb-7ypb","title":"Consolidate duplicate getNestedValue implementations","description":"getNestedValue function is duplicated in: utils/comparison.ts:156-174, indexes/secondary/hash.ts:467-478, indexes/secondary/sst.ts:495-506. Use canonical implementation from utils/comparison.ts everywhere.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T05:32:48.286665-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T06:32:50.267928-06:00","closed_at":"2026-02-02T06:32:50.267928-06:00","close_reason":"Closed"}
{"id":"parquedb-80pq","title":"Write: Stream collections concept doc","description":"Create new docs/concepts/stream-collections.md explaining the  directive, imported collections, and how they differ from regular collections","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:28:19.718476-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:29:41.471471-06:00","closed_at":"2026-02-03T09:29:41.471471-06:00","close_reason":"Closed"}
{"id":"parquedb-80x5","title":"Fix: Race condition in batch processing","description":"Fixed race conditions in RelationshipBatchLoader.flush() and EmbeddingQueue.processQueue() that could cause concurrent operations to process the same items multiple times or lose requests. Added mutex locks to serialize flush/process operations.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:19:29.426227-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:19:35.640723-06:00","closed_at":"2026-02-03T10:19:35.640727-06:00"}
{"id":"parquedb-819m","title":"Refactor: Storage - FsxBackend uses generic Error instead of typed errors","description":"FsxBackend throws generic Error instances instead of using typed error classes:\\n\\nAt lines 274-277 and 284-299:\\n```typescript\\nif (options?.ifNoneMatch === '*') {\\n  const fileExists = await this.fsx.exists(fullPath)\\n  if (fileExists) {\\n    throw new Error('File already exists')  // Should be FileExistsError\\n  }\\n}\\n...\\nthrow new Error('Version mismatch: file exists but expected it not to')  // Should be VersionMismatchError\\nthrow new Error(\\`Version mismatch: expected ${expectedVersion}, got ${stats.etag}\\`)  // Should be VersionMismatchError\\n```\\n\\nRefactor to:\\n- Import and use typed error classes (FileExistsError, VersionMismatchError)\\n- Ensure consistent error handling with other backends\\n- Add proper error mapping from FsxError to typed errors\\n\\nFiles affected:\\n- /Users/nathanclevenger/projects/parquedb/src/storage/FsxBackend.ts","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:32:54.897938-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:34:40.687041-06:00","closed_at":"2026-02-03T11:34:40.687041-06:00","close_reason":"Closed"}
{"id":"parquedb-81gh","title":"Fix flush-race-condition tests","description":"tests/unit/flush-race-condition.test.ts has 2 failing tests: should properly rollback entity state on flush failure, should remove failed events from pending queue.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:32:04.923584-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:33:04.439042-06:00","closed_at":"2026-02-03T12:33:04.439042-06:00","close_reason":"Closed"}
{"id":"parquedb-828b","title":"Implement Delta log entries during compaction","description":"When writing to Delta format, must write Delta log entry. Currently marked as TODO at src/workflows/compaction-migration.ts:428. Required for Delta Lake compatibility.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:56:36.208925-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:14:11.899073-06:00","closed_at":"2026-02-03T13:14:11.899073-06:00","close_reason":"Closed"}
{"id":"parquedb-838h","title":"Remove dangerous new Function() in RPC legacy deserializer","description":"The deserializeLegacyFunction() in src/client/rpc-promise.ts:575-600 uses new Function() which is a code injection risk. Either remove the legacy mapper type entirely or replace with capnweb RPC.do. Critical security issue.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T05:32:27.838601-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T06:30:19.428971-06:00","closed_at":"2026-02-02T06:30:19.428971-06:00","close_reason":"Closed"}
{"id":"parquedb-84xq","title":"Refactor ParqueDB.ts into smaller classes","description":"ParqueDB.ts is 3,680 lines - should be 300-500. Extract into:\n- EntityManager - handle entity CRUD\n- RelationshipManager - handle relationships\n- SnapshotManager - handle snapshots\n- EventManager - handle event log\nKeep ParqueDB as orchestrator.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T12:42:17.335174-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:39:45.726096-06:00","closed_at":"2026-02-01T14:39:45.726096-06:00","close_reason":"Deferred: File is large (3749 lines) but internally well-organized with clear section comments. Shared state is deeply intertwined (44+ references across entities, events, snapshots, storage). Extracting 4 managers with proper coordination is not minimal/backwards-compatible. 32+ importing files and 80+ tests make this high-risk. Existing src/events/ module unused - future refactor should integrate. Recommend as future milestone with proper planning."}
{"id":"parquedb-89pi","title":"Expand transaction system tests","description":"Transaction system has only 1 test file (tests/unit/transaction/index.test.ts).\n\nAdd tests for:\n- Multi-operation transactions\n- Abort/rollback scenarios\n- Nested transactions (if supported)\n- Concurrent transaction conflicts\n- Transaction timeout handling\n- Recovery after crash","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:31:58.187386-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:08:23.692214-06:00","closed_at":"2026-02-03T10:08:23.692214-06:00","close_reason":"Closed"}
{"id":"parquedb-8aar","title":"Add streaming support to migration utilities","description":"Migration utilities load entire files into memory. Add streaming support for large file imports using Node.js streams or async iterators to handle files larger than available memory.","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T17:01:06.316852-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T04:43:33.83976-06:00","closed_at":"2026-02-02T04:43:33.83976-06:00","close_reason":"Completed by agents"}
{"id":"parquedb-8c4","title":"[RED] Filter evaluation tests - comparison operators","description":"Write failing tests for $eq, $ne, $gt, $gte, $lt, $lte, $in, $nin","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:05.906217-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:13:07.40126-06:00","closed_at":"2026-02-01T13:13:07.40126-06:00","close_reason":"Closed"}
{"id":"parquedb-8fql","title":"Testing: Reduce test flakiness - fix 65 time-dependent tests using Promise delays","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:49:43.412793-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:20:08.966499-06:00","closed_at":"2026-02-03T16:20:08.966499-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-8gi","title":"Benchmark: Node.js FsBackend disk I/O","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T15:47:12.48-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-30T16:05:02.067935-06:00","closed_at":"2026-01-30T16:05:02.067935-06:00","close_reason":"FsBackend Disk I/O Benchmarks completed. Key results: Cold reads ~200-550ms depending on file size, metadata reads ~1.5ms p50, byte-range reads sub-millisecond, column projection shows 4x improvement when reading fewer columns. Standalone benchmark script created at tests/benchmarks/run-fs-benchmark.ts for accurate disk I/O measurement."}
{"id":"parquedb-8gmd","title":"Refactor: Worker - QueryExecutor._extractIdFilter and _fileSizeCache are public but unused","description":"QueryExecutor has several public members prefixed with underscore that appear to be internal or reserved:\n\n1. _extractIdFilter() - documented as '@internal Reserved for compatibility'\n2. _fileSizeCache - commented as 'reserved for future optimization'\n3. _MAX_CACHE_SIZE_LIMIT - 'reserved for future size limits'\n4. _bucket, _cdnBucket, _r2DevUrl - constructor params exposed\n\nFiles:\n- src/worker/QueryExecutor.ts\n\nThese suggest:\n1. Internal methods that shouldn't be public\n2. Future features that aren't implemented\n3. Debug/test exposure of private state\n\nRefactor:\n- Make truly internal methods private\n- Remove reserved fields until needed\n- If public exposure needed for testing, use a dedicated test interface\n\nImpact: Cleaner public API, clearer intent","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:34:16.441342-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:00:02.645054-06:00","closed_at":"2026-02-03T11:00:02.645054-06:00","close_reason":"Closed"}
{"id":"parquedb-8he4","title":"Define ParqueDBDOStub interface for DO RPC casts","description":"worker/index.ts has 8 identical 'as unknown as { method(...) }' casts for DO RPC calls. Define a ParqueDBDOStub interface describing the RPC surface and cast once.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T07:16:02.623685-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T03:38:49.548908-06:00","closed_at":"2026-02-03T03:38:49.548908-06:00","close_reason":"Closed"}
{"id":"parquedb-8i3k","title":"Streaming merge-sort for large windows exceeding memory","description":"Current merge-sort loads all rows into memory - fails for large windows. Implement streaming/external merge-sort: 1) Read files in chunks (e.g., 10K rows), 2) Use min-heap for k-way merge, 3) Stream output directly to R2 multipart upload, 4) Memory bounded regardless of window size. Alternative: split large windows into sub-windows and compact hierarchically. Critical for production with large data volumes.","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:57:29.972324-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:17:08.516326-06:00","closed_at":"2026-02-03T14:17:08.516326-06:00","close_reason":"Closed"}
{"id":"parquedb-8jm","title":"[REFACTOR] Schema parsing cleanup","description":"Refactor schema parsing","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:52:00.445618-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:16:31.069574-06:00","closed_at":"2026-02-01T13:16:31.069574-06:00","close_reason":"Closed"}
{"id":"parquedb-8mhm","title":"DeltaBackend catches errors without checking type - silent failures","description":"**High: Silent Failures**\n\nIn /src/backends/delta.ts, numerous catch blocks swallow all errors:\n\n1. Lines 497-499, 503-504 (listSnapshots):\n```typescript\n} catch {\n  // Skip invalid commit files\n}\n```\n\n2. Lines 531-535 (getSchema):\n```typescript\n} catch {\n  // Invalid schema JSON in metadata\n  return null\n}\n```\n\n3. Lines 1018-1021 (getCurrentVersion - checkpoint parsing):\n```typescript\n} catch {\n  // Invalid checkpoint JSON - fall through to scan from beginning\n  throw new Error('Invalid checkpoint JSON')\n}\n```\n\n4. Lines 1072-1075 (parseCommitFile):\n```typescript\n} catch {\n  // Skip invalid JSON lines in commit file\n  continue\n}\n```\n\n5. Lines 1425-1426 (createCheckpoint):\n```typescript\n} catch {\n  // Skip unreadable commits\n}\n```\n\n6. Lines 1619 and 1658 (readEntitiesAtVersion):\n```typescript\n} catch {\n  // Skip unreadable commits/files\n}\n```\n\n**Impact**: Permission errors, network errors, and corruption are treated as 'file not found'.\n\n**Fix**: Check if error is NotFoundError before swallowing. Log or propagate other errors.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:37:09.493844-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:18:26.416815-06:00","closed_at":"2026-02-03T12:18:26.416815-06:00","close_reason":"Fixed all catch blocks in DeltaBackend to check for NotFoundError before swallowing. Other errors (permission, network, corruption) are now properly propagated."}
{"id":"parquedb-8o8k","title":"Implement WAL-only SQLite for cost reduction","description":"Current DO SQLite stores both entity state and event log (4.5x more expensive than R2). Implement WAL-only mode as proposed in docs/architecture/DO_WAL_REWRITE.md for 50-70% cost reduction.","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T16:28:06.085504-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:55:53.48534-06:00","closed_at":"2026-02-03T16:55:53.48534-06:00","close_reason":"Implemented WAL-only SQLite schema mode in src/worker/do/wal-only-schema.ts with 38 passing tests. Separates WAL core tables from snapshot tables, providing 50-70% DO storage cost reduction. Added WAL optimizer constants to src/constants.ts."}
{"id":"parquedb-8oie","title":"Add rate limiting to public endpoints","description":"No rate limiting on public API endpoints or remote client requests. Risk of DoS attacks and database enumeration. Implement rate limiting at worker level or use Cloudflare's built-in rate limiting.","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:05:06.356521-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:46:21.437427-06:00","closed_at":"2026-02-03T08:46:21.437427-06:00","close_reason":"Closed"}
{"id":"parquedb-8p4","title":"[RED] upsertMany tests","description":"Write failing tests for Collection.upsertMany(). Bulk upsert with conflict handling for loading large datasets.","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T14:30:03.690734-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-30T14:32:17.069864-06:00","closed_at":"2026-01-30T14:32:17.069864-06:00","close_reason":"Closed"}
{"id":"parquedb-8qj3","title":"Deploy real dataset snippets to cdn.workers.do/search","description":"Deploy ParqueDB snippets with real datasets to test Cloudflare Snippets limits.\n\n## Datasets (already at https://cdn.workers.do/parquedb-benchmarks/)\n- O*NET (occupational data)\n- UNSPSC (product classification)  \n- IMDB (movies/TV)\n- Wiktionary (dictionary)\n\n## Deployment Target\n- Route: cdn.workers.do/search/*\n- CF_API_TOKEN and CF_ZONE_ID in .env\n\n## Goals\n- Test 5ms CPU limit with real data\n- Test 5 subrequest limit\n- Test 32KB memory limit\n- Measure actual query performance\n- Find the limits of what's possible\n\n## Snippet Routes\n- /search/onet?q=software - O*NET occupation search\n- /search/unspsc?q=computer - UNSPSC category lookup\n- /search/imdb?q=matrix - IMDB title search\n- /search/wiktionary?q=hello - Wiktionary definition lookup\n\n## Tasks\n1. Create snippets for each dataset\n2. Build pre-computed indexes for fast lookups\n3. Deploy to Cloudflare Snippets API\n4. Test and measure performance\n5. Document what works and what hits limits","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:44:43.00791-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:58:12.284267-06:00","closed_at":"2026-02-03T10:58:12.284267-06:00","close_reason":"Closed"}
{"id":"parquedb-8r4x","title":"P3: Make relationship mapping schema-driven","description":"src/Collection.ts:843-849 - Hardcoded reverse relationship mappings. Make configurable or derive from schema.","status":"open","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:31.55079-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:31.55079-06:00"}
{"id":"parquedb-8rhj","title":"Unify IngestSource type definitions","description":"Three different IngestSource definitions exist: types.ts (loose 'literal | string'), ingest-source.ts (correct template literal), streaming/views.ts (different pattern). Remove duplicates, export only from ingest-source.ts.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:02:16.746029-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:04:25.679492-06:00","closed_at":"2026-02-03T11:04:25.679492-06:00","close_reason":"Closed"}
{"id":"parquedb-8rld","title":"Define constants for magic numbers in search worker","description":"Magic numbers scattered: default limits (50, 50, 20), max limit (100), cache TTLs (3600, 3600, 300). Should be named constants.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:27:33.301102-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T17:03:48.684091-06:00","closed_at":"2026-02-03T17:03:48.684091-06:00","close_reason":"All 10 search worker snippet files now use named constants instead of magic numbers"}
{"id":"parquedb-8sg","title":"[GREEN] Unique index implementation","description":"Implement unique indexes to pass tests","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:52:03.275538-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:32:42.825176-06:00","closed_at":"2026-02-01T14:32:42.825176-06:00","close_reason":"GREEN phase complete: Implemented unique constraint enforcement in HashIndex and SSTIndex with UniqueConstraintError, checkUnique method, and sparse index support"}
{"id":"parquedb-8t4q","title":"Fix temp directory race conditions in tests","description":"ParqueDB.test.ts shows 37 ENOENT errors from temp directory race conditions. Implement proper async cleanup with retries and unique directories per test.","status":"open","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:03.621547-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:03.621547-06:00"}
{"id":"parquedb-8t8","title":"[REFACTOR] Create operation cleanup","description":"Refactor create operations","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:17.816875-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:05:20.687407-06:00","closed_at":"2026-02-01T14:05:20.687407-06:00","close_reason":"Closed"}
{"id":"parquedb-8tmn","title":"Add: Error recovery and partial failure tests","description":"Minimal error handling tests. Add tests for partial batch failure recovery, rollback behavior, and retry semantics.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:15:55.231249-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:46:40.453579-06:00","closed_at":"2026-02-03T10:46:40.453579-06:00","close_reason":"Closed"}
{"id":"parquedb-8u6l","title":"Complete CLI push/pull implementation","description":"The push and pull commands in sync.ts simulate file operations instead of actually syncing. Need to implement: 1) Get presigned URLs from service, 2) Upload/download files to/from R2, 3) Update manifests properly.","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:05:03.409849-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:46:16.450886-06:00","closed_at":"2026-02-03T08:46:16.450886-06:00","close_reason":"Closed"}
{"id":"parquedb-8ybw","title":"Create constants file for magic numbers","description":"Magic numbers scattered throughout code:\n- FNV_OFFSET_BASIS = 2166136261 (encoding.ts, key-encoder.ts)\n- FNV_PRIME = 16777619\n- MAX_CACHE_SIZE = 2*1024*1024 (QueryExecutor.ts - defined twice!)\n- DEFAULT_BLOOM_SIZE = 131072\n- INDEX_PROGRESS_BATCH = 10000\n\nCreate /src/constants.ts and update all references.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T12:42:20.011954-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T12:54:07.355568-06:00","closed_at":"2026-02-01T12:54:07.355568-06:00","close_reason":"Completed by parallel agents"}
{"id":"parquedb-90nf","title":"Replace SimpleDB with real ParqueDB in tests","description":"crud-workflow.test.ts implements a simplified SimpleDB class (lines 28-223) instead of using real ParqueDB. This tests a mock, not actual implementation. Replace with ParqueDB + MemoryBackend to catch real bugs.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:30.879905-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T16:06:29.267331-06:00","closed_at":"2026-02-01T16:06:29.267331-06:00","close_reason":"Closed"}
{"id":"parquedb-955","title":"[RED] Relationship reading tests","description":"Write failing tests for reading outbound and inbound relationships","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:30.166354-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T04:45:59.528318-06:00","closed_at":"2026-02-02T04:45:59.528318-06:00","close_reason":"Tests implemented in earlier waves"}
{"id":"parquedb-95vl","title":"Deploy test-search worker route for debugging","description":"Deploy a parallel test route for the search worker:\n- Route: /test-search/* (Worker with full observability)\n- Route: /search/* (eventually Snippet, free tier)\n\nThis allows debugging and optimization before Snippet deployment.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:16:14.544461-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:56:05.53319-06:00","closed_at":"2026-02-03T15:56:05.53319-06:00","close_reason":"Closed"}
{"id":"parquedb-96o","title":"Benchmark: Node.js to remote R2Backend","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T15:47:13.440909-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-30T16:03:05.711796-06:00","closed_at":"2026-01-30T16:03:05.711796-06:00","close_reason":"Completed R2 remote benchmark from Node.js to Cloudflare R2.\n\n## Benchmark Results\n\n### Test Environment\n- Location: Node.js on macOS\n- Target: Cloudflare R2 (parquedb bucket)\n- Protocol: S3-compatible API via @aws-sdk/client-s3\n\n### Single File Read Latency\n| Size | Avg | P50 | P95 |\n|------|-----|-----|-----|\n| 1KB | 171ms | 171ms | 196ms |\n| 64KB | 176ms | 191ms | 227ms |\n| 1MB | 232ms | 173ms | 608ms |\n| 8MB | 559ms | 508ms | 970ms |\n\n### Range Read Latency (Parquet Row Groups)\n| Range | Avg | P50 | P95 |\n|-------|-----|-----|-----|\n| footer (8 bytes) | 145ms | 118ms | 227ms |\n| metadata (4KB) | 125ms | 109ms | 194ms |\n| row group 64KB | 140ms | 129ms | 205ms |\n| row group 512KB | 157ms | 138ms | 218ms |\n| middle 1MB | 258ms | 251ms | 318ms |\n\n### List Operations\n| Files | Avg | P50 | P95 |\n|-------|-----|-----|-----|\n| 10 | 129ms | 129ms | 161ms |\n| 50 | 136ms | 124ms | 193ms |\n| 100 | 163ms | 125ms | 413ms |\n\n### HEAD Operations\nAverage: ~121ms (file size independent)\n\n### Concurrent Operations\n| Concurrency | Total | Avg/File |\n|-------------|-------|----------|\n| 5 | 200ms | 40ms |\n| 10 | 224ms | 22ms |\n| 20 | 250ms | 13ms |\n\n## Key Findings\n1. **Base latency**: ~100-120ms for any R2 operation (network RTT)\n2. **Throughput scales**: Concurrent reads show near-linear scaling\n3. **Range reads efficient**: Small range reads (footer, metadata) add minimal overhead over HEAD\n4. **File size impact**: Only significant for files \u003e1MB\n\n## Benchmark File\ntests/benchmarks/r2-remote.bench.ts updated with:\n- dotenv loading for credentials\n- Correct default bucket (parquedb)\n- Comprehensive test coverage\n\nRun with: npx vitest bench tests/benchmarks/r2-remote.bench.ts"}
{"id":"parquedb-97kj","title":"Fix inconsistent message acknowledgment in queue consumer","description":"In compaction-queue-consumer.ts, files with unexpected formats are ack'd (line 188) but files without namespace prefix are not (line 172-175). This inconsistency could lead to message redelivery loops. Standardize: ack all processed messages regardless of filter outcome.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:56:31.166448-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:57:53.145722-06:00","closed_at":"2026-02-03T12:57:53.145722-06:00","close_reason":"Closed"}
{"id":"parquedb-9ays","title":"P1: Add transaction size limits to TransactionalBackend","description":"The originalStates array in src/storage/TransactionalBackend.ts:171-206 grows without bounds during commit. Add configurable transaction size limit or streaming approach for large transactions to prevent memory pressure.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:10:27.80386-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:24:28.87965-06:00","closed_at":"2026-02-03T15:24:28.87965-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-9bq3","title":"Add override modifier to error classes","description":"Custom error classes in src/indexes/errors.ts need override modifier for name property per noImplicitOverride. Change 'readonly name = ...' to 'override readonly name = ...' for UniqueConstraintError and other custom errors.","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:40.037332-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T16:19:02.846284-06:00","closed_at":"2026-02-01T16:19:02.846284-06:00","close_reason":"Closed"}
{"id":"parquedb-9e30","title":"Fix ai-sdk-mv-integration test failures","description":"tests/unit/integrations/ai-sdk-mv-integration.test.ts has 30+ failing tests. All tests in AIObservabilityMVIntegration suite are failing. Need to investigate initialization, lifecycle, log processing, builtin views, custom views, and state management.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:25.806999-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:36:20.010753-06:00","closed_at":"2026-02-03T11:36:20.010753-06:00","close_reason":"Closed"}
{"id":"parquedb-9f1","title":"[RED] Real R2Backend tests","description":"Write failing tests for R2Backend using real R2 bucket. Use .env S3-compat creds from Node, real bindings in Workers.","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T14:30:00.346069-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-30T14:37:13.832826-06:00","closed_at":"2026-01-30T14:37:13.832826-06:00","close_reason":"Closed"}
{"id":"parquedb-9fba","title":"Fix documentation inaccuracy for sharded window keys","description":"docs/guides/compaction-workflow.md shows window key format as 'users:1700000000000' but after namespace sharding, the actual format is just '1700000000000' (timestamp only, since namespace is implicit per-DO). Update documentation to reflect current implementation.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:33:35.647936-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:45:39.633541-06:00","closed_at":"2026-02-03T13:45:39.633541-06:00","close_reason":"Closed"}
{"id":"parquedb-9fmk","title":"Two-Phase Commit Race Condition in Compaction Queue Consumer","description":"**File:** src/workflows/compaction-queue-consumer.ts\n\n**Issue:** Crash between workflow.create() and confirm-dispatch can cause duplicate processing\n\n**Impact:** Potential duplicate compaction and data corruption\n\n**Fix:** Include workflow ID in initial processing state, check workflow status on timeout recovery to prevent duplicate processing","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T14:34:12.913076-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:49:43.066372-06:00","closed_at":"2026-02-03T14:49:43.066372-06:00","close_reason":"Closed"}
{"id":"parquedb-9fmv","title":"Add runtime validation at JSON deserialization boundaries","description":"After safeJsonParse(), code casts directly to IndexCatalog, ShardedIndexManifest etc without shape validation. Add type guard validators for critical JSON boundaries: IndexCache.ts:396, indexes/manager.ts, config/dataset.ts.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T07:16:02.822365-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T07:21:13.721116-06:00","closed_at":"2026-02-02T07:21:13.721116-06:00","close_reason":"Closed"}
{"id":"parquedb-9lhg","title":"Fix incomplete transaction rollback in ParqueDBDO","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:30.205094-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:49:18.553026-06:00","closed_at":"2026-02-03T08:49:18.553026-06:00","close_reason":"Closed"}
{"id":"parquedb-9m6","title":"[GREEN] FsBackend implementation","description":"Implement FsBackend to pass tests","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:49.898757-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:12:35.939821-06:00","closed_at":"2026-02-01T14:12:35.939821-06:00","close_reason":"Closed"}
{"id":"parquedb-9p14","title":"Extract constants for hardcoded magic numbers","description":"Hardcoded values should be extracted into named constants:\n- 128 * 1024 * 1024 (target file size)\n- 1000 (batch sizes)\n- 3 (retry counts)\n- 100, 200, 400 (backoff delays)\n\nCreate a constants file or config object to centralize these values.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:57:41.702698-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:02:49.677249-06:00","closed_at":"2026-02-03T11:02:49.677249-06:00","close_reason":"Closed"}
{"id":"parquedb-9qga","title":"Bug: Race condition in flushEvents - pendingEvents cleared before async writes","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:47.390346-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:46:15.162285-06:00","closed_at":"2026-02-03T11:46:15.162285-06:00","close_reason":"Closed"}
{"id":"parquedb-9tvv","title":"Inconsistent Naming Conventions (MVId vs ViewName)","description":"**Issue:** Inconsistent naming patterns across codebase - MVId vs ViewName, different casing patterns\n\n**Fix:** \n1. Establish naming conventions document\n2. Document in CLAUDE.md or CONTRIBUTING.md\n3. Refactor existing code to follow conventions\n\n**Impact:** Code consistency and reduced cognitive load","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:10:39.794547-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T18:22:33.258815-06:00","closed_at":"2026-02-03T18:22:33.258815-06:00","close_reason":"Fixed in commit ab28519"}
{"id":"parquedb-9vd3","title":"Studio: Add delete confirmation dialogs","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:34.297358-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:46:24.838089-06:00","closed_at":"2026-02-03T08:46:24.838089-06:00","close_reason":"Closed"}
{"id":"parquedb-9wl","title":"[GREEN] Filter string/array operators","description":"Implement string/array operators to pass tests","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:11.161543-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:03:47.588138-06:00","closed_at":"2026-02-01T14:03:47.588138-06:00","close_reason":"Closed"}
{"id":"parquedb-9yrd","title":"High: Flawed rollback in BranchManager checkout","description":"In branch-manager.ts (lines 326-341), when state reconstruction fails, the rollback logic is flawed. The code checks if currentBranch \\!== name (line 331), but at this point HEAD was already updated to point to 'name' at line 321, so this check will always be false. The HEAD is never actually rolled back on failure.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:07.725008-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:36:19.428769-06:00","closed_at":"2026-02-03T11:36:19.428769-06:00","close_reason":"Fixed by saving previousHeadState before updating HEAD, then using it to restore on failure"}
{"id":"parquedb-a03i","title":"Fix parquet-utils shredding tests","description":"tests/unit/backends/parquet-utils-shredding.test.ts has multiple failing tests for entityToRow and rowToEntity with shredding enabled. Tests for shredded fields extraction, null handling, data types, and edge cases.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:04:20.152651-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:10:55.717084-06:00","closed_at":"2026-02-03T12:10:55.717084-06:00","close_reason":"Closed"}
{"id":"parquedb-a0xs","title":"Fix StreamingRefreshEngine busy-wait polling","description":"The flush() method uses a busy-wait loop with setTimeout(resolve, 1) that wastes CPU cycles. Replace with proper async signaling mechanism.","status":"open","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:17:58.509613-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:17:58.509613-06:00"}
{"id":"parquedb-a2hc","title":"Add runtime validation for JSON parsing","description":"Several places cast parsed JSON without validation (e.g., ParqueDBDO.ts: JSON.parse(current.data) as Record\u003cstring, unknown\u003e). Add runtime validation for untrusted data or use Zod/similar.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:43.646831-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T16:27:32.158586-06:00","closed_at":"2026-02-01T16:27:32.158586-06:00","close_reason":"Closed"}
{"id":"parquedb-a2i5","title":"Add E2E integration test for compaction workflow","description":"No E2E test validates the full compaction flow: R2 Write -\u003e Event Notification -\u003e Queue -\u003e Consumer -\u003e DO -\u003e Workflow -\u003e Compacted File. Create integration test using miniflare or similar that exercises the complete path. Should test: 1) Multiple writers in same window, 2) Late writer detection, 3) Iceberg metadata commit, 4) Delta log entry creation.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:33:10.470267-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:48:42.688665-06:00","closed_at":"2026-02-03T13:48:42.688665-06:00","close_reason":"Closed"}
{"id":"parquedb-a4md","title":"Add circuit breaker pattern for storage failures","description":"If storage fails repeatedly, no mechanism to stop trying temporarily. Add circuit breaker that opens after N consecutive failures and closes after timeout. Prevents resource exhaustion during outages.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:02:24.903654-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:04:08.595618-06:00","closed_at":"2026-02-03T11:04:08.595618-06:00","close_reason":"Closed"}
{"id":"parquedb-a4tk","title":"Fix R2Backend.append() race condition","description":"R2Backend.ts append() reads existing object, concatenates, writes back without locking. Concurrent appends cause lost updates. Use R2 conditional writes (onlyIf: { etagMatches }) and retry.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T07:16:01.965394-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T07:21:35.739209-06:00","closed_at":"2026-02-02T07:21:35.739209-06:00","close_reason":"Closed"}
{"id":"parquedb-a5xr","title":"Refactor workflow tests to import production code","description":"Tests reimplement production code (type guards, state handling) rather than importing. This means tests could pass even if production code differs. Refactor: 1) Import isWindowsReadyResponse from production, 2) Import actual CompactionStateDO or test against it, 3) Extract TestableCompactionStateDO to shared test utility (duplicated in 2 files).","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:33:33.74969-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:51:41.071903-06:00","closed_at":"2026-02-03T13:51:41.071903-06:00","close_reason":"Refactored workflow tests to import production code. Exported isWindowsReadyResponse from compaction-queue-consumer.ts. Extracted TestableCompactionStateDO to shared helper file. Updated validation.test.ts and compaction-state-do.test.ts to use shared utilities."}
{"id":"parquedb-a6qf","title":"Fix unsafe JSON.parse without try-catch","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:27.358059-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:35:55.094205-06:00","closed_at":"2026-02-03T08:35:55.094205-06:00","close_reason":"Closed"}
{"id":"parquedb-a817","title":"Extract hydration logic in ParqueDB.ts","description":"The get() method in ParqueDB.ts (lines 984-1241) contains nearly identical hydration blocks repeated twice for maxInbound and hydrate options. Extract into a private hydrateEntity() method to reduce duplication and bug risk.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:07.953143-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T15:51:16.894957-06:00","closed_at":"2026-02-01T15:51:16.894957-06:00","close_reason":"Closed"}
{"id":"parquedb-abcm","title":"Testing: Add integration tests for storage backends R2/Fsx with real Cloudflare bindings","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:49:51.084874-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:58:10.621951-06:00","closed_at":"2026-02-03T15:58:10.621951-06:00","close_reason":"Closed"}
{"id":"parquedb-ac1","title":"Update GraphDL/IceType integration for v0.3/v0.2","description":"Update integration tests and usage for @graphdl/core@0.3.0, @icetype/core@0.2.0, @icetype/iceberg@0.2.0. Use new unified API.","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T14:30:53.852203-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-30T14:38:25.243872-06:00","closed_at":"2026-01-30T14:38:25.243872-06:00","close_reason":"Closed"}
{"id":"parquedb-achy","title":"Missing error handling: SnapshotManager delete does not handle storage errors","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:41:45.109768-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:50:29.070764-06:00","closed_at":"2026-02-03T12:50:29.070764-06:00","close_reason":"Closed"}
{"id":"parquedb-aeb5","title":"Fix prototype pollution risk in $set operator","description":"The $set operator with dot notation in ParqueDB.ts (lines 1410-1430) does not sanitize field names. Paths like '__proto__.polluted' or 'constructor.prototype.x' could lead to prototype pollution. Add validation to exclude reserved object properties.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T17:00:46.915388-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T17:06:57.383299-06:00","closed_at":"2026-02-01T17:06:57.383299-06:00","close_reason":"Closed"}
{"id":"parquedb-aewx","title":"Refactor: Storage - FsxBackend list() does not use cursor for pagination","description":"FsxBackend.list() at lines 119-184 implements incomplete pagination:\\n\\n```typescript\\nconst allFiles = await this.fsx.glob(pattern, {})\\n\\nconst limit = options?.limit\\nlet files: string[]\\n\\nif (limit) {\\n  files = allFiles.slice(0, limit)\\n  hasMore = allFiles.length \u003e limit\\n  if (hasMore) {\\n    cursor = btoa(JSON.stringify({ offset: limit }))  // Creates cursor\\n  }\\n}\\n```\\n\\nIssues:\\n1. Cursor is generated but never consumed - the function ignores options?.cursor\\n2. Always fetches ALL files via glob, then slices - inefficient for large directories\\n3. Pattern creates cursor encoding but second call ignores it\\n\\nRefactor to:\\n- Parse and use the cursor on subsequent calls\\n- Consider more efficient glob patterns or caching\\n\\nFiles affected:\\n- /Users/nathanclevenger/projects/parquedb/src/storage/FsxBackend.ts","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:33:35.548468-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:23:23.240334-06:00","closed_at":"2026-02-03T11:23:23.240334-06:00","close_reason":"Closed"}
{"id":"parquedb-af49","title":"P2: Add JSDoc to CollectionImpl methods","description":"CollectionImpl class methods in src/ParqueDB/collection.ts:53-102 lack JSDoc documentation. Add documentation for all public methods.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:11:03.427179-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:36:57.314088-06:00","closed_at":"2026-02-03T15:36:57.314088-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-afch","title":"P1: Complete MCP integration tests","description":"MCP integration at src/integrations/mcp/ has significant functionality but only 2 test files. Add comprehensive tests for tool invocation, streaming, and error handling at tests/integration/mcp/.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:10:41.356711-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:36:57.026778-06:00","closed_at":"2026-02-03T15:36:57.026778-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-ag0z","title":"P2: Add debug logging for swallowed errors","description":"src/storage/FsBackend.ts:363-367,549-556 - Multiple catch blocks silently ignore errors. Add logger.debug() for debugging.","status":"open","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:03.855247-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:03.855247-06:00"}
{"id":"parquedb-ahjh","title":"Indexed queries still scan 400K+ rows: row group skip not effective","description":"Hash index lookup for titleType=movie returns 400K docIds across all 20 row groups. Since all row groups contain 'movie' entries, no row groups are skipped. The index narrows the candidate set but still reads all rows from targeted row groups. For low-selectivity filters (40% of data), the index adds overhead (352ms lookup) without reducing I/O. Need: (1) investigate if -based filtering within row groups can use pushdown, (2) consider returning early if selectivity is too low, (3) benchmark high-selectivity queries separately.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T08:27:41.413158-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T08:38:25.485572-06:00","closed_at":"2026-02-02T08:38:25.485572-06:00","close_reason":"Closed"}
{"id":"parquedb-ahw","title":"[RED] Vector index tests","description":"Write failing tests for vector indexing and $near queries","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:59.037193-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:30:37.067608-06:00","closed_at":"2026-02-01T14:30:37.067608-06:00","close_reason":"Created vector index tests in tests/unit/indexes/vector-index.test.ts with 29 comprehensive tests covering basic operations, distance metrics, CRUD operations, persistence, and HNSW parameters"}
{"id":"parquedb-ai29","title":"Refactor: Centralize hardcoded magic numbers","description":"Default configs scattered: DEFAULT_BUFFER_SIZE=1000, DEFAULT_AI_BUFFER_SIZE=500, retention 30 days, etc. Move to central config module with documentation.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:15:58.397639-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:36:02.111506-06:00","closed_at":"2026-02-03T10:36:02.111506-06:00","close_reason":"Closed"}
{"id":"parquedb-ai6u","title":"P1: Add ParqueDBDO direct unit tests","description":"ParqueDBDO (Durable Object) is a critical write path with limited direct unit tests. Create tests at tests/unit/worker/ParqueDBDO.test.ts covering CRUD, transactions, event sourcing, and error scenarios.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:10:39.498832-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:36:56.968625-06:00","closed_at":"2026-02-03T15:36:56.968625-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-ai8o","title":"[SECURITY] Replace Math.random() with crypto.randomUUID()","description":"ID generation uses Math.random() which is not cryptographically secure. Replace with crypto.randomUUID() or crypto.getRandomValues() for all ID generation.\n\nLocations to fix:\n- src/utils/id.ts (if present)\n- Any direct Math.random() calls for IDs\n- Consider using ULID library consistently","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T13:35:44.02468-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:40:50.136048-06:00","closed_at":"2026-02-01T13:40:50.136048-06:00","close_reason":"Closed"}
{"id":"parquedb-aio","title":"Implement ParqueDB class with Proxy","description":"Core ParqueDB class with Proxy-based collection access (db.Posts.find())","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:26.437509-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:03:00.535157-06:00","closed_at":"2026-02-01T14:03:00.535157-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-aio","depends_on_id":"parquedb-atd","type":"blocks","created_at":"2026-01-30T11:51:35.312493-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-aio","depends_on_id":"parquedb-ho9","type":"blocks","created_at":"2026-01-30T11:51:35.398051-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-aio","depends_on_id":"parquedb-2kk","type":"blocks","created_at":"2026-01-30T11:51:35.483361-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-akkr","title":"Studio: Implement settings page","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:33.653964-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:44:41.697265-06:00","closed_at":"2026-02-03T08:44:41.697265-06:00","close_reason":"Closed"}
{"id":"parquedb-akn2","title":"Integration: Add rpc.do DurableRPC wrapper","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:39.783085-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:33:42.347373-06:00","closed_at":"2026-02-03T10:33:42.347373-06:00","close_reason":"Closed"}
{"id":"parquedb-aml7","title":"parseRelation returns incomplete ParsedRelationship objects","description":"File: src/types/schema.ts:342-404\n\nThe parseRelation function returns ParsedRelationship objects with several fields set to empty strings that are meant to be filled in by the caller:\n- fromType: '' (for forward relations)\n- fromField: '' (for forward relations)\n- predicate: '' (for forward relations)\n- toType: '' (for backward relations)\n- reverse: '' (for backward relations)\n\nThis creates a type mismatch: ParsedRelationship requires string fields, but the returned values are semantically incomplete.\n\nSuggested fix: Return a partial type (Omit\u003cParsedRelationship, 'fromType' | 'fromField' | 'predicate'\u003e) or use a builder pattern that enforces all required fields are set before returning a complete ParsedRelationship.","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:12.813745-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:58:26.829936-06:00","closed_at":"2026-02-03T15:58:26.829936-06:00","close_reason":"Closed"}
{"id":"parquedb-an4","title":"Implement event archival","description":"Archive old events by time period","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:44.276134-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:22:33.603114-06:00","closed_at":"2026-02-01T14:22:33.603114-06:00","close_reason":"Closed"}
{"id":"parquedb-aolp","title":"P2: Standardize null check patterns","description":"Inconsistent patterns: 'if (value === null || value === undefined)' vs 'if (\\!value)' with different semantics. Standardize on explicit checks or utility function isNullish() throughout codebase.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:11:02.508285-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:36:57.251025-06:00","closed_at":"2026-02-03T15:36:57.251025-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-aoz","title":"[RED] IceType integration tests","description":"Write failing tests for fromIceType() conversion","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:52:07.959893-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T04:45:58.836788-06:00","closed_at":"2026-02-02T04:45:58.836788-06:00","close_reason":"Tests implemented in earlier waves"}
{"id":"parquedb-apv","title":"Implement field projection","description":"Include/exclude fields from query results","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:23.767362-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:15:30.559615-06:00","closed_at":"2026-02-01T14:15:30.559615-06:00","close_reason":"Closed"}
{"id":"parquedb-ar9b","title":"Add proper exhaustiveness checking for BackendType","description":"Switch statement in compaction-migration.ts:470-490 uses default fallback instead of never assertion. If a new BackendType is added, compiler won't catch it. Replace: case 'native': default: with separate case 'native' and default: { const _exhaustive: never = format; throw new Error(...) }","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:33:30.203117-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:36:18.983402-06:00","closed_at":"2026-02-03T13:36:18.983402-06:00","close_reason":"Closed"}
{"id":"parquedb-atar","title":"Replace as any casts with proper types","description":"80+ 'as any' casts found bypassing TypeScript type checking:\n- ParqueDB.ts:749,914-1156 - entity field access\n- types/integrations.ts:182-197 - external schema objects\n- worker/index.ts:846,854,862,919 - R2 bucket casts\n- parquet/writer.ts:440 - element type casting\n\nCreate utility type for dynamic entity access. Use type guards instead of unsafe casts.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T12:42:15.770568-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:18:47.813668-06:00","closed_at":"2026-02-01T13:18:47.813668-06:00","close_reason":"Closed"}
{"id":"parquedb-atd","title":"[RED] ParqueDB class tests","description":"Write failing tests for ParqueDB Proxy pattern and collection access","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:30.133623-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:02:56.285861-06:00","closed_at":"2026-02-01T14:02:56.285861-06:00","close_reason":"Closed"}
{"id":"parquedb-att","title":"[RED] Delete operation tests","description":"Write failing tests for delete, deleteMany, destroy, restore","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:35.126823-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T04:45:59.370873-06:00","closed_at":"2026-02-02T04:45:59.370873-06:00","close_reason":"Tests implemented in earlier waves"}
{"id":"parquedb-av03","title":"Background embedding generation needs error monitoring","description":"The EmbeddingQueue (src/embeddings/background.ts) handles background embedding generation but lacks observability features needed for production:\n\nMissing features:\n1. No metrics/telemetry for queue depth, processing time, error rates\n2. No dead letter queue for repeatedly failed items\n3. Failed items silently dropped after max retries (cleanupExhaustedItems)\n4. No alerting hooks for queue backlog growth\n5. No visibility into which entities are pending embedding\n\nCurrent behavior:\n- Items retry up to retryAttempts times (default 3)\n- After exhausting retries, items are deleted with no logging\n- No way to manually inspect or retry failed items\n\nNeeded:\n1. Add queue metrics (queue_depth, items_processed, items_failed)\n2. Move exhausted items to dead letter storage instead of deleting\n3. Add webhook/callback for failed items\n4. Add API to list failed items and retry manually\n5. Integrate with logger for observability","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:16:05.405111-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:05:15.277813-06:00","closed_at":"2026-02-03T15:05:15.277813-06:00","close_reason":"Closed"}
{"id":"parquedb-awde","title":"Refactor: Storage - DOSqliteBackend.list includeMetadata does N+1 queries","description":"DOSqliteBackend.list() at lines 368-378 has an N+1 query problem:\\n\\n```typescript\\n// Include metadata if requested\\nif (options?.includeMetadata) {\\n  const stats: FileStat[] = []\\n  for (const file of files) {\\n    const stat = await this.stat(file)  // Individual query per file\\!\\n    if (stat) {\\n      stats.push(stat)\\n    }\\n  }\\n  listResult.stats = stats\\n}\\n```\\n\\nThe main query already selects all needed columns but excludes 'data'. Instead of calling stat() N times, the code should:\\n- Use the results already fetched in the main query\\n- Transform result.results directly to FileStat array\\n\\nThis is also a problem in FsxBackend.list() at lines 172-181.\\n\\nFiles affected:\\n- /Users/nathanclevenger/projects/parquedb/src/storage/DOSqliteBackend.ts\\n- /Users/nathanclevenger/projects/parquedb/src/storage/FsxBackend.ts","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:33:44.599324-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:14:40.95287-06:00","closed_at":"2026-02-03T11:14:40.95287-06:00","close_reason":"Closed"}
{"id":"parquedb-ax2y","title":"Document backup/restore procedures","description":"Sync system exists but backup/restore not documented.\n\nDocument:\n- Data export patterns (full, incremental)\n- Disaster recovery procedures\n- Point-in-time recovery using event log\n- Cross-region backup strategies\n- Restore verification steps","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:32:12.070909-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:06:11.799986-06:00","closed_at":"2026-02-03T10:06:11.799986-06:00","close_reason":"Closed"}
{"id":"parquedb-ay0r","title":"Add tests for worker HTTP handlers","description":"worker/handlers/ (datasets, entity, relationships, ns, debug, health, root) totaling 752 lines have zero test coverage. Add integration tests with mock request/response pairs. Also test routing.ts path matching and responses.ts.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T07:16:03.188839-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T07:24:02.792132-06:00","closed_at":"2026-02-02T07:24:02.792132-06:00","close_reason":"Closed"}
{"id":"parquedb-azer","title":"CRITICAL: Complete merge implementation TODO","description":"merge.ts:308-310 has '// TODO: Apply resolved changes' - merge conflicts are detected but resolution is not applied. This is a P0 blocker for production use.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:54:37.037849-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:07:46.853613-06:00","closed_at":"2026-02-03T09:07:46.853613-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-azlw","title":"DO WAL: Phase 3 - Remove entity table","description":"## TDD Task: Remove entities table, derive state from WAL\n\n### Context\nThe `entities` table duplicates data. Entity state should be derived from events (event sourcing).\n\n### Red (Write failing tests first)\n1. Test entity state reconstructed from events\n2. Test reads work without entities table\n3. Test updates apply to event log only\n4. Test deletes are soft (tombstone event)\n\n### Green (Implement)\n1. Remove INSERT/UPDATE to entities table\n2. Add event replay for entity state\n3. Cache recent entities in memory (LRU)\n4. Update get/find to use cache + events + R2\n\n### Refactor\n- Migration path: read from entities if exists, then events\n- Eventually drop entities table\n\n### Files\n- `src/worker/ParqueDBDO.ts` - remove entity writes\n- `src/worker/EntityCache.ts` - new LRU cache","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:07:35.868013-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T06:54:24.891978-06:00","closed_at":"2026-02-03T06:54:24.891978-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-azlw","depends_on_id":"parquedb-vq7i","type":"blocks","created_at":"2026-02-03T06:07:44.884396-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-b03e","title":"Fix sync client and event-merge tests","description":"tests/unit/sync/client.test.ts has 10 failing tests for retry with exponential backoff. tests/unit/sync/event-merge.test.ts has 7 failing tests for merge cases.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:51:37.936193-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:52:20.98953-06:00","closed_at":"2026-02-03T13:52:20.98953-06:00","close_reason":"Closed"}
{"id":"parquedb-b1tn","title":"Complete sharded index test coverage","description":"Sharded indexes recently added with skeleton tests:\n- sharded-hash.ts - needs comprehensive tests\n- sharded-sst.ts - needs comprehensive tests\n- v3 compact format support needs validation\n\nAdd tests for all shard operations, edge cases, and format compatibility.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T12:42:46.084372-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T12:54:07.473876-06:00","closed_at":"2026-02-01T12:54:07.473876-06:00","close_reason":"Completed by parallel agents"}
{"id":"parquedb-b445","title":"Standardize input validation across backends","description":"R2Backend validates range parameters (134-150) but MemoryBackend silently handles invalid ranges differently. Standardize validation across all storage backends.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:15.224257-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T16:16:38.705048-06:00","closed_at":"2026-02-01T16:16:38.705048-06:00","close_reason":"Closed"}
{"id":"parquedb-b4bu","title":"validatePath throws generic Error - should use InvalidPathError","description":"**High: Missing Typed Error**\n\nIn /src/storage/validation.ts lines 54-59, validatePath throws a generic Error:\n\n```typescript\nexport function validatePath(path: string, operation: string): void {\n  if (path === undefined || path === null) {\n    throw new Error(`${operation}: path is required`)\n  }\n}\n```\n\n**Impact**: Cannot distinguish path validation errors from other errors.\n\n**Fix**: Import and use InvalidPathError from './errors':\n\n```typescript\nimport { InvalidPathError } from './errors'\n\nexport function validatePath(path: string, operation: string): void {\n  if (path === undefined || path === null) {\n    throw new InvalidPathError('', 'path is required')\n  }\n}\n```","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:37:16.371284-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:47:49.160014-06:00","closed_at":"2026-02-03T12:47:49.160014-06:00","close_reason":"Closed"}
{"id":"parquedb-b603","title":"Refactor: ParqueDB.ts - Remove dead stub methods after Proxy delegation","description":"In src/ParqueDB.ts, lines 250-535 contain stub method implementations that throw 'Not implemented' errors. These methods are never called because the Proxy handler intercepts all method calls and delegates to ParqueDBImpl.\n\nThe stubs exist for TypeScript type inference but could be replaced with a cleaner approach:\n1. Use a type interface for ParqueDB that describes the API\n2. Remove the dead stub methods\n3. Document that ParqueDB uses Proxy delegation\n\nFiles affected:\n- src/ParqueDB.ts (lines 246-536)\n\nThis reduces ~290 lines of dead code that may confuse developers who don't understand the Proxy pattern.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:32:33.573255-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T06:55:48.018247-06:00","closed_at":"2026-02-03T06:55:48.018247-06:00","close_reason":"Implemented by agents","labels":["dead-code","refactor"]}
{"id":"parquedb-b9dl","title":"P1: Address global storage map memory leak potential","description":"Module-level storage maps in src/collection.ts:80-97 (globalStorage, globalRelationships, globalEventLog) grow indefinitely. While clearGlobalStorage() exists, it requires explicit calling. Long-running processes could accumulate data. Consider adding LRU eviction or TTL.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:10:35.751002-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:36:57.134029-06:00","closed_at":"2026-02-03T15:36:57.134029-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-be1i","title":"High: MigrationDO HTTP endpoints lack authentication","description":"In src/worker/MigrationDO.ts:122-157, the HTTP fetch handler exposes migration endpoints without any authentication:\n\n- POST /migrate - Start migration (could trigger expensive R2 operations)\n- GET /status - Exposes migration status\n- POST /cancel - Cancel running migration\n- GET /jobs - List migration history\n\nAn attacker could:\n1. Start unauthorized migrations that consume resources\n2. Cancel legitimate migrations in progress\n3. Discover internal database structure via job history\n\nFix: Add authentication check before processing any migration requests. Consider requiring an admin token or limiting access to internal service bindings only.\n\nFile: /Users/nathanclevenger/projects/parquedb/src/worker/MigrationDO.ts:122-157","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:39.63974-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:17:27.635042-06:00","closed_at":"2026-02-03T13:17:27.635042-06:00","close_reason":"Closed"}
{"id":"parquedb-bffx","title":"Duplicate Code Between crud.ts and entity-operations.ts","description":"**Files:** src/ParqueDB/crud.ts and src/ParqueDB/entity-operations.ts\n\n**Issue:** Both files define identical helper functions:\n- deriveTypeFromNamespace\n- isFieldRequired\n- hasDefault\n- validateFieldType\n- applySchemaDefaults\n\n**Impact:** Changes to one copy won't update the other, leading to inconsistent behavior across the codebase.\n\n**Fix:** Extract these shared functions to a new src/ParqueDB/schema-utils.ts module and import from both files.\n\n**Priority:** P0 CRITICAL","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:10:09.888477-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T17:34:34.704111-06:00","closed_at":"2026-02-03T17:34:34.704111-06:00","close_reason":"Closed"}
{"id":"parquedb-bfvt","title":"Refactor: core.ts - Extract sorting logic into reusable utility","description":"In src/ParqueDB/core.ts, sorting logic is duplicated across methods with slight variations:\n\n1. find() lines 256-278 - Handles nulls last, uses compareValues utility\n2. getRelated() lines 569-594 - Different null handling, manual type-specific comparisons\n3. reconstructEntityAtTime() lines 1667-1671 - Sorts by timestamp and ID\n\nThe getRelated() sorting implementation is inconsistent with find() - it handles null/undefined differently and doesn't use the compareValues utility.\n\nSuggested refactor:\n1. Create a generic sortEntities(items, sortSpec, options?) utility in src/query/sort.ts\n2. Options could include { nullsLast: boolean, caseInsensitive: boolean }\n3. Reuse across all methods that need sorting\n\nThis improves consistency and reduces ~50 lines of duplicated sorting logic.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:32:56.890456-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:23:02.721122-06:00","closed_at":"2026-02-03T11:23:02.721122-06:00","close_reason":"Closed","labels":["consistency","duplication","refactor"]}
{"id":"parquedb-bglc","title":"Extract duplicate sort logic to utility","description":"Sort implementation is duplicated in Collection.ts (lines 362-377 and 658-668).\n\nExtract to:\n```typescript\nfunction sortEntities\u003cT\u003e(entities: T[], sortSpec: SortSpec): T[]\n```\n\nDRY compliance improvement.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:32:16.445122-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:08:09.630772-06:00","closed_at":"2026-02-03T10:08:09.630772-06:00","close_reason":"Closed"}
{"id":"parquedb-bh1","title":"[RED] Time-travel query tests","description":"Write failing tests for asOf option in find/get","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:39.570046-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:20:43.169168-06:00","closed_at":"2026-02-01T14:20:43.169168-06:00","close_reason":"Closed"}
{"id":"parquedb-bhqd","title":"Fix ai-database-adapter tests","description":"tests/unit/integrations/ai-database-adapter.test.ts has multiple failing tests for Write Operations, Relationship Operations, Semantic Search, Hybrid Search, and Actions API.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:17:07.537552-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:22:45.362849-06:00","closed_at":"2026-02-03T12:22:45.362849-06:00","close_reason":"Closed"}
{"id":"parquedb-bon","title":"E2E: Node.js client to deployed Worker benchmarks","description":"Deploy ParqueDB Worker and benchmark Node.js client calling remote RpcTarget via capnweb. Measure real network latency from local machine to Cloudflare edge.","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T15:58:33.108309-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:13:03.458067-06:00","closed_at":"2026-02-01T13:13:03.458067-06:00","close_reason":"Closed"}
{"id":"parquedb-bpo5","title":"Fix: Inconsistent null/undefined checks","description":"Multiple files use || instead of ?? for nullish checks. Empty string passes through. Examples: tail/index.ts line 403-409, streaming/ai-requests.ts tokens field.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:15:24.731813-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:43:54.823515-06:00","closed_at":"2026-02-03T10:43:54.823515-06:00","close_reason":"Closed"}
{"id":"parquedb-bqy3","title":"Implement schema validation functions in schema/parser.ts","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:19:37.805421-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:23:08.970384-06:00","closed_at":"2026-02-03T15:23:08.970384-06:00","close_reason":"Closed"}
{"id":"parquedb-br1w","title":"Add circuit breakers for external service calls","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:31.850824-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:33:03.989209-06:00","closed_at":"2026-02-03T10:33:03.989209-06:00","close_reason":"Closed"}
{"id":"parquedb-breb","title":"[TS] Remove remaining any types in parquet/reader.ts","description":"8 'any' types remain in src/parquet/reader.ts. These should be replaced with proper types:\n\n- Use hyparquet types where available\n- Create interfaces for Parquet metadata structures\n- Use unknown with type guards where necessary\n- Consider contributing types upstream to @types/hyparquet","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T13:35:59.546383-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:38:42.256702-06:00","closed_at":"2026-02-01T13:38:42.256702-06:00","close_reason":"Closed"}
{"id":"parquedb-bsyc","title":"Integration: Create Evalite database adapter for AI evaluations","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:42.644224-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:49:55.036458-06:00","closed_at":"2026-02-03T08:49:55.036458-06:00","close_reason":"Closed"}
{"id":"parquedb-bu2f","title":"Stabilize Cloudflare Workers integration","description":"Workers pool tests fail with isolated storage errors. R2Backend and Durable Object integration needs stabilization for production use. This is the core value proposition - must work reliably.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:42.460528-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T16:31:40.844071-06:00","closed_at":"2026-02-01T16:31:40.844071-06:00","close_reason":"Closed"}
{"id":"parquedb-bu85","title":"Fix circular dependencies in types","description":"3 circular dependencies found: types/index.ts \u003e types/options.ts, types/index.ts \u003e types/update.ts, client/collection.ts \u003e client/rpc-promise.ts. Fix by changing imports to use direct file imports instead of re-export index.","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:41.350149-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T16:06:52.721214-06:00","closed_at":"2026-02-01T16:06:52.721214-06:00","close_reason":"Closed"}
{"id":"parquedb-buh","title":"Implement populate (hydration)","description":"Fetch related entities in find/get operations","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:40.335294-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:14:51.646112-06:00","closed_at":"2026-02-01T14:14:51.646112-06:00","close_reason":"Closed"}
{"id":"parquedb-bw2b","title":"P1: Fix URL encoding bypass in path traversal","description":"src/storage/FsBackend.ts:63-67 - Double-encoding attacks could bypass path traversal check. Check for .. before and after decoding.","status":"open","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:00.341332-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:00.341332-06:00"}
{"id":"parquedb-bwx9","title":"Add MCP server authentication","description":"MCP server needs authentication support: bearer tokens, API key validation, scope-based permissions","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:09:50.830307-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:18:25.566952-06:00","closed_at":"2026-02-03T10:18:25.566952-06:00","close_reason":"Authentication has been implemented and tested. Implementation includes:\n1. Token-based authentication via TokenAuthenticator interface\n2. API key authentication (createApiKeyAuthenticator)\n3. JWT/custom token authentication (createCustomAuthenticator)\n4. Scope-based access control (PARQUEDB_SCOPES)\n5. Authentication middleware integrated into server.ts\n6. Comprehensive tests (56 auth tests + 161 integration tests)\n7. Documentation updated in docs/integrations/ai/mcp-server.md"}
{"id":"parquedb-bxl","title":"[REFACTOR] Relationship storage cleanup","description":"Optimize relationship storage","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:25.321855-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:08:29.242218-06:00","closed_at":"2026-02-01T14:08:29.242218-06:00","close_reason":"Closed"}
{"id":"parquedb-byc","title":"[RED] Schema parsing tests","description":"Write failing tests for parseFieldType, parseRelation, schema validation","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:58.922648-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:16:30.807713-06:00","closed_at":"2026-02-01T13:16:30.807713-06:00","close_reason":"Closed"}
{"id":"parquedb-byeo","title":"Fix streaming memory-management tests","description":"tests/unit/streaming/memory-management.test.ts has 20+ failing tests. Tests for dispose(), onError unsubscribe, onWarning unsubscribe, removeAllErrorListeners, removeAllWarningListeners, and memory release verification. StreamingRefreshEngine memory management methods may not be implemented correctly.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:53:37.328419-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:54:24.657815-06:00","closed_at":"2026-02-03T11:54:24.657815-06:00","close_reason":"Closed"}
{"id":"parquedb-byvg","title":"Add Capacity Planning Guide for Compaction","description":"No guidance on memory requirements, R2 costs, when to enable sharding. Create docs/guides/compaction-capacity-planning.md with:\n- Memory requirements based on data size\n- R2 storage and operation cost estimates\n- When to enable partition sharding\n- Scaling considerations\n- Performance tuning recommendations\n- Cost optimization strategies","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T14:34:41.84288-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:39:54.814665-06:00","closed_at":"2026-02-03T14:39:54.814665-06:00","close_reason":"Closed"}
{"id":"parquedb-bz03","title":"P0: Fix window removal race condition in CompactionStateDO","description":"Critical: Windows are removed from DO state BEFORE workflow.create() is confirmed. If workflow creation fails, the window tracking is lost and files won't be compacted. Location: compaction-queue-consumer.ts around line 467. Fix: Move window deletion to AFTER workflow creation succeeds, or mark windows as 'processing' instead of deleting, only delete after workflow confirms completion.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:33:05.039605-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:44:21.283861-06:00","closed_at":"2026-02-03T13:44:21.283861-06:00","close_reason":"Closed"}
{"id":"parquedb-bzhh","title":"Add sync conflict resolution tests","description":"Missing tests for src/sync/: commutative-ops.ts, conflict-detection.ts, conflict-resolution.ts, event-manifest.ts, merge-state.ts, segment-writer.ts, segment-reader.ts. File-level sync conflicts with entity-level semantics need testing.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:38:31.483854-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:41:56.183826-06:00","closed_at":"2026-02-03T08:41:56.183826-06:00","close_reason":"Closed"}
{"id":"parquedb-c0cg","title":"Add 'Why ParqueDB?' section to README","description":"README lacks compelling comparison to alternatives. Add section explaining: why not DuckDB (no Workers), why not SQLite (no columnar), why not MongoDB (no edge). Include the excellent benchmark numbers.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T17:01:10.292212-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T17:18:59.574692-06:00","closed_at":"2026-02-01T17:18:59.574692-06:00","close_reason":"Closed"}
{"id":"parquedb-c1m","title":"[GREEN] Entity history implementation","description":"Implement history() to pass tests","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:43.396415-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:20:12.677312-06:00","closed_at":"2026-02-01T14:20:12.677312-06:00","close_reason":"Closed"}
{"id":"parquedb-c3ya","title":"Narrow Worker RPC types from unknown","description":"ParqueDBDOStub interface in src/types/worker.ts uses unknown throughout. Replace with more specific interfaces or generic parameters for type safety at RPC boundary.","status":"open","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:33.908838-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:33.908838-06:00"}
{"id":"parquedb-c5r6","title":"Missing Data Integrity Tests for Compaction","description":"**Issue:** No tests verifying row count preservation, sort order maintenance post-compaction\n\n**Impact:** Could miss data corruption bugs\n\n**Fix:** Add data integrity verification tests to E2E suite covering:\n- Row count preservation before/after compaction\n- Sort order maintenance\n- Data consistency verification","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T14:34:09.748985-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:40:27.231387-06:00","closed_at":"2026-02-03T14:40:27.231387-06:00","close_reason":"Closed"}
{"id":"parquedb-c7zr","title":"Add Migration Validation for Compaction","description":"No checksum/row-count verification post-migration. Implement:\n- Row count verification before/after compaction\n- Checksum validation of compacted files\n- Data integrity checks\n- Rollback capability on validation failure\n- Validation reporting and logging","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T14:34:46.301958-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:41:49.292591-06:00","closed_at":"2026-02-03T14:41:49.292591-06:00","close_reason":"Closed"}
{"id":"parquedb-cai9","title":"Fix async-error-boundaries test","description":"tests/unit/async-error-boundaries.test.ts has 1 failing test: should handle errors in fire-and-forget patterns with proper logging.","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:47:36.766887-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:48:18.933972-06:00","closed_at":"2026-02-03T12:48:18.933972-06:00","close_reason":"Closed"}
{"id":"parquedb-cay","title":"[RED] Filter evaluation tests - logical operators","description":"Write failing tests for $and, $or, $not, $nor","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:08.059483-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:12:57.659356-06:00","closed_at":"2026-02-01T13:12:57.659356-06:00","close_reason":"Closed"}
{"id":"parquedb-ce0","title":"[REFACTOR] StorageBackend cleanup","description":"Refactor StorageBackend for clarity and efficiency","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:04.272793-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:03:05.115845-06:00","closed_at":"2026-02-01T14:03:05.115845-06:00","close_reason":"Closed"}
{"id":"parquedb-cis","title":"[GREEN] Array operators implementation","description":"Implement array operators to pass tests","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:26.348199-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:08:21.77634-06:00","closed_at":"2026-02-01T14:08:21.77634-06:00","close_reason":"Closed"}
{"id":"parquedb-co9p","title":"Remove dead code: isFilterObject, backup files","description":"query/update.ts has unused isFilterObject function. Source tree has .bak/.backup_working/.orig files that should be deleted and added to .gitignore.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T07:16:02.999667-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T07:20:11.959062-06:00","closed_at":"2026-02-02T07:20:11.959062-06:00","close_reason":"Closed"}
{"id":"parquedb-cp1","title":"Epic: Examples using ParqueDB API","description":"All examples must use ParqueDB API (createMany, $link) not raw parquet files. Data flows to data/{ns}/ and rels/ structure.","status":"closed","priority":0,"issue_type":"epic","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T14:30:11.389079-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-30T14:46:18.256147-06:00","closed_at":"2026-01-30T14:46:18.256147-06:00","close_reason":"Closed"}
{"id":"parquedb-cp4o","title":"Fix: No bounds checking on percentile array access","description":"percentile() assumes array has elements, could return undefined for empty array. Add empty check. File: src/streaming/ai-requests.ts","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:15:35.060538-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:37:33.500576-06:00","closed_at":"2026-02-03T10:37:33.500576-06:00","close_reason":"Closed"}
{"id":"parquedb-cpl9","title":"Epic: Backend Code Review Fixes","description":"Address issues identified in code reviews of:\n1. Backend abstraction layer (types.ts, index.ts)\n2. Storage backends (FsBackend, R2Backend, MemoryBackend)\n3. IcebergBackend\n4. DeltaBackend\n\nCritical issues:\n- Non-standard file formats breaking interoperability\n- Missing optimistic concurrency control","status":"open","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:58:26.328196-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:58:26.328196-06:00"}
{"id":"parquedb-cpl9.1","title":"Add BackendError type hierarchy","description":"Add typed errors to src/backends/types.ts:\n\n```typescript\nexport class BackendError extends Error {\n  constructor(message: string, public code: BackendErrorCode, public cause?: unknown) {\n    super(message)\n    this.name = 'BackendError'\n  }\n}\n\nexport type BackendErrorCode = \n  | 'NOT_FOUND'\n  | 'ALREADY_EXISTS'\n  | 'VERSION_CONFLICT'\n  | 'READ_ONLY'\n  | 'VALIDATION_FAILED'\n  | 'STORAGE_ERROR'\n  | 'NOT_INITIALIZED'\n\nexport class NotFoundError extends BackendError { ... }\nexport class VersionConflictError extends BackendError { ... }\nexport class ReadOnlyError extends BackendError { ... }\n```\n\nUpdate IcebergBackend and DeltaBackend to use these errors.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:58:33.71099-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:02:03.438812-06:00","closed_at":"2026-02-03T08:02:03.438812-06:00","close_reason":"Implemented BackendError type hierarchy","dependencies":[{"issue_id":"parquedb-cpl9.1","depends_on_id":"parquedb-cpl9","type":"parent-child","created_at":"2026-02-03T07:58:33.711941-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-cpl9.10","title":"[CRITICAL] DeltaBackend: Add optimistic concurrency control","description":"CRITICAL: DeltaBackend uses Promise-based locks that don't work across processes.\n\nCurrent code (delta.ts:789-795):\n```typescript\nconst lock = this.operationLocks.get(ns) ?? Promise.resolve()\nconst newLock = lock.then(async () =\u003e { ... })\n```\n\nFix: Use atomic file creation for commits:\n1. Determine next version from current state\n2. Try to create commit file atomically (storage.writeIfNotExists)\n3. On conflict (file exists), retry with incremented version\n4. Max retries with exponential backoff","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:59:10.673961-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:17:39.432764-06:00","closed_at":"2026-02-03T08:17:39.432764-06:00","close_reason":"Removed unused operationLocks, OCC already implemented","dependencies":[{"issue_id":"parquedb-cpl9.10","depends_on_id":"parquedb-cpl9","type":"parent-child","created_at":"2026-02-03T07:59:10.674768-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-cpl9.11","title":"DeltaBackend: Add file statistics to add actions","description":"Add actions missing stats field (delta.ts:829-836).\n\nAdd statistics calculation when writing data files:\n```typescript\nconst stats = {\n  numRecords: rows.length,\n  minValues: { $id: min, ... },\n  maxValues: { $id: max, ... },\n  nullCount: { deletedAt: count, ... },\n}\n\nconst add: AddAction = {\n  add: {\n    path: ...,\n    stats: JSON.stringify(stats),\n  }\n}\n```\n\nThis enables predicate pushdown and file skipping.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:59:14.971744-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:06:27.381605-06:00","closed_at":"2026-02-03T08:06:27.381605-06:00","close_reason":"Added file statistics to DeltaBackend add actions","dependencies":[{"issue_id":"parquedb-cpl9.11","depends_on_id":"parquedb-cpl9","type":"parent-child","created_at":"2026-02-03T07:59:14.972733-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-cpl9.12","title":"DeltaBackend: Implement vacuum and compact","description":"vacuum() and compact() return empty results without doing anything.\n\nImplement:\n1. vacuum(): Delete data files no longer referenced by any commit within retention period\n2. compact(): Merge small files into larger ones, rewrite with remove+add actions","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:59:19.043583-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:17:39.605829-06:00","closed_at":"2026-02-03T08:17:39.605829-06:00","close_reason":"Implemented vacuum and compact with 13 new tests","dependencies":[{"issue_id":"parquedb-cpl9.12","depends_on_id":"parquedb-cpl9","type":"parent-child","created_at":"2026-02-03T07:59:19.044411-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-cpl9.13","title":"DeltaBackend: Add remove actions for updates","description":"Updates write new files but don't remove old ones. Entity deduplication works but:\n1. Old files accumulate indefinitely\n2. Full scans required to find latest versions\n\nImplement copy-on-write:\n1. Identify files containing updated entities\n2. Read affected files, apply updates\n3. Write new file with updated data\n4. Create remove action for old file + add action for new file","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:59:20.290121-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:27:24.227639-06:00","closed_at":"2026-02-03T08:27:24.227639-06:00","close_reason":"Implemented copy-on-write updates with remove+add actions","dependencies":[{"issue_id":"parquedb-cpl9.13","depends_on_id":"parquedb-cpl9","type":"parent-child","created_at":"2026-02-03T07:59:20.291402-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-cpl9.14","title":"Unify error types across storage backends","description":"R2Backend defines its own error classes (R2NotFoundError, R2ETagMismatchError) instead of using shared errors from ./errors.ts. Unify to use shared errors or add type guards.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:59:21.295484-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:56:08.384442-06:00","closed_at":"2026-02-03T10:56:08.384442-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-cpl9.14","depends_on_id":"parquedb-cpl9","type":"parent-child","created_at":"2026-02-03T07:59:21.296425-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-cpl9.2","title":"Complete JSDoc documentation in types.ts","description":"Add JSDoc comments to all undocumented methods in src/backends/types.ts: count, exists, bulkUpdate, bulkDelete, and all Options types.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:58:35.381793-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:01:22.290677-06:00","closed_at":"2026-02-03T11:01:22.290677-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-cpl9.2","depends_on_id":"parquedb-cpl9","type":"parent-child","created_at":"2026-02-03T07:58:35.382611-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-cpl9.3","title":"Standardize readRange end position semantics","description":"FsBackend and R2Backend have inconsistent readRange semantics:\n- FsBackend: end is EXCLUSIVE\n- R2Backend: end is INCLUSIVE (adds +1 to length)\n\nStandardize on exclusive (matching Node.js Buffer.slice convention).\nFix R2Backend.readRange to use exclusive end.\nDocument the convention in StorageBackend interface.","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:58:38.363191-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:07:06.046236-06:00","closed_at":"2026-02-03T08:07:06.046236-06:00","close_reason":"Fixed readRange end position semantics across all backends: R2Backend and RemoteBackend were using inclusive end but should use exclusive (like Array.slice). Added comprehensive tests for all backends.","dependencies":[{"issue_id":"parquedb-cpl9.3","depends_on_id":"parquedb-cpl9","type":"parent-child","created_at":"2026-02-03T07:58:38.363907-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-cpl9.4","title":"Fix R2Backend append race condition","description":"In R2Backend.append(), when creating a new file, there's no conditional check:\n\n```typescript\n} else {\n  newData = data\n  putOptions = {}  // No conditional check!\n}\n```\n\nTwo concurrent appends to a non-existent file will race, with one overwriting the other.\n\nFix: Add `onlyIf: { etagDoesNotMatch: '*' }` for new file creation.","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:58:40.978797-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:02:03.632561-06:00","closed_at":"2026-02-03T08:02:03.632561-06:00","close_reason":"Fixed R2Backend append race condition","dependencies":[{"issue_id":"parquedb-cpl9.4","depends_on_id":"parquedb-cpl9","type":"parent-child","created_at":"2026-02-03T07:58:40.979563-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-cpl9.5","title":"[CRITICAL] IcebergBackend: Implement Avro manifest format","description":"CRITICAL: IcebergBackend writes manifests and manifest lists as JSON instead of Avro.\n\nCurrent code (iceberg.ts:790-792):\n```typescript\nconst manifestContent = new TextEncoder().encode(JSON.stringify(manifestResult.entries, null, 2))\n```\n\nThis breaks interoperability - DuckDB, Spark, Snowflake cannot read these tables.\n\nFix: Use @dotdo/iceberg's Avro utilities or implement Avro encoding for manifest/manifest-list schemas.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:58:48.31124-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:42:14.238571-06:00","closed_at":"2026-02-03T08:42:14.238571-06:00","close_reason":"Integrated @dotdo/iceberg Avro utilities - all 45 Iceberg tests pass","dependencies":[{"issue_id":"parquedb-cpl9.5","depends_on_id":"parquedb-cpl9","type":"parent-child","created_at":"2026-02-03T07:58:48.311995-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-cpl9.6","title":"[CRITICAL] IcebergBackend: Add optimistic concurrency control","description":"CRITICAL: IcebergBackend uses in-memory Promise locks that don't work across processes.\n\nCurrent code only protects within single process:\n```typescript\nprivate async acquireWriteLock(ns: string): Promise\u003c() =\u003e void\u003e {\n  const existingLock = this.writeLocks.get(ns)\n  // ...\n}\n```\n\nIceberg spec requires optimistic concurrency:\n1. Read current metadata version\n2. Prepare commit\n3. Atomically update version-hint (fail if changed)\n4. On conflict, re-read and retry\n\nImplement proper OCC with atomic version-hint updates.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:58:51.054476-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:28:38.618894-06:00","closed_at":"2026-02-03T08:28:38.618894-06:00","close_reason":"Implemented OCC with retry loop, writeConditional for version-hint.text, and ETag mismatch handling. All 14 OCC tests pass.","dependencies":[{"issue_id":"parquedb-cpl9.6","depends_on_id":"parquedb-cpl9","type":"parent-child","created_at":"2026-02-03T07:58:51.055343-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-cpl9.7","title":"IcebergBackend: Implement hard delete","description":"hardDeleteEntities() is a no-op (iceberg.ts:911-916):\n```typescript\nprivate async hardDeleteEntities(_ns: string, _ids: (string | EntityId)[]): Promise\u003cvoid\u003e {\n  // TODO: Create position delete or equality delete file\n}\n```\n\nImplement equality delete files per Iceberg spec:\n1. Write equality delete file with $id column\n2. Create delete manifest\n3. Commit with new snapshot","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:58:55.348812-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:43:11.36779-06:00","closed_at":"2026-02-03T10:43:11.36779-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-cpl9.7","depends_on_id":"parquedb-cpl9","type":"parent-child","created_at":"2026-02-03T07:58:55.349747-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-cpl9.8","title":"IcebergBackend: Add predicate pushdown","description":"readEntitiesFromSnapshot loads ALL data files into memory then filters.\n\nAdd predicate pushdown:\n1. Use manifest statistics to skip irrelevant manifests\n2. Use Parquet row group statistics to skip row groups\n3. Add projection pushdown (only read needed columns)\n4. Add limit pushdown\n\nThis is critical for performance on large datasets.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:58:58.086761-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:38:38.872831-06:00","closed_at":"2026-02-03T10:38:38.872831-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-cpl9.8","depends_on_id":"parquedb-cpl9","type":"parent-child","created_at":"2026-02-03T07:58:58.087671-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-cpl9.9","title":"[CRITICAL] DeltaBackend: Fix checkpoint Parquet format","description":"CRITICAL: Checkpoint files are named .checkpoint.parquet but contain JSON.\n\nCurrent code (delta.ts:935-937):\n```typescript\nconst checkpointPath = \\`\\${logPath}\\${this.formatVersion(version)}.checkpoint.parquet\\`\nconst checkpointContent = checkpointActions.map(a =\u003e JSON.stringify(a)).join('\\n')\n```\n\nDelta Lake spec requires checkpoint files to be actual Parquet with columns:\n- txn, add, remove, metaData, protocol (all STRING, JSON-encoded)\n\nFix: Write proper Parquet file using ParquetWriter.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:59:07.554715-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:17:39.346979-06:00","closed_at":"2026-02-03T08:17:39.346979-06:00","close_reason":"Fixed checkpoint to write proper Parquet format","dependencies":[{"issue_id":"parquedb-cpl9.9","depends_on_id":"parquedb-cpl9","type":"parent-child","created_at":"2026-02-03T07:59:07.555577-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-cq6x","title":"Add Pre-built Grafana Dashboard for Compaction","description":"Prometheus metrics exist but no pre-built Grafana JSON. Create docs/monitoring/grafana-compaction.json with:\n- Compaction job status overview\n- Duration and throughput metrics\n- Error rate panels\n- Storage size trends\n- Queue depth monitoring\n- Alerting thresholds visualization","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T14:34:36.511455-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:39:53.550999-06:00","closed_at":"2026-02-03T14:39:53.550999-06:00","close_reason":"Closed"}
{"id":"parquedb-cqap","title":"[CRITICAL] Fix XSS vulnerability in HTML rendering","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:25.905646-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:31:28.907489-06:00","closed_at":"2026-02-03T08:31:28.907489-06:00","close_reason":"Closed"}
{"id":"parquedb-crs","title":"Write: README.md (project overview)","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T12:40:25.239456-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:34:20.474469-06:00","closed_at":"2026-02-01T14:34:20.474469-06:00","close_reason":"Closed"}
{"id":"parquedb-csat","title":"Fix FTS boolean query tests","description":"tests/unit/indexes/fts-boolean.test.ts has failing tests for AND/OR queries. Tests for boolean search functionality in FTSIndex.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:04:22.070507-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:07:33.129568-06:00","closed_at":"2026-02-03T12:07:33.129568-06:00","close_reason":"Closed"}
{"id":"parquedb-cwtj","title":"Add custom test matchers for domain types","description":"Only 4 custom matchers exist for complex domain. Add:\n- toBeValidParquetFile()\n- toHaveRelationship()\n- toMatchEvent()\n- toBeValidIndex()\n- toHaveRowGroups()\n- toBeCompressed()\n\nUpdate tests/matchers.ts","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T12:42:32.027919-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T12:54:07.503713-06:00","closed_at":"2026-02-01T12:54:07.503713-06:00","close_reason":"Completed by parallel agents"}
{"id":"parquedb-cyq","title":"Implement bloom filters","description":"Bloom filters for entity and edge existence checks","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:45.162541-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:24:19.192129-06:00","closed_at":"2026-02-01T14:24:19.192129-06:00","close_reason":"Closed"}
{"id":"parquedb-czm","title":"[RED] Event archival tests","description":"Write failing tests for event archival","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:45.729157-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:22:30.088581-06:00","closed_at":"2026-02-01T14:22:30.088581-06:00","close_reason":"Closed"}
{"id":"parquedb-czot","title":"Implement event replay for time-travel queries","description":"Support time-travel queries with { at: timestamp } option:\n\n```typescript\ndb.find('users', filter, { at: timestamp })\ndb.get('users:u1', { at: timestamp })\ndb.rels('users:u1', 'authored', { at: timestamp })\n```\n\n- Read current state from data.parquet\n- Read events from segments in timestamp range\n- Replay events backwards to reconstruct state at point in time\n\nFile: src/events/replay.ts","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T06:37:57.72466-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T07:14:07.625806-06:00","closed_at":"2026-02-01T07:14:07.625806-06:00","close_reason":"Implemented EventReplayer with InMemoryEventSource, BatchEventSource, forward/backward replay, batch replay, and state history. All 37 tests passing.","dependencies":[{"issue_id":"parquedb-czot","depends_on_id":"parquedb-o3d3","type":"blocks","created_at":"2026-02-01T06:38:10.11599-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-d20q","title":"Refactor worker fetch handler into route modules","description":"Monolithic fetch handler in worker module should use route registry pattern. Extract into separate route modules for better maintainability.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:15:08.919995-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T17:38:54.806857-06:00","closed_at":"2026-02-03T17:38:54.806857-06:00","close_reason":"Implemented route registry pattern and extracted handlers into separate modules"}
{"id":"parquedb-d2o","title":"Implement relationship traversal","description":"Traverse relationships with related() and referencedBy()","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:47.027377-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:13:17.040016-06:00","closed_at":"2026-02-01T14:13:17.040016-06:00","close_reason":"Closed"}
{"id":"parquedb-d352","title":"Add path aliases to tsconfig","description":"No path aliases configured. All imports use relative paths.\n\nAdd to tsconfig.json:\n- baseUrl: '.'\n- paths: { '@/*': ['./src/*'] }\n\nUpdate imports throughout codebase.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T12:42:35.655996-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:38:56.817237-06:00","closed_at":"2026-02-01T14:38:56.817237-06:00","close_reason":"Closed"}
{"id":"parquedb-d3j","title":"[REFACTOR] MemoryBackend cleanup","description":"Refactor MemoryBackend","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:18.081774-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:11:45.157306-06:00","closed_at":"2026-02-01T13:11:45.157306-06:00","close_reason":"Closed"}
{"id":"parquedb-d3mj","title":"Testing: Improve mock quality - replace 70+ inline vi.fn() mocks with proper test doubles","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:49:48.394222-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:20:08.962716-06:00","closed_at":"2026-02-03T16:20:08.962716-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-d3op","title":"Duplicate IndexType definition creates confusion","description":"Two different IndexType definitions exist in the codebase:\n\n1. src/types/schema.ts:90 - IndexType for schema field definitions:\n   ```typescript\n   export type IndexType = boolean | 'unique' | 'fts' | 'vector' | 'hash'\n   ```\n\n2. src/indexes/types.ts:16 - IndexType for index definitions:\n   ```typescript\n   export type IndexType = 'fts' | 'bloom' | 'vector'\n   ```\n\nThese are semantically different types:\n- The schema.ts version is for field-level index annotations\n- The indexes/types.ts version is for actual index implementations\n\nBut they share the same name 'IndexType' which can cause confusion:\n- 'hash' is in schema but not in indexes (deprecated per comments)\n- 'bloom' is in indexes but not in schema\n- 'unique' and boolean are only in schema\n\nThis violates the principle that types with the same name should have the same meaning.\n\nFix options:\n1. Rename one of them (e.g., SchemaIndexType vs IndexImplementationType)\n2. Unify them if they should represent the same concept\n3. Add clear documentation explaining the distinction","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:38:18.797228-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:59:35.824795-06:00","closed_at":"2026-02-03T15:59:35.824795-06:00","close_reason":"Fixed by renaming IndexType in src/types/schema.ts to SchemaIndexType to distinguish it from the IndexType in src/indexes/types.ts. The schema version is for field-level index annotations (boolean | 'unique' | 'fts' | 'vector' | 'hash') while the indexes version is for actual index implementation types ('fts' | 'bloom' | 'vector' | 'geo'). Updated all usages in schema.ts and parser.ts to use the new SchemaIndexType name."}
{"id":"parquedb-d488","title":"Create shared Delta Lake utilities library","description":"Extract shared components between ParqueDB and Delta Lake:\n- Parquet Variant encoding/decoding\n- Storage backend interface alignment\n- Query/filter operators\n- CDC primitives\n\nCreate @dotdo/deltalake package with shared utilities.","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T12:42:42.088717-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:17:51.176763-06:00","closed_at":"2026-02-01T13:17:51.176763-06:00","close_reason":"Closed"}
{"id":"parquedb-d6ff","title":"Refactor: Query - Add predicate pushdown for Parquet row groups","description":"## Summary\nThe filter system evaluates all filters in-memory after reading rows. For large datasets, we should push predicates down to Parquet row group level using column statistics.\n\n### Current State\n- Filter evaluation happens entirely in `src/query/filter.ts`\n- No integration with Parquet metadata (min/max stats, bloom filters)\n- Every row is read and filtered in memory\n\n### Optimization Opportunities\n\n1. **Row Group Pruning using Column Statistics**:\n   - Skip entire row groups when $gt/$gte/$lt/$lte cannot match\n   - Use min/max values from column metadata\n   - Especially beneficial for time-series or ID-based queries\n\n2. **Bloom Filter Integration**:\n   - For $eq and $in operators, check bloom filter first\n   - Skip row groups that definitely don't contain the value\n\n3. **Filter Analysis**:\n   - Extract pushdown-able predicates from complex filters\n   - Identify fields that have column statistics\n   - Determine which predicates benefit from pushdown\n\n### Files to Update\n- `src/query/filter.ts` - Add filter analysis functions\n- `src/query/executor.ts` - Integrate with Parquet reader\n- New: `src/query/pushdown.ts` - Predicate pushdown logic\n\n### Acceptance Criteria\n- [ ] Create filter analysis to extract pushdown predicates\n- [ ] Implement row group pruning for comparison operators\n- [ ] Integrate with bloom filter index (if available)\n- [ ] Add benchmarks showing improvement\n- [ ] Document pushdown behavior","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:33:01.993826-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:18:40.891627-06:00","closed_at":"2026-02-03T11:18:40.891627-06:00","close_reason":"Closed","labels":["refactor"]}
{"id":"parquedb-d6zj","title":"Add concurrent operation tests","description":"Testing: Missing concurrent operation tests across ai-database adapter, batch-loader, and embedding queue. Add thread-safety tests","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:09:48.945675-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:10:04.406697-06:00","closed_at":"2026-02-03T10:10:04.406697-06:00","close_reason":"Closed"}
{"id":"parquedb-dagp","title":"TypeScript: Add branded type for event target strings","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:46:01.56032-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T17:03:30.670421-06:00","closed_at":"2026-02-03T17:03:30.670421-06:00","close_reason":"Closed"}
{"id":"parquedb-de53","title":"Priority-based compaction scheduling for hot namespaces","description":"Under load, prioritize compaction for important namespaces. Implement: 1) Namespace priority configuration (P0-P3), 2) Higher priority = shorter max wait time, 3) Under backpressure, skip P3 namespaces, 4) Separate workflow queues per priority level, 5) Dashboard showing priority queue depths. Useful for multi-tenant scenarios where some namespaces are more critical.","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:57:38.997797-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:19:50.11386-06:00","closed_at":"2026-02-03T14:19:50.11386-06:00","close_reason":"Closed"}
{"id":"parquedb-dg34","title":"Refactor: core.ts upsertMany() - Reduce code duplication with upsert()","description":"In src/ParqueDB/core.ts, upsertMany() (lines 2524-2667) duplicates significant logic from upsert() (lines 2482-2518):\n\nBoth methods:\n1. Find existing entity by filter\n2. If exists: update with filter fields stripped\n3. If not exists: create from filter fields + $set + $setOnInsert\n\nupsertMany has additional inline logic for handling update operators during creation (lines 2598-2630):\n```typescript\n// Handle $inc - start from 0\nif (item.update.$inc) { ... }\n// Handle $push - create array with single element\nif (item.update.$push) { ... }\n// Handle $addToSet - create array with single element\nif (item.update.$addToSet) { ... }\n// Handle $currentDate\nif (item.update.$currentDate) { ... }\n```\n\nThis logic should be shared:\n1. Extract `buildCreateDataFromUpdate(filter, update)` utility\n2. Have both upsert() and upsertMany() use it\n3. This ensures consistent behavior between single and batch operations\n\nThe $link handling after creation (lines 2644-2649) also duplicates pattern from update().","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:34:31.623536-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:53:20.772968-06:00","closed_at":"2026-02-03T07:53:20.772968-06:00","close_reason":"Closed","labels":["duplication","refactor"]}
{"id":"parquedb-di8","title":"Mutation Engine","description":"Create, update, delete operations with MongoDB-style operators","status":"closed","priority":1,"issue_type":"epic","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:50:29.529479-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:34:08.826072-06:00","closed_at":"2026-02-01T14:34:08.826072-06:00","close_reason":"Closed"}
{"id":"parquedb-diof","title":"Consolidate 3 duplicated update engines into one","description":"Three separate update operator implementations exist: query/update.ts (639 lines), mutation/operators.ts (674 lines), Collection.ts (inline), and ParqueDB/core.ts (inline). Only mutation/operators.ts has prototype pollution protection. Consolidate to mutation/operators.ts and have all others delegate to it.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T07:16:01.064989-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T03:40:58.431625-06:00","closed_at":"2026-02-03T03:40:58.431625-06:00","close_reason":"Closed"}
{"id":"parquedb-djy","title":"[RED] Entity history tests","description":"Write failing tests for history() method","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:42.697431-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:20:12.589353-06:00","closed_at":"2026-02-01T14:20:12.589353-06:00","close_reason":"Closed"}
{"id":"parquedb-dkk7","title":"Add exhaustiveness checking to BackendType switch statements","description":"Switch statements handling BackendType (compaction-migration.ts:416-437) use default case that prevents exhaustiveness checking. Add never type assertion to catch future BackendType additions at compile time.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:56:33.362822-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:05:53.703843-06:00","closed_at":"2026-02-03T13:05:53.703843-06:00","close_reason":"Closed"}
{"id":"parquedb-dl2","title":"Implement inbound reference pagination","description":"$count and $next for large inbound sets","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:54.357926-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:25:00.541862-06:00","closed_at":"2026-02-01T14:25:00.541862-06:00","close_reason":"Closed"}
{"id":"parquedb-dlzc","title":"Clarify StorageBackend vs EntityBackend naming","description":"Two similar-sounding abstractions create confusion:\n- StorageBackend: File-level I/O (read, write, mkdir, delete)\n- EntityBackend: Table-format operations (create, find, update, delete)\n\nOptions:\n1. Rename StorageBackend to FileStorageBackend\n2. Add documentation explaining the distinction\n3. Create unified DataBackend with composition\n\nThis is architectural clarification, not breaking change.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:31:50.031372-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:06:18.369987-06:00","closed_at":"2026-02-03T10:06:18.369987-06:00","close_reason":"Closed"}
{"id":"parquedb-dn3e","title":"Enable noUncheckedIndexedAccess in tsconfig","description":"Currently disabled: noUncheckedIndexedAccess: false\n\nEnable this to catch unsafe obj[key] access patterns. Will require fixing code that assumes indexed access always returns defined values.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T12:42:39.431158-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:07:24.823133-06:00","closed_at":"2026-02-01T13:07:24.823133-06:00","close_reason":"Completed by parallel agents"}
{"id":"parquedb-dno1","title":"Refactor ParqueDBDO (2200 lines monolith)","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:31.170332-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:42:56.709397-06:00","closed_at":"2026-02-03T10:42:56.709397-06:00","close_reason":"Closed"}
{"id":"parquedb-dp05","title":"Decompose complex functions in ParqueDB/core.ts","description":"Several methods exceed reasonable complexity: find() ~170 lines, update() ~160 lines. Decompose into smaller focused functions: extract filter evaluation logic, separate relationship handling, create dedicated pagination helper.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:38:14.194417-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:48:22.278308-06:00","closed_at":"2026-02-03T08:48:22.278308-06:00","close_reason":"Closed"}
{"id":"parquedb-du4j","title":"P1: Add search module tests","description":"The src/search/ module has partial test coverage. Add comprehensive tests at tests/unit/search/ covering query parsing, client API, vector search, hybrid search, and error handling.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:10:42.724974-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:24:29.108895-06:00","closed_at":"2026-02-03T15:24:29.108895-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-dxbt","title":"IcebergBackend.close() does not await writeLocks completion","description":"**High: Resource Cleanup Issue**\n\nIn /src/backends/iceberg.ts lines 165-168, the close() method:\n\n```typescript\nasync close(): Promise\u003cvoid\u003e {\n  this.tableCache.clear()\n  this.initialized = false\n}\n```\n\nThis does NOT wait for any in-progress write operations to complete before clearing state. The writeLocks Map (line 127) may have pending operations.\n\n**Impact**:\n- If close() is called during a write operation, the operation may fail or produce inconsistent state\n- The write lock release callbacks will try to operate on cleared state\n\n**Fix**: Wait for all pending write locks to complete before clearing:\n\n```typescript\nasync close(): Promise\u003cvoid\u003e {\n  // Wait for all pending write operations to complete\n  const pendingLocks = Array.from(this.writeLocks.values())\n  await Promise.all(pendingLocks)\n  this.writeLocks.clear()\n  this.tableCache.clear()\n  this.initialized = false\n}\n```","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:36:17.932942-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:42:28.06427-06:00","closed_at":"2026-02-03T11:42:28.06427-06:00","close_reason":"Closed"}
{"id":"parquedb-dyui","title":"Epic: Tail Worker Analytics","description":"Create a tail worker that materializes worker logs, errors, and requests into ParqueDB for observability analytics. Ingest Cloudflare tail events into ParqueDB MVs.","status":"open","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:06:59.016536-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:06:59.016536-06:00"}
{"id":"parquedb-dyui.1","title":"defineStreamView() API","description":"Create API for external stream MVs with $stream, $schema, $transform, $filter","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:07:03.088532-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:10:52.215678-06:00","closed_at":"2026-02-03T09:10:52.215678-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-dyui.1","depends_on_id":"parquedb-dyui","type":"parent-child","created_at":"2026-02-03T09:07:03.089127-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-dyui.10","title":"Tail Worker: createTailHandler creates new batch state per call","description":"In tail.ts, the createTailHandler function creates a local batchState variable that is reset each time the handler is recreated. Since Workers are stateless and the tail handler is created fresh on each invocation (line 743-746), the batching logic doesn't actually work as intended.\n\nLocation: /Users/nathanclevenger/projects/parquedb/src/worker/tail.ts lines 637-644\n\nThe comment on line 643 acknowledges this: 'Note: In a real implementation, batch state would be managed externally (e.g., in Durable Object) since Workers are stateless'\n\nFix: Either remove the batching logic from the stateless worker or document that batching only works when handler instance is persisted.\n\nImpact: Batching ineffective in production - MEDIUM","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:16:00.339448-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:38:37.088863-06:00","closed_at":"2026-02-03T12:38:37.088863-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-dyui.10","depends_on_id":"parquedb-dyui","type":"parent-child","created_at":"2026-02-03T12:16:00.34019-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-dyui.11","title":"StreamingTail: Global connection state causes memory leak","description":"In tail-streaming.ts, connectionStates is a module-level Map that persists across invocations. This can grow unbounded since connections are added but may not be properly cleaned up when WebSocket errors occur.\n\nLocation: /Users/nathanclevenger/projects/parquedb/src/worker/tail-streaming.ts line 110\n\nFix: Add cleanup logic for stale connections or switch to per-request connection handling.\n\nImpact: Memory growth over time - MEDIUM","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:16:07.60775-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:54:35.709873-06:00","closed_at":"2026-02-03T14:54:35.709873-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-dyui.11","depends_on_id":"parquedb-dyui","type":"parent-child","created_at":"2026-02-03T12:16:07.608953-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-dyui.12","title":"WorkerLogsMV: flushTimer uses setInterval which can drift","description":"WorkerLogsMV uses setInterval for periodic flushing which can drift or stack up if flush operations take longer than the interval.\n\nLocation: /Users/nathanclevenger/projects/parquedb/src/streaming/worker-logs.ts lines 634-644\n\nFix: Use setTimeout with rescheduling after flush completion to prevent overlap.\n\nImpact: Potential flush overlap under load - LOW","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:16:15.850825-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:53:31.729677-06:00","closed_at":"2026-02-03T14:53:31.729677-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-dyui.12","depends_on_id":"parquedb-dyui","type":"parent-child","created_at":"2026-02-03T12:16:15.851594-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-dyui.13","title":"WorkerErrorsMV: In-memory error storage has no Parquet persistence","description":"WorkerErrorsMV stores all errors in memory (this.errors array) with a maximum size limit, but never persists to Parquet storage. Unlike WorkerLogsMV which writes to Parquet, WorkerErrorsMV only stores in-memory.\n\nLocation: /Users/nathanclevenger/projects/parquedb/src/streaming/worker-errors.ts lines 73-81\n\nFix: Add Parquet persistence similar to WorkerLogsMV, or document that this MV is in-memory only and requires a separate storage strategy.\n\nImpact: Errors lost on worker restart - HIGH","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:16:23.591436-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:43:31.037226-06:00","closed_at":"2026-02-03T12:43:31.037226-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-dyui.13","depends_on_id":"parquedb-dyui","type":"parent-child","created_at":"2026-02-03T12:16:23.592468-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-dyui.14","title":"Compaction Consumer: Missing dead letter queue for failed messages","description":"The CompactionConsumer retries failed messages but has no dead letter queue (DLQ) handling. Messages that consistently fail will be retried indefinitely.\n\nLocation: /Users/nathanclevenger/projects/parquedb/src/worker/compaction-consumer.ts lines 299-329\n\nFix: Add retry count tracking and move to DLQ after max retries (configure in wrangler.toml).\n\nImpact: Poison messages can block processing - MEDIUM","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:16:30.526418-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:57:15.455224-06:00","closed_at":"2026-02-03T14:57:15.455224-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-dyui.14","depends_on_id":"parquedb-dyui","type":"parent-child","created_at":"2026-02-03T12:16:30.53083-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-dyui.15","title":"Add integration test for full tail event pipeline","description":"Current tests are mostly unit tests with mocks. Need an E2E integration test that verifies:\n1. Tail events flow through TailDO\n2. Raw events are written to R2\n3. Compaction Consumer processes them to Parquet\n4. WorkerLogsMV can query the results\n\nThis will verify the complete event-driven architecture works end-to-end.\n\nImpact: Integration bugs may go undetected - MEDIUM","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:16:37.58548-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:55:17.861208-06:00","closed_at":"2026-02-03T14:55:17.861208-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-dyui.15","depends_on_id":"parquedb-dyui","type":"parent-child","created_at":"2026-02-03T12:16:37.586301-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-dyui.16","title":"TailDO: Missing backpressure handling for slow R2 writes","description":"TailDO sends acknowledgments to clients immediately after buffering events (line 371), but if R2 writes are slow or fail, there's no backpressure mechanism to slow down incoming events.\n\nLocation: /Users/nathanclevenger/projects/parquedb/src/worker/TailDO.ts lines 363-378\n\nFix: Consider delaying ack until flush completes, or implement a write-ahead log pattern.\n\nImpact: Potential data loss under high load - MEDIUM","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:16:44.70519-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:18:32.74141-06:00","closed_at":"2026-02-03T13:18:32.74141-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-dyui.16","depends_on_id":"parquedb-dyui","type":"parent-child","created_at":"2026-02-03T12:16:44.706191-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-dyui.2","title":"db.ingestStream() method","description":"Implement stream ingestion that applies transforms and writes to MV storage","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:07:03.849748-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:15:13.090838-06:00","closed_at":"2026-02-03T09:15:13.090838-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-dyui.2","depends_on_id":"parquedb-dyui","type":"parent-child","created_at":"2026-02-03T09:07:03.85035-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-dyui.3","title":"Tail Worker scaffold","description":"Create src/tail/index.ts with tail handler and R2 integration","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:07:04.639372-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:13:08.383697-06:00","closed_at":"2026-02-03T09:13:08.383697-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-dyui.3","depends_on_id":"parquedb-dyui","type":"parent-child","created_at":"2026-02-03T09:07:04.640272-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-dyui.4","title":"WorkerLogs MV","description":"Materialize all console.log messages","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:07:05.548931-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:16:58.122679-06:00","closed_at":"2026-02-03T09:16:58.122679-06:00","close_reason":"Implemented WorkerLogs Materialized View with comprehensive test coverage (42 tests)","dependencies":[{"issue_id":"parquedb-dyui.4","depends_on_id":"parquedb-dyui","type":"parent-child","created_at":"2026-02-03T09:07:05.549727-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-dyui.5","title":"WorkerErrors MV","description":"Materialize exceptions and error outcomes","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:07:06.707033-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:16:05.800999-06:00","closed_at":"2026-02-03T09:16:05.800999-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-dyui.5","depends_on_id":"parquedb-dyui","type":"parent-child","created_at":"2026-02-03T09:07:06.70772-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-dyui.6","title":"WorkerRequests MV","description":"Materialize request analytics (url, status, colo, timing)","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:07:07.350543-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:16:54.514666-06:00","closed_at":"2026-02-03T09:16:54.514666-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-dyui.6","depends_on_id":"parquedb-dyui","type":"parent-child","created_at":"2026-02-03T09:07:07.351463-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-dyui.7","title":"Tail Worker tests","description":"Integration tests with mock tail events","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:07:08.418989-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:11:11.154288-06:00","closed_at":"2026-02-03T09:11:11.154288-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-dyui.7","depends_on_id":"parquedb-dyui","type":"parent-child","created_at":"2026-02-03T09:07:08.424195-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-dyui.8","title":"TailDO: In-memory buffer not persisted across hibernation cycles","description":"The TailDO class uses an in-memory rawEventsBuffer array that is NOT persisted via ctx.storage. When the DO hibernates, any events in this buffer will be LOST. Critical for production analytics.\n\nLocation: /Users/nathanclevenger/projects/parquedb/src/worker/TailDO.ts lines 157-159\n\nFix: Use ctx.storage.put/get to persist buffer state, or flush before hibernation via webSocketClose.\n\nImpact: Data loss under hibernation - HIGH","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:15:36.340202-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:39:34.460703-06:00","closed_at":"2026-02-03T12:39:34.460703-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-dyui.8","depends_on_id":"parquedb-dyui","type":"parent-child","created_at":"2026-02-03T12:15:36.341-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-dyui.9","title":"TailDO: batchSeq counter resets after hibernation","description":"The batchSeq counter is stored in memory and resets to 0 when the DO restarts after hibernation. This can cause batch ordering issues and potential file overwrites if the same timestamp is used.\n\nLocation: /Users/nathanclevenger/projects/parquedb/src/worker/TailDO.ts line 161\n\nFix: Persist batchSeq using ctx.storage.put('batchSeq', value) and restore on init.\n\nImpact: Potential file conflicts and incorrect batch ordering - MEDIUM","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:15:51.255981-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:39:13.634123-06:00","closed_at":"2026-02-03T12:39:13.634123-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-dyui.9","depends_on_id":"parquedb-dyui","type":"parent-child","created_at":"2026-02-03T12:15:51.256856-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-dyyo","title":"Missing type validation for string operators ($startsWith, $endsWith, $contains)","description":"## Location\nsrc/query/filter.ts, lines 451-462\n\n## Problem\nThe string operators ($startsWith, $endsWith, $contains) cast opValue to string without validating the type first. If opValue is not a string (e.g., null, number, object), this could cause runtime errors or unexpected behavior.\n\nCurrent code:\n```typescript\ncase '$startsWith':\n  if (typeof value !== 'string') return false\n  if (!value.startsWith(opValue as string)) return false  // No validation of opValue!\n  break\n\ncase '$endsWith':\n  if (typeof value !== 'string') return false\n  if (!value.endsWith(opValue as string)) return false  // No validation of opValue!\n  break\n\ncase '$contains':\n  if (typeof value !== 'string') return false\n  if (!value.includes(opValue as string)) return false  // No validation of opValue!\n  break\n```\n\n## Fix\nAdd type validation for opValue before using it:\n```typescript\ncase '$startsWith':\n  if (typeof value !== 'string') return false\n  if (typeof opValue !== 'string') return false  // Add this check\n  if (!value.startsWith(opValue)) return false\n  break\n```\n\n## Impact\n- Could cause TypeError if opValue.toString is called implicitly on null/undefined\n- Could match incorrectly if opValue is coerced to string (e.g., number 123 becomes \"123\")","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:50.52877-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:38:19.385293-06:00","closed_at":"2026-02-03T11:38:19.385293-06:00","close_reason":"Closed"}
{"id":"parquedb-e46t","title":"Refactor: core.ts get() - Extract corruption detection into dedicated function","description":"In src/ParqueDB/core.ts, the get() method (lines 325-478) contains inline corruption detection logic (lines 350-378) that reads event log data and checks for invalid bytes.\n\nThis corruption detection logic:\n```typescript\nif (eventLogData \u0026\u0026 eventLogData.length \u003e 0) {\n  // Parquet files have a magic number 'PAR1' at both start and end\n  if (eventLogData.length \u003e= 4) {\n    const lastBytes = eventLogData.slice(-12)\n    let invalidByteCount = 0\n    for (let i = 0; i \u003c lastBytes.length; i++) {\n      if (lastBytes[i] === 0xFF) {\n        invalidByteCount++\n      }\n    }\n    if (invalidByteCount \u003e= 2) {\n      throw new Error('Event log corruption detected: invalid checksum in parquet file')\n    }\n  }\n}\n```\n\nShould be extracted to:\n```typescript\n// src/utils/parquet.ts\nexport function validateParquetIntegrity(data: Uint8Array): void\nexport function isParquetCorrupted(data: Uint8Array): boolean\n```\n\nBenefits:\n1. Reusable across other read operations\n2. Testable in isolation\n3. Keeps get() focused on entity retrieval\n4. The heuristic (0xFF bytes) could be documented/improved","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:33:32.272621-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:22:04.963491-06:00","closed_at":"2026-02-03T11:22:04.963491-06:00","close_reason":"Closed","labels":["complexity","refactor"]}
{"id":"parquedb-e4zg","title":"Extract common patterns from storage backends","description":"Similar logic for path normalization, error handling, and conditional writes is repeated across R2Backend.ts, DOSqliteBackend.ts, MemoryBackend.ts. Extract to shared utilities or base class to reduce duplication.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:38:42.190127-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:42:44.302477-06:00","closed_at":"2026-02-03T08:42:44.302477-06:00","close_reason":"Closed"}
{"id":"parquedb-e7mf","title":"Replace Function types in iceberg-native.ts","description":"iceberg-native.ts:172-186 uses generic Function type which provides no type safety. Replace with properly typed function signatures for createTableWithSnapshot, readTableMetadata, etc.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T05:32:46.448264-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T06:31:37.463951-06:00","closed_at":"2026-02-02T06:31:37.463951-06:00","close_reason":"Closed"}
{"id":"parquedb-ebu","title":"Benchmark: Workers to DO and R2","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T15:47:14.301107-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-30T15:54:30.031337-06:00","closed_at":"2026-01-30T15:54:30.031337-06:00","close_reason":"Benchmark file created at tests/benchmarks/workers.workers.bench.ts with 39 benchmarks covering R2 read/write, DO RPC cold/warm, DO SQLite queries, sequential vs batched operations. Results show DO cold start ~3ms, warm ops ~1ms, R2 ops \u003c1ms."}
{"id":"parquedb-ec5a","title":"DO WAL: Phase 4 - Relationship batching","description":"## TDD Task: Batch relationship writes\n\n### Context\nCurrently each link() = 1 SQLite row. Relationships should be events in the WAL.\n\n### Red (Write failing tests first)\n1. Test link creates relationship event, not SQLite row\n2. Test bulk links batched together\n3. Test relationship queries work from events\n4. Test flush writes rels to R2 Parquet\n\n### Green (Implement)\n1. Remove relationships table writes\n2. Add REL_CREATE/REL_DELETE event types\n3. Derive relationship state from events\n4. Update getRelated to use events + R2\n\n### Files\n- `src/worker/ParqueDBDO.ts` - relationship events\n- `src/events/types.ts` - new event types","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:07:39.052095-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:29:20.32306-06:00","closed_at":"2026-02-03T07:29:20.32306-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-ec5a","depends_on_id":"parquedb-azlw","type":"blocks","created_at":"2026-02-03T06:07:44.991743-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-edd","title":"Implement entity history","description":"Get change history for an entity","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:41.39247-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:20:12.481055-06:00","closed_at":"2026-02-01T14:20:12.481055-06:00","close_reason":"Closed"}
{"id":"parquedb-egay","title":"Iceberg OCC Missing Exponential Backoff","description":"**File:** src/backends/iceberg-commit.ts\n\n**Issue:** IcebergCommitter retries on conflict immediately without backoff/jitter\n\n**Impact:** Thundering herd under contention, unlike DeltaCommitter which has proper backoff\n\n**Fix:** Add exponential backoff with jitter like DeltaCommitter\n\n**Severity:** CRITICAL","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T14:33:59.62478-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:30:07.717366-06:00","closed_at":"2026-02-03T15:30:07.717366-06:00","close_reason":"Fixed in commit 6947aa0"}
{"id":"parquedb-eh0","title":"Implement event logging","description":"Log CREATE, UPDATE, DELETE events to events.parquet","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:34.048308-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:21:44.644082-06:00","closed_at":"2026-02-01T14:21:44.644082-06:00","close_reason":"Closed"}
{"id":"parquedb-ejjw","title":"Replace __date__ sentinel in deepClone with structuredClone","description":"utils/comparison.ts deepClone uses __date__ sentinel key for Date serialization. User data with a field named __date__ gets corrupted into a Date object. Use structuredClone() which is available in Node 17+ and Workers.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T07:16:02.199064-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T07:19:02.773549-06:00","closed_at":"2026-02-02T07:19:02.773549-06:00","close_reason":"Closed"}
{"id":"parquedb-elmc","title":"Product: Getting started tutorial and quickstart guide","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:45:46.07033-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:48:21.170557-06:00","closed_at":"2026-02-03T12:48:21.170557-06:00","close_reason":"Closed"}
{"id":"parquedb-em5","title":"[RED] O*NET example using ParqueDB","description":"Rewrite O*NET loader to use db.collection().createMany() and $link for relationships. Test that data appears in data/ and rels/ structure.","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T14:30:12.218151-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-30T14:33:27.45591-06:00","closed_at":"2026-01-30T14:33:27.45591-06:00","close_reason":"Closed"}
{"id":"parquedb-eniy","title":"High: SyncEngine operations don't acquire locks","description":"Critical operations in engine.ts (push/pull/sync at lines 84-395) don't acquire locks before modifying data. Concurrent push/pull operations from different processes could corrupt the manifest or create race conditions when updating remote storage. Should use the LockManager to acquire 'sync' lock before these operations.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:05.952888-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:00:06.454208-06:00","closed_at":"2026-02-03T13:00:06.454208-06:00","close_reason":"Closed"}
{"id":"parquedb-ep6z","title":"Fix N+1 query pattern in relationship traversal","description":"getRelated() in core.ts:623-670 iterates over all entities when handling reverse relationships using forEach. For large datasets, this full scan causes performance degradation. Implement relationship indexing or use relationships table more effectively.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:38:15.706455-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:47:43.091701-06:00","closed_at":"2026-02-03T08:47:43.091701-06:00","close_reason":"Closed"}
{"id":"parquedb-eplt","title":"Document search worker architecture","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:53:12.936211-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:55:54.588942-06:00","closed_at":"2026-02-03T13:55:54.588942-06:00","close_reason":"Added SEARCH_WORKER.md with full architecture docs"}
{"id":"parquedb-ewif","title":"Refactor: Worker - Incomplete flushToParquet implementation in ParqueDBDO","description":"The flushToParquet() method in ParqueDBDO has a TODO comment and doesn't actually write Parquet files to R2:\n\n```typescript\n// TODO: Actually write Parquet file to R2\n// In a real implementation, we would:\n// 1. Convert events to Parquet format\n// 2. Write to R2: this.env.BUCKET.put(parquetPath, parquetBuffer)\n// 3. Verify the write succeeded\n```\n\nFiles:\n- src/worker/ParqueDBDO.ts (lines 1292-1297)\n\nThis is a critical path for event log archival. Currently events are marked as flushed but not actually persisted to Parquet.\n\nImpact: Data durability - events may be lost if DO restarts","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:33:25.991035-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:24:49.345088-06:00","closed_at":"2026-02-03T11:24:49.345088-06:00","close_reason":"Closed"}
{"id":"parquedb-eyk","title":"Implement query execution","description":"Execute find queries against Parquet files","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:15.921574-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:02:48.162465-06:00","closed_at":"2026-02-01T14:02:48.162465-06:00","close_reason":"Closed"}
{"id":"parquedb-ezw","title":"[RED] MemoryBackend tests","description":"Write failing tests for MemoryBackend","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:16.165975-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:11:44.88681-06:00","closed_at":"2026-02-01T13:11:44.88681-06:00","close_reason":"Closed"}
{"id":"parquedb-f0pt","title":"Add CI/CD pipeline for search worker deployment","description":"No automated deployment pipeline. Manual wrangler deploy required. Add GitHub Actions workflow for:\n- Build and size check\n- Deploy to staging\n- E2E test\n- Promote to production","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:27:50.051907-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:20:31.919461-06:00","closed_at":"2026-02-03T13:20:31.919461-06:00","close_reason":"Closed"}
{"id":"parquedb-f18d","title":"Product: Public website and landing page","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:45:44.677385-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:48:33.142542-06:00","closed_at":"2026-02-03T12:48:33.142542-06:00","close_reason":"Closed"}
{"id":"parquedb-f43g","title":"Reduce 57 double type assertions (as unknown as T)","description":"Codebase has 57 occurrences of double assertions. Create proper type bridges for Cloudflare Workers types, use proper generic constraints instead of as unknown as T patterns. Key areas: Proxy returns, config parsing, storage adapters.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:38:17.981283-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:51:59.983758-06:00","closed_at":"2026-02-03T08:51:59.983758-06:00","close_reason":"Closed"}
{"id":"parquedb-f6e0","title":"Refactor: validation.ts - Add validation for entity ID format","description":"In src/ParqueDB/validation.ts, there's validation for namespaces but no validation for entity IDs. Entity IDs are accepted as raw strings throughout the codebase without validation.\n\nCurrent state:\n- validateNamespace() validates namespace strings\n- normalizeNamespace() lowercases namespace\n- No validateEntityId() exists\n\nIssues this causes:\n1. IDs with '/' are sometimes treated as full IDs, sometimes as partial\n2. No validation that ID portion doesn't contain invalid characters\n3. No validation of EntityId format (`namespace/id`)\n\nSuggested additions to validation.ts:\n```typescript\nexport function validateEntityId(id: string): void {\n  if (\\!id || typeof id \\!== 'string') {\n    throw new Error('Entity ID is required and must be a non-empty string')\n  }\n  // Validate format\n}\n\nexport function parseEntityId(id: string): { namespace: string; id: string } {\n  // Already exists in types.ts as parseEntityTarget, consider consolidating\n}\n\nexport function normalizeEntityId(namespace: string, id: string): EntityId {\n  return (id.includes('/') ? id : `${namespace}/${id}`) as EntityId\n}\n```\n\nThis improves error messages and catches invalid IDs early.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:34:01.435211-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:11:42.359421-06:00","closed_at":"2026-02-03T11:11:42.359421-06:00","close_reason":"Closed","labels":["consistency","refactor","validation"]}
{"id":"parquedb-f766","title":"Add: Chunked reading for full refresh","description":"Full refresh loads all source data into memory causing pressure for large datasets. Implement chunked reading: for await (const chunk of reader.readChunked(path, size))","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:15:43.326817-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:49:52.312089-06:00","closed_at":"2026-02-03T10:49:52.312089-06:00","close_reason":"Implemented chunked reading for full MV refresh with partial aggregation state support. All 40 refresh tests pass."}
{"id":"parquedb-f7cl","title":"Add: MV + Parquet storage integration tests","description":"No tests verify MV data correctly written/read from actual Parquet files. Add tests for MVStorageManager.writeViewData() with real hyparquet-writer, then read and verify.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:15:45.223937-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:23:23.390897-06:00","closed_at":"2026-02-03T10:23:23.390897-06:00","close_reason":"Integration tests for MV + Parquet storage are already in place and passing. Test suite covers: (1) MV data persistence to Parquet with write/read cycles using ParquetWriter/ParquetReader, (2) Schema compatibility tests with different schemas, nullable fields, empty views, and various data types, (3) Large data handling with 1000+ rows, 10000 rows, streaming, and sharding, (4) R2/S3 backend integration with mock bucket. All 125 tests passing across 3 test files."}
{"id":"parquedb-f7eb","title":"Refactor: Worker - Duplicate ULID generation logic","description":"ULID generation is implemented twice in the codebase:\n\n1. ParqueDBDO.ts has inline generateULID() function (lines 48-76)\n2. src/utils/id.ts likely has a similar implementation (generateId function)\n\nFiles:\n- src/worker/ParqueDBDO.ts (generateULID, lines 48-76)\n- src/utils/index.ts (generateId)\n\nThe DO implementation uses crypto-secure random via getRandom48Bit, but this logic should be centralized.\n\nRefactor:\n- Move generateULID to src/utils/id.ts\n- Import and use from ParqueDBDO\n- Ensure consistent ULID generation across the codebase\n\nImpact: Single source of truth for ID generation, easier testing","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:34:06.873131-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:05:31.09958-06:00","closed_at":"2026-02-03T11:05:31.09958-06:00","close_reason":"Closed"}
{"id":"parquedb-f7h","title":"[REFACTOR] Populate optimization","description":"Batch relationship fetching for performance","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:43.648403-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:14:51.747804-06:00","closed_at":"2026-02-01T14:14:51.747804-06:00","close_reason":"Closed"}
{"id":"parquedb-f84s","title":"Fix 97 TypeScript type errors","description":"npx tsc --noEmit reports 97 type errors. Most are noUncheckedIndexedAccess issues (array/object index access returning undefined). Priority files: ParqueDBDO.ts (20+ errors), IndexCache.ts (15+ errors), query/bloom.ts (8 errors), parquet/writer.ts (5 errors). Add null checks before array element and object property access.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:38.534107-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T15:58:30.453502-06:00","closed_at":"2026-02-01T15:58:30.453502-06:00","close_reason":"Closed"}
{"id":"parquedb-fcnn","title":"Studio: Add database cloning feature","status":"closed","priority":3,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:35.678822-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:56:26.051834-06:00","closed_at":"2026-02-03T16:56:26.051834-06:00","close_reason":"Closed"}
{"id":"parquedb-feys","title":"Add predicate pushdown for typed mode reads","description":"## Context\nPart of typed storage implementation (parquedb-k7jj design).\n\n## Requirements\n\n1. Extend query execution for typed mode:\n   - Map MongoDB-style filters to Parquet predicates\n   - Apply column statistics for row group skipping\n   - Use native column filtering instead of variant path\n\n2. Filter mapping:\n   - `{ field: value }` -\u003e `{ column: field, op: 'eq', value }`\n   - `{ field: { $gt: n } }` -\u003e `{ column: field, op: 'gt', value: n }`\n   - `{ field: { $in: [...] } }` -\u003e `{ column: field, op: 'in', value: [...] }`\n\n3. Entity reconstruction:\n   - Fast path: use $data if present\n   - Slow path: reconstruct from columns\n\n4. Testing:\n   - Unit tests for filter mapping\n   - Integration tests verifying row group skipping\n   - Performance benchmarks vs flexible mode\n\n## Acceptance Criteria\n- [ ] Filters map to Parquet predicates correctly\n- [ ] Row group statistics used for skipping\n- [ ] Entity reconstruction works with/without $data\n- [ ] Performance improvement measurable\n- [ ] Tests pass\n\n## References\n- Design: docs/architecture/typed-storage.md\n- Depends on: parquedb-xi44 (ParquetWriter typed mode)","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:41:38.645493-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:22:10.178199-06:00","closed_at":"2026-02-03T07:22:10.178199-06:00","close_reason":"Implemented with full test coverage"}
{"id":"parquedb-fgij","title":"Refactor: Storage - DOSqliteBackend.copy has dead code","description":"DOSqliteBackend.copy() at lines 543-562 has an unused variable:\\n\\n```typescript\\nasync copy(source: string, dest: string): Promise\u003cvoid\u003e {\\n  this.ensureSchema()\\n  const sourceKey = this.withPrefix(source)\\n  const _destKey = this.withPrefix(dest)  // Unused\\!\\n  void _destKey // Used in future implementation  // Comment acknowledges it's unused\\n\\n  // ... uses sourceKey but not _destKey\\n  await this.write(dest, data)  // Recalculates destKey inside write()\\n}\\n```\\n\\nRefactor to:\\n- Remove the unused _destKey variable and void statement\\n- Or actually use it by writing directly to the table instead of calling write()\\n\\nFiles affected:\\n- /Users/nathanclevenger/projects/parquedb/src/storage/DOSqliteBackend.ts","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:33:09.83281-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:16:13.600756-06:00","closed_at":"2026-02-03T11:16:13.600756-06:00","close_reason":"Closed"}
{"id":"parquedb-fktt","title":"Implement typeToNamespace/namespaceToType in utils/type-utils.ts","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:19:37.014722-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:22:06.832221-06:00","closed_at":"2026-02-03T15:22:06.832221-06:00","close_reason":"Closed"}
{"id":"parquedb-flhe","title":"Use generic type parameters in CreateInput/FindOptions","description":"CreateInput\u003c_T\u003e and FindOptions\u003c_T\u003e have unused type parameters. These could provide stronger typing for entity operations.","status":"open","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:35.366983-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:35.366983-06:00"}
{"id":"parquedb-fmby","title":"SQL Injection Risk in Dynamic Query Building","description":"**File:** src/worker/DatabaseIndexDO.ts (lines 431-438)\n\n**Issue:** String concatenation pattern for SQL column names could allow SQL injection\n\n**Fix:** \n1. Validate column names against an allowlist of known columns\n2. Use parameterized queries where possible\n3. Escape/quote identifiers properly\n\n**Security Impact:** Potential SQL injection vulnerability","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:10:44.827742-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T18:22:32.853214-06:00","closed_at":"2026-02-03T18:22:32.853214-06:00","close_reason":"Fixed in commit ab28519"}
{"id":"parquedb-frla","title":"Add date validation in manifest conflict detection","description":"manifest.ts:277-278 converts modifiedAt to timestamps without validating they are valid ISO dates. Invalid dates result in NaN, causing incorrect conflict detection.","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:05:17.707358-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:20:22.914694-06:00","closed_at":"2026-02-03T09:20:22.914694-06:00","close_reason":"Added date validation in manifest conflict detection: (1) Created InvalidManifestDateError class for proper error handling, (2) Added parseManifestDate() function that validates and parses date strings, throwing descriptive errors for invalid dates, (3) Added isValidManifestDate() helper for non-throwing validation, (4) Updated diffManifests() to validate dates before comparing timestamps, (5) Updated resolveConflicts() to validate dates when using 'newest' strategy. Includes comprehensive test coverage for all new functionality."}
{"id":"parquedb-froh","title":"Add R2 event notifications for event-driven compaction","description":"Use R2 bucket event notifications instead of polling/timers for compaction and MV refresh.\n\n## Architecture\n\n```\nTailDO  writes raw events to R2\n               object-create notification\n        Queue  Compaction Worker (observable)\n               writes Parquet segments  \n        R2  object-create notification\n              \n        Queue  MV refresh / downstream\n```\n\n## Benefits\n- No polling - push-based, only runs when needed\n- Every step observable via tail workers\n- 5,000 msg/sec throughput\n- Natural backpressure via queue\n- Cost efficient - only runs when data arrives\n\n## Tasks\n\n1. **Set up R2 event notifications**\n   - Configure object-create notifications on logs bucket\n   - Create queue for notifications\n\n2. **Create compaction queue consumer**\n   - Triggered by R2 object-create events\n   - Reads raw event files\n   - Compacts into Parquet segments\n   - Observable via tail\n\n3. **Create MV refresh queue consumer**\n   - Triggered when new Parquet segments written\n   - Refreshes dependent MVs\n   - Observable via tail\n\n4. **Update TailDO**\n   - Write raw events to R2 (triggers notification)\n   - Remove timer-based flush (event-driven instead)\n\n## References\n- https://developers.cloudflare.com/r2/buckets/event-notifications/","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:06:31.024133-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:13:43.930867-06:00","closed_at":"2026-02-03T11:13:43.930867-06:00","close_reason":"Implemented R2 event notifications for event-driven compaction"}
{"id":"parquedb-fsjn","title":"Fix onet-optimized dataset format mismatch","description":"The onet-optimized dataset expects per-collection files:\n- occupations.parquet\n- skills.parquet\n- abilities.parquet\n- knowledge.parquet\n\nBut the ETL script (scripts/etl-onet-optimized.ts) produces consolidated files:\n- data.parquet\n- rels.parquet\n\nNeed to either:\n1. Update the ETL to produce per-collection files\n2. Update the dataset handler to support consolidated format\n3. Remove onet-optimized from DATASETS config until fixed","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T03:24:43.161855-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T03:33:27.93757-06:00","closed_at":"2026-02-03T03:33:27.93757-06:00","close_reason":"Closed"}
{"id":"parquedb-ftc1","title":"Silent Error in Chunked Parquet Reader","description":"**File:** src/workflows/streaming-merge.ts (lines 292-297)\n\n**Issue:** Errors logged but generator returns silently, causing silent data loss\n\n**Fix:** Surface warning in merge stats or throw for critical files","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T14:34:32.306217-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:41:07.713174-06:00","closed_at":"2026-02-03T14:41:07.713174-06:00","close_reason":"Closed"}
{"id":"parquedb-fv0u","title":"Consolidate filter evaluation logic","description":"Filter evaluation logic is duplicated across 4 files: Collection.ts (211-368), query/filter.ts (22-251), query/predicate.ts (339-529), query/update.ts (566-715). Centralize into a single module and import everywhere.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:09.860438-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T16:08:26.917767-06:00","closed_at":"2026-02-01T16:08:26.917767-06:00","close_reason":"Closed"}
{"id":"parquedb-fyk","title":"[GREEN] Create operation implementation","description":"Implement create operations to pass tests","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:16.00472-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:05:20.65757-06:00","closed_at":"2026-02-01T14:05:20.65757-06:00","close_reason":"Closed"}
{"id":"parquedb-g0dc","title":"P2: Document single-writer DO bottleneck and mitigation","description":"Single DO per namespace is a write bottleneck (~30 req/sec). Document this limitation clearly and implement/document sharding by type/time/hash as mitigation strategy.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:10:53.784354-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:36:57.370929-06:00","closed_at":"2026-02-03T15:36:57.370929-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-g2ve","title":"Remove unused fire-and-forget.ts utility","description":"src/utils/fire-and-forget.ts (380 lines) is not imported anywhere. Remove the file after confirming no usage.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:15:07.965853-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T17:16:34.266986-06:00","closed_at":"2026-02-03T17:16:34.266986-06:00","close_reason":"Closed"}
{"id":"parquedb-g4pa","title":"Add Filter Type Guards","description":"Type-safe filter operator checking. Implement isComparisonFilter(), isLogicalFilter() and exhaustive operator handling.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:57:50.343692-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:10:45.697142-06:00","closed_at":"2026-02-03T09:10:45.697142-06:00","close_reason":"Closed"}
{"id":"parquedb-g8r","title":"Implement FsBackend (Node.js filesystem)","description":"Storage backend for Node.js fs module","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:47.66854-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:12:35.878101-06:00","closed_at":"2026-02-01T14:12:35.878101-06:00","close_reason":"Closed"}
{"id":"parquedb-gacl","title":"Refactor: core.ts getSnapshotManager() - Extract to dedicated class","description":"In src/ParqueDB/core.ts, getSnapshotManager() (lines 2371-2476) returns an inline object literal with ~100 lines of implementation. This makes the code harder to read, test, and maintain.\n\nThe snapshot manager should be extracted to a dedicated class:\n```typescript\n// src/ParqueDB/SnapshotManagerImpl.ts\nexport class SnapshotManagerImpl implements SnapshotManager {\n  constructor(\n    private entities: Map\u003cstring, Entity\u003e,\n    private events: Event[],\n    private snapshots: Snapshot[],\n    private storage: StorageBackend\n  ) {}\n  \n  async createSnapshot(entityId: EntityId): Promise\u003cSnapshot\u003e { ... }\n  async listSnapshots(entityId: EntityId): Promise\u003cSnapshot[]\u003e { ... }\n  // ...\n}\n```\n\nBenefits:\n1. Better separation of concerns\n2. Enables unit testing of snapshot logic in isolation\n3. Cleaner dependency injection\n4. Removes 100 lines from the already large core.ts file","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:33:04.206027-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:27:14.157246-06:00","closed_at":"2026-02-03T11:27:14.157246-06:00","close_reason":"Closed","labels":["complexity","refactor"]}
{"id":"parquedb-gbem","title":"DOCS: Write getting started guide for branching","description":"No 'Set up database branching in 5 minutes' tutorial. Need step-by-step for GitHub Actions setup, troubleshooting guide.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:54:49.570685-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:08:07.235762-06:00","closed_at":"2026-02-03T09:08:07.235762-06:00","close_reason":"Closed"}
{"id":"parquedb-gcd","title":"[GREEN] Query execution implementation","description":"Implement query execution to pass tests","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:18.866736-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:02:48.233608-06:00","closed_at":"2026-02-01T14:02:48.233608-06:00","close_reason":"Closed"}
{"id":"parquedb-gd7a","title":"Refactor: Query - Rename non-standard operators to distinguish from MongoDB","description":"## Summary\nParqueDB adds some custom string operators that aren't in MongoDB. These should be clearly marked as extensions to avoid confusion.\n\n### Non-standard Operators\n\n1. **$startsWith** - Not in MongoDB (MongoDB uses $regex)\n2. **$endsWith** - Not in MongoDB (MongoDB uses $regex)  \n3. **$contains** - Not in MongoDB (MongoDB uses $regex)\n\nThese are convenient shortcuts but developers familiar with MongoDB might expect them to work there too.\n\n### Options\n\n1. **Keep as-is but document clearly**:\n   - Add 'ParqueDB extension' notes in JSDoc\n   - Document in README that these are non-standard\n\n2. **Rename to clearly mark as extensions**:\n   - $startsWith -\u003e $pq_startsWith or $_startsWith\n   - This makes it obvious they're not MongoDB standard\n\n3. **Add MongoDB-compatible alternatives**:\n   - Keep both syntaxes\n   - `{ name: { $startsWith: 'foo' } }` (ParqueDB)\n   - `{ name: { $regex: '^foo' } }` (MongoDB)\n\n### Recommendation\nOption 1 - keep names but document clearly. The operators are intuitive and useful.\n\n### Files to Update\n- `src/types/filter.ts` - Add JSDoc noting these are ParqueDB extensions\n- README.md - Document extensions clearly\n\n### Acceptance Criteria\n- [ ] Add 'ParqueDB extension' JSDoc to $startsWith, $endsWith, $contains\n- [ ] Update README filter documentation\n- [ ] Ensure $regex provides equivalent functionality\n- [ ] Add migration guide if renaming","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:33:57.570764-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:05:51.747697-06:00","closed_at":"2026-02-03T11:05:51.747697-06:00","close_reason":"Closed","labels":["refactor"]}
{"id":"parquedb-gf2w","title":"Add generic constraints to RPCCollection interface","description":"TypeScript: Add base constraint to RPCCollection\u003cT\u003e generic - should be RPCCollection\u003cT extends Record\u003cstring, unknown\u003e\u003e","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:09:44.826744-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:01:01.79051-06:00","closed_at":"2026-02-03T11:01:01.79051-06:00","close_reason":"Closed"}
{"id":"parquedb-gjyi","title":"Create MongoDB Migration Guide","description":"**Issue:** No documentation for migrating from MongoDB to ParqueDB\n\n**Fix:** Create docs/guides/migrate-from-mongodb.md covering:\n- API compatibility (find, insert, update, delete)\n- Query operator mapping\n- Schema migration strategies\n- Relationship mapping (refs vs embedded)\n- Data export/import tooling\n- Performance considerations\n\n**Impact:** Lower barrier to adoption for MongoDB users","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:10:49.476714-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T18:22:33.698823-06:00","closed_at":"2026-02-03T18:22:33.698823-06:00","close_reason":"Fixed in commit ab28519"}
{"id":"parquedb-gplh","title":"Add observability/monitoring tests","description":"Observability has only 1 test file (hooks only).\n\nAdd tests for:\n- Performance metrics collection\n- Telemetry validation\n- Tracing/span verification\n- Error rate tracking\n- Query latency histograms\n- Hook execution order","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:32:03.803292-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:06:50.040226-06:00","closed_at":"2026-02-03T10:06:50.040226-06:00","close_reason":"Closed"}
{"id":"parquedb-gpxb","title":"Fix CLI utils tests","description":"tests/unit/cli/utils.test.ts has failing tests for formatBytes() and formatDuration(). Tests expect specific formatting but getting different results.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:04:17.324594-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:05:06.775119-06:00","closed_at":"2026-02-03T12:05:06.775119-06:00","close_reason":"Closed"}
{"id":"parquedb-gqe0","title":"Add runtime type validation for JSON responses in workflows","description":"JSON responses are cast without validation (compaction-queue-consumer.ts:229, 368). Add type guards or runtime validation to prevent silent failures if DO returns unexpected shape. Consider using zod or custom type guards.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:56:28.423181-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:06:22.649986-06:00","closed_at":"2026-02-03T13:06:22.649986-06:00","close_reason":"Closed"}
{"id":"parquedb-gsr5","title":"IcebergBackend catches errors without propagating - silent failures","description":"**High: Silent Failures**\n\nIn /src/backends/iceberg.ts, several catch blocks swallow errors silently:\n\n1. Lines 157-159 (initialize):\n```typescript\nawait this.storage.mkdir(this.warehouse).catch(() =\u003e {\n  // Directory might already exist\n})\n```\n\n2. Lines 748-751 (getTableMetadata):\n```typescript\n} catch {\n  // Table doesn't exist or error reading\n  return null\n}\n```\n\n3. Lines 994-996 (addParentManifests):\n```typescript\n} catch {\n  // Parent manifest list not found or invalid\n}\n```\n\n4. Lines 1138-1139, 1159, 1182 (readEntitiesFromSnapshot):\n```typescript\n} catch {\n  return [] // Manifest list doesn't exist or is invalid\n}\n```\n\n**Impact**: \n- Real errors (permission denied, network issues) are treated the same as 'not found'\n- Debugging production issues becomes very difficult\n- Data corruption could go unnoticed\n\n**Fix**: \n1. Check error types before swallowing\n2. Log unexpected errors even if swallowing\n3. Distinguish between 'not found' and 'error reading'","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:36:59.145501-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:18:27.159831-06:00","closed_at":"2026-02-03T12:18:27.159831-06:00","close_reason":"Fixed all catch blocks in IcebergBackend to check for NotFoundError before swallowing. Other errors (permission, network, corruption) are now properly propagated."}
{"id":"parquedb-gu1o","title":"Architecture: Implement backpressure for event buffering","description":"Event buffering in ParqueDBDO uses fixed thresholds (EVENT_BATCH_COUNT_THRESHOLD=100, EVENT_BATCH_SIZE_THRESHOLD=64KB) without backpressure handling. Under sustained write load, this could lead to unbounded memory growth if flush operations fall behind. Need to implement backpressure: pause accepting writes when buffer exceeds threshold, track pending flush promises, and provide feedback to callers.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:45:53.900884-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:23:49.155635-06:00","closed_at":"2026-02-03T13:23:49.155635-06:00","close_reason":"Implemented backpressure handling for event buffering in both EventWalManager and RelationshipWalManager. Added 26 tests for backpressure functionality."}
{"id":"parquedb-gwvc","title":"Consolidate duplicate getNestedValue implementations","description":"14 different implementations of getNestedValue exist across the codebase. Consolidate into src/utils/nested.ts and update all imports.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:15:02.955325-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T17:28:13.152196-06:00","closed_at":"2026-02-03T17:28:13.152196-06:00","close_reason":"Closed"}
{"id":"parquedb-gxwh","title":"Unsafe JSON Type Assertions Without Validation","description":"**Issue:** Multiple files use `as SomeType` on response.json() without validation\n\n**Fix:** Use type guards consistently or Zod validation","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T14:34:44.397991-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:47:27.47899-06:00","closed_at":"2026-02-03T14:47:27.47899-06:00","close_reason":"Closed"}
{"id":"parquedb-h1q0","title":"Missing validation: deleteMany does not validate filter","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:36:14.909711-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:36:25.343566-06:00","closed_at":"2026-02-03T11:36:25.343566-06:00","close_reason":"Closed"}
{"id":"parquedb-h3fj","title":"CLI: deploy command missing file path validation","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:10.710013-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:36:42.550163-06:00","closed_at":"2026-02-03T11:36:42.550163-06:00","close_reason":"Closed"}
{"id":"parquedb-h75f","title":"Rewrite: Restructure MV docs for clarity","description":"Restructure docs/architecture/materialized-views.md - move API reference to separate doc, add more practical examples at top, ensure / pattern is clearly explained upfront","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:28:24.327098-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:31:34.813455-06:00","closed_at":"2026-02-03T09:31:34.813455-06:00","close_reason":"Closed"}
{"id":"parquedb-h833","title":"Use shared filter module instead of inline filtering","description":"search.ts has inline filtering logic (lines 154-200) that duplicates lib/filter.ts. Should use shared module for consistency and maintainability.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:27:37.326503-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:23:41.487447-06:00","closed_at":"2026-02-03T13:23:41.487447-06:00","close_reason":"Closed"}
{"id":"parquedb-ha40","title":"QueryBuilder missing $mod and $type operators","description":"## Location\nsrc/query/builder.ts, lines 64-84\n\n## Problem\nThe QueryBuilder operatorMap is missing support for:\n- \\$mod: Modulo operator (value % divisor === remainder)\n- \\$type: Type checking operator\n\nThese operators are fully supported in the filter evaluation logic but cannot be used via the fluent QueryBuilder API.\n\n## Impact\n- Users must fall back to raw filter objects to use these operators\n- Not a critical issue since raw filters work fine\n\n## Fix\nAdd to operatorMap:\n```typescript\nconst operatorMap: Record\u003cstring, string\u003e = {\n  // ... existing operators ...\n  'mod': '\\$mod',\n  'type': '\\$type',\n}\n```\n\nAnd update the ComparisonOp type to include 'mod' and 'type'.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:38:41.377814-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:06:05.819541-06:00","closed_at":"2026-02-03T15:06:05.819541-06:00","close_reason":"Closed"}
{"id":"parquedb-hca9","title":"Flaky Test Risks from Real Timers","description":"**Issue:** 61 files use setTimeout without fake timers\n\n**Impact:** Timing-dependent tests can fail intermittently on CI or slower machines\n\n**Fix:**\n1. Identify all timing-dependent tests\n2. Standardize on vi.useFakeTimers() for timer-based tests\n3. Document pattern in testing guidelines\n\n**Files affected:** ~61 test files using setTimeout","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:10:47.321991-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T18:22:32.945149-06:00","closed_at":"2026-02-03T18:22:32.945149-06:00","close_reason":"Fixed in commit ab28519"}
{"id":"parquedb-hcff","title":"Security: JSON.parse calls lack try/catch in multiple migration files","description":"Multiple JSON.parse calls lack proper error handling, which can crash the application with malformed input:\n\nCritical locations:\n- src/migration/mongodb.ts:107,121,237,692 - Parsing user-provided MongoDB exports\n- src/migration/json.ts:80,235,472,551 - Parsing user-provided JSON files\n- src/migration/csv.ts:248 - Parsing CSV with JSON values\n- src/materialized-views/stream-persistence.ts:289,334,503,554,603 - WAL/DLQ parsing\n\nThese are user-facing operations where input is untrusted. Malformed JSON should:\n1. Be caught and logged\n2. Return descriptive error to user\n3. Not crash the process\n\nPattern to implement:\n```typescript\nfunction safeJsonParse\u003cT\u003e(input: string, context: string): Result\u003cT, ValidationError\u003e {\n  try {\n    return { ok: true, value: JSON.parse(input) }\n  } catch (e) {\n    return { ok: false, error: new ValidationError(`Invalid JSON in ${context}: ${e.message}`) }\n  }\n}\n```","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:47:34.616409-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:57:29.58583-06:00","closed_at":"2026-02-03T12:57:29.58583-06:00","close_reason":"Closed"}
{"id":"parquedb-hcv","title":"Implement relationship reading","description":"Read outbound predicates and inbound reverses","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:29.477507-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:13:17.477055-06:00","closed_at":"2026-02-01T14:13:17.477055-06:00","close_reason":"Closed"}
{"id":"parquedb-hex","title":"[RED] Update operation tests","description":"Write failing tests for update, updateOne, updateMany with version checking","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:32.385821-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T04:45:59.415108-06:00","closed_at":"2026-02-02T04:45:59.415108-06:00","close_reason":"Tests implemented in earlier waves"}
{"id":"parquedb-hi2","title":"Implement Collection class","description":"Collection class with MongoDB-style API (find, findOne, get, create, update, delete)","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:41.504177-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:02:39.307626-06:00","closed_at":"2026-02-01T14:02:39.307626-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-hi2","depends_on_id":"parquedb-3yy","type":"blocks","created_at":"2026-01-30T11:51:51.062132-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-hi2","depends_on_id":"parquedb-18b","type":"blocks","created_at":"2026-01-30T11:51:51.145285-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-hi2","depends_on_id":"parquedb-xi8","type":"blocks","created_at":"2026-01-30T11:51:51.22681-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-hibc","title":"Refactor: ParqueDB.ts Proxy - Extract method map to reduce repetitive if-chains","description":"In src/ParqueDB.ts, the Proxy handler's get trap has a long chain of if statements (lines 137-221) checking each method name. This should be refactored to use a method map lookup for cleaner, more maintainable code.\n\nCurrent pattern:\n```typescript\nif (prop === 'registerSchema') {\n  return impl.registerSchema.bind(impl)\n}\nif (prop === 'collection') {\n  return impl.collection.bind(impl)\n}\n// ... 20+ more similar blocks\n```\n\nSuggested refactor:\n```typescript\nconst methodMap: Record\u003cstring, Function\u003e = {\n  registerSchema: impl.registerSchema,\n  collection: impl.collection,\n  find: impl.find,\n  // ...\n}\nconst method = methodMap[prop as string]\nif (method) return method.bind(impl)\n```\n\nThis reduces ~85 lines of code to ~30 lines and makes adding new methods trivial.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:32:26.292847-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:31:57.865133-06:00","closed_at":"2026-02-03T11:31:57.865133-06:00","close_reason":"Closed","labels":["code-quality","refactor"]}
{"id":"parquedb-hijd","title":"CRITICAL: Implement state reconstruction for checkout","description":"checkout command doesn't restore database state - has TODO comments. Branches exist but switching doesn't change data. Need to: 1) Load commit snapshot, 2) Rebuild Parquet files, 3) Restore relationships and event log.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:54:40.865214-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:04:03.885546-06:00","closed_at":"2026-02-03T09:04:03.885546-06:00","close_reason":"Implemented checkout state reconstruction with content-addressed object store"}
{"id":"parquedb-hipy","title":"Add tests for src/worker/DatabaseIndexDO.ts (0% coverage)","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:31.088438-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:06:38.259874-06:00","closed_at":"2026-02-03T09:06:38.259874-06:00","close_reason":"Closed"}
{"id":"parquedb-ho29","title":"Implement Cloudflare Worker API for places.org.ai","description":"Create Cloudflare Worker to serve Places API:\n\nEndpoints:\n- GET /place/:id - Get place by ID (supports geonames, overture, wof IDs)\n- GET /search?q=\u0026near=\u0026type=\u0026limit= - Search places by name\n- GET /geo/nearby?lat=\u0026lng=\u0026radius=\u0026types= - Proximity search\n- GET /geo/within?bounds= - Bounding box search\n- GET /hierarchy/:id - Get admin hierarchy\n- GET /concordance/:source/:id - Cross-reference lookup\n\nSources to query:\n- GeoNames (13M places)\n- Overture Places (64M POIs)\n- Natural Earth (boundaries)\n- Who's On First (hierarchy)\n\nDeploy to places.org.ai domain with Cloudflare Workers.","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:18:40.586784-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:21:37.955873-06:00","closed_at":"2026-02-03T14:21:37.955873-06:00","close_reason":"Implemented places.org.ai Cloudflare Worker with all API endpoints for multi-source place data","dependencies":[{"issue_id":"parquedb-ho29","depends_on_id":"parquedb-2i0i","type":"blocks","created_at":"2026-02-03T13:19:18.628271-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-ho9","title":"[GREEN] ParqueDB class implementation","description":"Implement ParqueDB class to pass tests","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:30.921649-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:02:41.947725-06:00","closed_at":"2026-02-01T14:02:41.947725-06:00","close_reason":"Closed"}
{"id":"parquedb-hodc","title":"Create R2Bucket type adapter to eliminate unsafe casts","description":"The pattern 'as unknown as R2Bucket' is repeated 5 times across workflow files (compaction-migration.ts:162,227,275 and migration-workflow.ts:112,146). Create a typed adapter function toR2Bucket() to centralize this and improve type safety.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:56:19.07988-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:03:15.992641-06:00","closed_at":"2026-02-03T13:03:15.992641-06:00","close_reason":"Closed"}
{"id":"parquedb-hrni","title":"Refactor: Worker - ReadPath background revalidation missing waitUntil","description":"ReadPath.revalidateInBackground() has a TODO noting that it should accept ExecutionContext to use ctx.waitUntil() for proper background task lifecycle management.\n\nFrom ReadPath.ts (lines 514-519):\n```typescript\n/**\n * TODO(parquedb-y9aw): Accept ExecutionContext to use ctx.waitUntil() for proper\n * background task lifecycle management in Workers. Without waitUntil, background\n * revalidation may be terminated early if the Worker instance is recycled.\n */\n```\n\nFiles:\n- src/worker/ReadPath.ts\n\nThe current fire-and-forget pattern works but may result in:\n- Dropped background revalidation if Worker terminates\n- Potential cache staleness\n\nFix: Pass ExecutionContext from caller and wrap revalidation in ctx.waitUntil()\n\nImpact: More reliable stale-while-revalidate caching","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:33:35.443174-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:18:02.660926-06:00","closed_at":"2026-02-03T11:18:02.660926-06:00","close_reason":"Closed"}
{"id":"parquedb-htku","title":"Architecture: Unify Node.js and Workers storage paths","description":"ParqueDB currently has two distinct storage implementations: (1) Node.js uses in-memory globalEntityStore with StorageBackend for persistence, (2) Workers use SQLite in Durable Objects for writes and R2 Parquet for reads. While documented in docs/architecture/entity-storage.md, this creates code duplication and divergent behavior. Consider Option B (Event-Sourced Core) from the doc for unification.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:45:51.857162-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:09:42.108548-06:00","closed_at":"2026-02-03T13:09:42.108548-06:00","close_reason":"Closed"}
{"id":"parquedb-hucx","title":"TEST: Add E2E tests for branch/merge operations","description":"Testing review found weak E2E/integration coverage. Need end-to-end tests for: branch create/checkout/delete, merge with conflicts, state reconstruction.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:54:48.39105-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:07:47.246793-06:00","closed_at":"2026-02-03T09:07:47.246793-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-hufn","title":"Fix: Consolidate duplicate StreamingRefreshEngine","description":"Two separate implementations exist: src/materialized-views/streaming.ts and src/streaming/engine.ts. Violates DRY, can lead to bugs. Consolidate into single implementation or use inheritance/composition.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:15:18.911331-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:18:29.517644-06:00","closed_at":"2026-02-03T10:18:29.517644-06:00","close_reason":"Closed"}
{"id":"parquedb-hzg","title":"Implement pagination","description":"Cursor-based and offset pagination","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:30.117182-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:11:19.132736-06:00","closed_at":"2026-02-01T14:11:19.132736-06:00","close_reason":"Closed"}
{"id":"parquedb-hztu","title":"Refactor: Storage - R2Backend multipart upload state leaks memory","description":"R2Backend tracks active multipart uploads in a Map at line 654:\\n\\n```typescript\\nprivate activeUploads = new Map\u003cstring, { upload: R2MultipartUpload; createdAt: number }\u003e()\\n```\\n\\nWhile there is a cleanupStaleUploads() method, it only runs:\\n1. When startMultipartUpload is called\\n2. Only cleans uploads older than TTL (default 30 minutes)\\n\\nIssues:\\n- If no new uploads are started, stale uploads persist indefinitely\\n- Memory leak if many uploads are started but not completed/aborted\\n- No periodic cleanup mechanism\\n\\nConsider:\\n- Adding a periodic cleanup timer (though this is tricky in Workers)\\n- Adding a maxActiveUploads limit\\n- Documenting the memory implications\\n- Or making cleanup more aggressive\\n\\nFiles affected:\\n- /Users/nathanclevenger/projects/parquedb/src/storage/R2Backend.ts","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:34:10.73399-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:53:19.682988-06:00","closed_at":"2026-02-03T10:53:19.682988-06:00","close_reason":"Closed"}
{"id":"parquedb-i0q8","title":"Cost: Worker - Add SQLite row reduction opportunities in ParqueDBDO","description":"ParqueDBDO can further reduce SQLite row costs (billed per row read/written):\n\nCurrent optimizations:\n- WAL batching for events (events_wal table)\n- Sequence counters initialized once\n\nPotential improvements:\n1. Batch entity metadata updates instead of individual INSERTs\n2. Cache counters across DO invocations (currently re-initialized)\n3. Consider read-through caching for frequently accessed entities\n4. Batch relationship INSERTs when creating entities with links\n\nFiles:\n- src/worker/ParqueDBDO.ts\n\nSpecific areas:\n- create() does separate INSERT for entity and each relationship (line 419-428)\n- update() does individual UPDATE per entity\n- getUnflushedEventCount() queries SUM on every appendEvent check\n\nImpact: Reduced DO costs at scale","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:34:34.541605-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:52:35.711708-06:00","closed_at":"2026-02-03T10:52:35.711708-06:00","close_reason":"Documented SQLite row reduction opportunities in docs/architecture/SQLITE_ROW_REDUCTION.md"}
{"id":"parquedb-i0rb","title":"Add observability hooks","description":"Missing monitoring/tracing integration. Add hooks for: OpenTelemetry spans, structured logging, metrics (query latency, cache hits, storage ops).","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:49.792902-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T16:23:42.259423-06:00","closed_at":"2026-02-01T16:23:42.259423-06:00","close_reason":"Closed"}
{"id":"parquedb-i1qj","title":"R2Backend multipart uploads lack cleanup on error during writeStreaming","description":"**High: Resource Cleanup Issue**\n\nIn /src/storage/R2Backend.ts lines 1004-1013, the writeStreaming method attempts to abort a multipart upload on error:\n\n```typescript\n} catch (error: unknown) {\n  // Attempt to abort on failure\n  try {\n    await upload.abort()\n  } catch (abortError: unknown) {\n    // Best-effort abort: multipart upload abort failure is non-critical\n    logger.debug('Failed to abort multipart upload after error', abortError)\n  }\n  throw error\n}\n```\n\nWhile this is good practice, there are issues:\n\n1. If abort fails, orphaned multipart upload parts remain in R2 billing the customer\n2. The upload variable is obtained from this.bucket.createMultipartUpload() which could also fail between obtaining upload and the try block\n3. No mechanism to track and clean up orphaned uploads periodically\n\n**Impact**: \n- Orphaned multipart upload parts in R2 incur storage costs\n- No visibility into orphaned uploads for cleanup\n\n**Recommendation**:\n1. Add a method to list and abort stale multipart uploads\n2. Consider implementing lifecycle rules or periodic cleanup\n3. Log more context about failed aborts for debugging","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:36:09.112977-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:53:07.552016-06:00","closed_at":"2026-02-03T12:53:07.552016-06:00","close_reason":"Closed"}
{"id":"parquedb-i1zg","title":"TypeScript: Enable exactOptionalPropertyTypes","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:45:56.902174-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T18:24:30.392775-06:00","closed_at":"2026-02-03T18:24:30.392775-06:00","close_reason":"Enabled exactOptionalPropertyTypes in tsconfig.json and fixed all resulting type errors (51 TS2375/TS2379/TS2412 errors). All related tests pass."}
{"id":"parquedb-i2bm","title":"Bug: Soft delete in delete() does not record after state in event","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:56.380746-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:55:45.951514-06:00","closed_at":"2026-02-03T12:55:45.951514-06:00","close_reason":"Closed"}
{"id":"parquedb-i61c","title":"VectorIndex deserialize ignores metric code mismatch","description":"File: src/indexes/vector/hnsw.ts:1188-1190\n\nIn the deserialize method, the metric code is read from the serialized data but ignored:\n```typescript\nconst __metricCode = view.getUint8(offset)\noffset += 1\nvoid __metricCode // Reserved field for future use\n```\n\nThis means if an index was serialized with metric='euclidean' and later loaded with metric='cosine', the search results would be incorrect because distances would be calculated using the wrong metric.\n\nShould validate: if (__metricCode !== expectedMetricCode) throw new Error('Metric mismatch')\n\nNote: This is medium priority because the metric is typically configured via the index definition which would be consistent, but it's still a potential source of silent bugs.","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:38:00.069866-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:57:53.360882-06:00","closed_at":"2026-02-03T15:57:53.360882-06:00","close_reason":"Closed"}
{"id":"parquedb-i7gv","title":"Add Update Operator Type Guards","description":"Type-safe update operator checking. Implement isSetOperator(), isIncOperator(), etc. Validate operator values at runtime.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:57:51.158195-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:10:01.367333-06:00","closed_at":"2026-02-03T09:10:01.367333-06:00","close_reason":"Closed"}
{"id":"parquedb-i9om","title":"Refactor: Query - Consolidate duplicate operator evaluation logic","description":"## Summary\nThere is duplication between filter evaluation in `src/query/filter.ts` and pull condition matching in `src/mutation/operators.ts`.\n\n### Problem\nBoth files have separate implementations of comparison operator matching:\n- `src/query/filter.ts` line 213-329: `evaluateOperators()`\n- `src/mutation/operators.ts` line 609-639: `matchesComparisonOperators()`\n\nThe mutation operators version is less complete (only handles numbers for $gt/$gte/$lt/$lte) while filter.ts handles all types.\n\n### Files Affected\n- `src/query/filter.ts` - Full implementation\n- `src/mutation/operators.ts` - Partial implementation\n\n### Solution\n- Extract shared operator evaluation to a common utility\n- Have both files use the shared implementation\n- Ensure consistent behavior across filter and update operations\n\n### Acceptance Criteria\n- [ ] Create shared `evaluateComparisonOperator()` utility\n- [ ] Update filter.ts to use shared utility\n- [ ] Update operators.ts to use shared utility\n- [ ] Verify mutation operations handle all types correctly\n- [ ] Add tests for type consistency","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:32:39.266241-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:33:26.157669-06:00","closed_at":"2026-02-03T11:33:26.157669-06:00","close_reason":"Closed","labels":["refactor"]}
{"id":"parquedb-icfw","title":"DO WAL Rewrite: Cost optimization for ParqueDBDO","description":"## Summary\n\nRewrite ParqueDBDO to use SQLite as WAL-only (events buffer) rather than full entity store, with bulk operations bypassing SQLite entirely.\n\n## Problem\n\nCurrent implementation stores each entity as a separate SQLite row, causing cost explosion:\n- DO SQLite is 4.5x more expensive than R2 writes\n- Create 100 entities = 200 SQLite rows (entity + event each)\n- Bulk import 10K = 20,000 rows = massive cost\n\n## Solution\n\n1. **SQLite = WAL only**: Remove `entities` and `relationships` tables. Store event batches as blobs (~1000 events per row).\n2. **Bulk bypass**: 5+ entities stream directly to R2 as pending Parquet row groups.\n3. **Read merge**: QueryExecutor merges `data.parquet` + `pending/*.parquet` + unflushed WAL.\n\n## Implementation Phases\n\n### Phase 1: Add WAL batching (non-breaking)\n- [ ] Add `events_wal` table with blob storage\n- [ ] Add `pending_row_groups` table\n- [ ] Implement BatchCoalescer for event batching\n- [ ] Connect to existing SqliteWal infrastructure\n\n### Phase 2: Implement bulk bypass\n- [ ] Add `createMany()`, `updateMany()`, `deleteMany()` to DO\n- [ ] Implement R2 streaming for bulk ops (\u003e= 5 entities)\n- [ ] Update QueryExecutor to merge pending row groups\n\n### Phase 3: Remove entity storage\n- [ ] Stop writing to `entities` table\n- [ ] Update reads to use merged path\n- [ ] Migration script to flush existing entities to R2\n- [ ] Drop `entities` and `relationships` tables\n\n### Phase 4: Optimize reads\n- [ ] Efficient WAL event querying by namespace\n- [ ] Cache pending row groups\n- [ ] Optimize merge strategy\n\n## Cost Savings\n\n| Operation | Current (rows) | Proposed (rows) | Savings |\n|-----------|---------------|-----------------|---------|\n| Create 1 entity | 2 | 0-1 | 50-100% |\n| Create 100 entities | 200 | 1 | 99.5% |\n| Bulk import 10K | 20,000 | 10 | 99.95% |\n\n## Design Doc\n\nSee `docs/architecture/DO_WAL_REWRITE.md` for full details.\n\n## Acceptance Criteria\n\n- [ ] 90%+ reduction in DO SQLite row operations\n- [ ] Bulk imports 10x faster\n- [ ] All existing tests pass\n- [ ] p50 read latency unchanged or improved","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:05:07.977744-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:39:38.469657-06:00","closed_at":"2026-02-03T10:39:38.469657-06:00","close_reason":"Closed","labels":["cost-optimization","enhancement","workers"]}
{"id":"parquedb-icq8","title":"P1: Add AbortController timeout for JWKS fetch","description":"In src/integrations/payload/auth.ts:207-225, while timeout is configured, there's no abort signal handling for hung connections. Add AbortController with explicit timeout handling.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:10:31.592152-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:24:28.954308-06:00","closed_at":"2026-02-03T15:24:28.954308-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-idi2","title":"Fix memory leak in rpc-do batching","description":"Memory leak in rpc-do batching - pending requests not cleaned up on transport close","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:09:39.651276-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:15:30.259502-06:00","closed_at":"2026-02-03T09:15:30.259502-06:00","close_reason":"Fixed by parallel subagents"}
{"id":"parquedb-idii","title":"R2Backend.append uses undefined MAX_RETRIES and BASE_DELAY_MS constants","description":"**Critical Bug**: In /src/storage/R2Backend.ts lines 443-454, the append() method references MAX_RETRIES and BASE_DELAY_MS constants that are undefined. The constants R2_APPEND_MAX_RETRIES and R2_APPEND_BASE_DELAY_MS are imported from constants.ts, but lines 443-448 use the wrong names MAX_RETRIES and BASE_DELAY_MS. This will cause a ReferenceError at runtime when retrying append operations.\n\n**Location**: src/storage/R2Backend.ts:443-448\n\n**Impact**: Any concurrent append operation that needs to retry will crash with undefined reference error.\n\n**Fix**: Replace MAX_RETRIES with R2_APPEND_MAX_RETRIES and BASE_DELAY_MS with R2_APPEND_BASE_DELAY_MS in the retry logic.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:34:41.465249-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:35:03.404418-06:00","closed_at":"2026-02-03T11:35:03.404418-06:00","close_reason":"Closed"}
{"id":"parquedb-ie7c","title":"Add alerting for compaction backlog","description":"No alerting exists for compaction lag. If windows accumulate without being processed, failures are silent. Add monitoring that alerts when: 1) Window count exceeds threshold, 2) Oldest window age exceeds threshold, 3) Workflow failures exceed threshold. Consider integration with Cloudflare Analytics or external monitoring.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:33:12.797471-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:50:24.345465-06:00","closed_at":"2026-02-03T13:50:24.345465-06:00","close_reason":"Closed"}
{"id":"parquedb-iecq","title":"Product: End-to-end example application demonstrating core features","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:45:48.776996-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:54:45.020213-06:00","closed_at":"2026-02-03T12:54:45.020213-06:00","close_reason":"Closed"}
{"id":"parquedb-ihqb","title":"DeltaBackend uses generic Error instead of typed errors","description":"**High: Missing Typed Errors**\n\nIn /src/backends/delta.ts, many operations throw generic Error instead of typed errors:\n\n- Line 250: `throw new Error('Backend is read-only')`  \n- Line 287: `throw new Error('Entity not found: ...')`\n- Line 310: `throw new Error('Backend is read-only')`\n- Line 351: `throw new Error('Backend is read-only')`\n- Line 383: `throw new Error('Backend is read-only')`\n- Line 413: `throw new Error('Backend is read-only')`\n- Line 451: `throw new Error('Table not found: ...')`\n\n**Impact**: Callers cannot reliably catch and handle specific error types.\n\n**Fix**: Use the typed errors from src/backends/types.ts:\n- ReadOnlyError for read-only operations\n- BackendEntityNotFoundError for missing entities\n- TableNotFoundError for missing tables\n\nExample fix:\n```typescript\n// Before:\nthrow new Error('Backend is read-only')\n\n// After:\nthrow new ReadOnlyError('create', 'DeltaBackend')\n```","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:44.598581-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:19:14.039251-06:00","closed_at":"2026-02-03T13:19:14.039251-06:00","close_reason":"Closed"}
{"id":"parquedb-imes","title":"P3: Enable noPropertyAccessFromIndexSignature","description":"tsconfig.json:34 - Currently false. Consider enabling for safer property access on index signature types.","status":"open","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:33.101892-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:33.101892-06:00"}
{"id":"parquedb-inc9","title":"Horizontal queue partitioning for high-throughput namespaces","description":"Single queue consumer could bottleneck under extreme load. Implement queue partitioning: 1) Create multiple queues (parquedb-compaction-events-0 through -N), 2) R2 event notifications route to queue based on hash(namespace), 3) Each queue has dedicated consumer, 4) Consumers are stateless - just dispatch to namespace-sharded DOs. Benefits: N queues = Nx throughput. Cloudflare supports multiple queue consumers per worker.","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:57:27.332344-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:12:56.891761-06:00","closed_at":"2026-02-03T14:12:56.891761-06:00","close_reason":"Closed"}
{"id":"parquedb-ip3a","title":"Epic: Pluggable Entity Backends (Iceberg/Delta Lake)","description":"Support multiple table formats for entity storage while keeping relationships in ParqueDB format.\n\n## Goals\n- Native backend (current ParqueDB format)\n- Iceberg backend (DuckDB/Spark/Snowflake compatible)\n- Delta Lake backend\n- R2 Data Catalog integration\n\n## Architecture\n- EntityBackend interface abstracts storage format\n- Relationships always stored in ParqueDB rels/ format\n- IceType schema compatibility\n\n## Docs\n- docs/architecture/pluggable-backends.md\n- docs/architecture/icetype-relationships.md","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:04:36.783083-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:34:31.458344-06:00","closed_at":"2026-02-03T07:34:31.458344-06:00","close_reason":"Iceberg and Delta Lake backends fully implemented and tested. Native backend and R2 Data Catalog can be tracked separately."}
{"id":"parquedb-ip3a.1","title":"RED: Write tests for IcebergBackend.appendEntities","description":"Write failing tests for appendEntities that:\n- Creates Parquet file in correct Iceberg location\n- Encodes entities with Variant $data column\n- Creates manifest entry with file stats\n- Creates new snapshot\n- Updates table metadata\n\nTest file: tests/unit/backends/iceberg-write.test.ts","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:04:45.314689-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:06:49.734711-06:00","closed_at":"2026-02-03T07:06:49.734711-06:00","close_reason":"Tests written and failing as expected (14 fail, 5 pass)","dependencies":[{"issue_id":"parquedb-ip3a.1","depends_on_id":"parquedb-ip3a","type":"parent-child","created_at":"2026-02-03T07:04:45.315351-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-ip3a.2","title":"GREEN: Implement IcebergBackend.appendEntities","description":"Implement appendEntities to pass the tests:\n- Use ParquetWriter to create data file\n- Use encodeVariant for $data column\n- Use @dotdo/iceberg ManifestGenerator\n- Use SnapshotBuilder for new snapshot\n- Use MetadataWriter to commit\n\nLocation: src/backends/iceberg.ts","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:04:51.6066-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:18:49.522771-06:00","closed_at":"2026-02-03T07:18:49.522771-06:00","close_reason":"Implemented appendEntities with ParquetWriter, Variant encoding, ManifestGenerator, SnapshotBuilder. All 19 tests pass.","dependencies":[{"issue_id":"parquedb-ip3a.2","depends_on_id":"parquedb-ip3a","type":"parent-child","created_at":"2026-02-03T07:04:51.607275-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-ip3a.2","depends_on_id":"parquedb-ip3a.1","type":"blocks","created_at":"2026-02-03T07:05:11.862212-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-ip3a.3","title":"RED: Write tests for IcebergBackend.readEntitiesFromSnapshot","description":"Write failing tests for readEntitiesFromSnapshot:\n- Reads manifest list from snapshot\n- Reads data files from manifests\n- Decodes Variant $data column\n- Applies filter predicates\n- Applies sort/limit/skip\n\nTest file: tests/unit/backends/iceberg-read.test.ts","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:05:03.914664-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:18:54.107734-06:00","closed_at":"2026-02-03T07:18:54.107734-06:00","close_reason":"Tests and implementation for readEntitiesFromSnapshot included in iceberg-write.test.ts. All 19 tests pass.","dependencies":[{"issue_id":"parquedb-ip3a.3","depends_on_id":"parquedb-ip3a","type":"parent-child","created_at":"2026-02-03T07:05:03.915507-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-ip3a.3","depends_on_id":"parquedb-ip3a.2","type":"blocks","created_at":"2026-02-03T07:05:12.09205-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-ip3a.4","title":"GREEN: Implement IcebergBackend.readEntitiesFromSnapshot","description":"Implement readEntitiesFromSnapshot:\n- Parse manifest list from snapshot\n- Read manifests to get file list\n- Use hyparquet to read Parquet files\n- Decode Variant $data with decodeVariant\n- Apply filter using matchesFilter\n- Apply sort/limit/skip\n\nLocation: src/backends/iceberg.ts","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:05:04.042622-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:18:54.136536-06:00","closed_at":"2026-02-03T07:18:54.136536-06:00","close_reason":"Tests and implementation for readEntitiesFromSnapshot included in iceberg-write.test.ts. All 19 tests pass.","dependencies":[{"issue_id":"parquedb-ip3a.4","depends_on_id":"parquedb-ip3a","type":"parent-child","created_at":"2026-02-03T07:05:04.043367-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-ip3a.4","depends_on_id":"parquedb-ip3a.3","type":"blocks","created_at":"2026-02-03T07:05:11.977463-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-ip3a.5","title":"REFACTOR: Extract shared Parquet utilities","description":"Refactor common patterns:\n- Entity row serialization (shredding + Variant)\n- Parquet write options (compression, row groups)\n- Schema generation for entities\n- Filter to predicate pushdown\n\nMay create src/backends/parquet-utils.ts","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:05:04.165977-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:31:04.948288-06:00","closed_at":"2026-02-03T07:31:04.948288-06:00","close_reason":"Extracted shared utilities to parquet-utils.ts: entityToRow, rowToEntity, buildEntityParquetSchema, matchesFilter, generateEntityId, extractDataFields, base64 helpers. All tests passing.","dependencies":[{"issue_id":"parquedb-ip3a.5","depends_on_id":"parquedb-ip3a","type":"parent-child","created_at":"2026-02-03T07:05:04.16679-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-ip3a.5","depends_on_id":"parquedb-ip3a.4","type":"blocks","created_at":"2026-02-03T07:05:12.205605-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-ip3a.6","title":"RED: Write integration tests for IcebergBackend","description":"Write integration tests:\n- Full CRUD cycle with Iceberg\n- Time travel queries\n- Schema evolution\n- Compaction/vacuum\n\nTest file: tests/integration/backends/iceberg.test.ts","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:05:04.291096-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:26:29.375838-06:00","closed_at":"2026-02-03T07:26:29.375838-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-ip3a.6","depends_on_id":"parquedb-ip3a","type":"parent-child","created_at":"2026-02-03T07:05:04.296423-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-ip3a.6","depends_on_id":"parquedb-ip3a.4","type":"blocks","created_at":"2026-02-03T07:05:12.322266-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-ip3a.7","title":"GREEN: Fix integration test failures","description":"Fix any integration test failures discovered during testing.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:05:04.419572-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:34:23.706331-06:00","closed_at":"2026-02-03T07:34:23.706331-06:00","close_reason":"All 44 Iceberg integration tests now passing","dependencies":[{"issue_id":"parquedb-ip3a.7","depends_on_id":"parquedb-ip3a","type":"parent-child","created_at":"2026-02-03T07:05:04.420402-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-ip3a.7","depends_on_id":"parquedb-ip3a.6","type":"blocks","created_at":"2026-02-03T07:05:12.438134-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-ip3a.8","title":"RED: Write tests for DeltaBackend","description":"Write unit tests for DeltaBackend following TDD:\n- create/bulkCreate operations\n- get/find/count operations  \n- update/delete operations\n- Delta Lake transaction log\n- Time travel queries\n- Schema evolution\n\nTest file: tests/unit/backends/delta-write.test.ts\n\nReference: tests/unit/backends/iceberg-write.test.ts for patterns","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:22:51.202609-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:26:15.579218-06:00","closed_at":"2026-02-03T07:26:15.579218-06:00","close_reason":"53 RED phase tests written for DeltaBackend, all failing as expected","dependencies":[{"issue_id":"parquedb-ip3a.8","depends_on_id":"parquedb-ip3a","type":"parent-child","created_at":"2026-02-03T07:22:51.203434-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-ip3a.9","title":"GREEN: Implement DeltaBackend","description":"Implement DeltaBackend to pass tests:\n- Use ParquetWriter for data files\n- Implement Delta Lake transaction log (_delta_log/)\n- JSON-based commit files (00000000000000000000.json)\n- Checkpoint files for optimization\n- Time travel via version/timestamp\n\nLocation: src/backends/delta.ts\n\nReference: src/backends/iceberg.ts for patterns","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:23:00.118274-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:32:24.702914-06:00","closed_at":"2026-02-03T07:32:24.702914-06:00","close_reason":"DeltaBackend GREEN phase complete - all 53 tests passing","dependencies":[{"issue_id":"parquedb-ip3a.9","depends_on_id":"parquedb-ip3a","type":"parent-child","created_at":"2026-02-03T07:23:00.119276-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-ip3a.9","depends_on_id":"parquedb-ip3a.8","type":"blocks","created_at":"2026-02-03T07:23:00.235049-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-isa6","title":"Fix weak hash fallback in SyncEngine","description":"The fallback hash function in engine.ts:547-553 is a weak DJB2-like hash with only 32 bits of entropy. This could cause hash collisions during sync. Either require crypto.subtle or use a proper hashing library.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:04:56.033616-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:06:36.84953-06:00","closed_at":"2026-02-03T08:06:36.84953-06:00","close_reason":"Closed"}
{"id":"parquedb-it8k","title":"Fix ai-sdk-middleware tests","description":"tests/unit/integrations/ai-sdk-middleware.test.ts has 17+ failing tests for wrapGenerate, wrapStream, queryCacheEntries, queryLogEntries, clearExpiredCache, getCacheStats, and Logging Levels.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:17:08.652819-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:19:49.871175-06:00","closed_at":"2026-02-03T12:19:49.871175-06:00","close_reason":"Closed"}
{"id":"parquedb-itbz","title":"Cold/warm/cached performance matrix benchmarks","description":"Create systematic benchmarks measuring:\n\n1. Cold Start: First request after worker idle (measures JIT, initialization)\n2. Warm: Subsequent requests without cache (measures pure execution)\n3. Cached: Requests hitting Cache API (measures cache efficiency)\n\nFor each category, measure:\n- Simple get by ID\n- Filter query with index\n- Filter query without index (scan)\n- Relationship traversal\n- Aggregation\n\nMust use deployed workers with external benchmark runner to ensure accurate cold start measurement.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:22:41.472521-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:32:56.194527-06:00","closed_at":"2026-02-03T10:32:56.194527-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-itbz","depends_on_id":"parquedb-4srx","type":"blocks","created_at":"2026-02-03T09:22:46.787415-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-itsc","title":"Fix mv-optimizer tests","description":"tests/unit/query/mv-optimizer.test.ts has 8 failing tests for MVQueryOptimizer: optimize, filter compatibility, and integration scenarios.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:51:36.715494-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:52:08.40364-06:00","closed_at":"2026-02-03T13:52:08.40364-06:00","close_reason":"Closed"}
{"id":"parquedb-iwb7","title":"Re-enable RPC client exports","description":"Client-side RPC exports are disabled in src/index.ts:\n\n```typescript\n// export {\n//   ParqueDBClient,\n//   createParqueDBClient,\n//   type ParqueDBClientOptions,\n// } from './client'\n```\n\nTasks:\n- Review why exports were disabled\n- Fix any issues preventing export\n- Re-enable and document RPC pipelining patterns\n- Add integration tests for client usage","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:31:52.603748-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:11:41.320038-06:00","closed_at":"2026-02-03T10:11:41.320038-06:00","close_reason":"Closed"}
{"id":"parquedb-ixbe","title":"Inconsistent Error Handling Patterns","description":"## Problem\nMix of custom errors, generic errors, instanceof checks that fail across modules.\n\n## Fix\nStandardize with error codes or string-based type checking.\n\n## Priority\nP1 - HIGH","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:10:22.475586-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T18:22:30.474223-06:00","closed_at":"2026-02-03T18:22:30.474223-06:00","close_reason":"Fixed in commit ab28519"}
{"id":"parquedb-j07m","title":"P0: Fix flaky/time-dependent tests","description":"65 time-dependent/flaky tests identified. Replace setTimeout in race tests with vi.useFakeTimers() and vi.advanceTimersByTime() for deterministic execution. Tests using real delays make suite slow and potentially flaky.","status":"in_progress","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:10:18.097699-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T18:54:45.557217-06:00"}
{"id":"parquedb-j0n8","title":"Implement compact() for filesystem Iceberg catalogs","description":"The IcebergBackend.compact() method currently returns no-op results for filesystem catalogs. Implement actual compaction: read multiple small data files, merge into fewer larger files, and update Iceberg metadata.","status":"open","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T18:03:21.445067-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T18:03:21.445067-06:00"}
{"id":"parquedb-j17q","title":"Refactor: Storage - Extract common error classes into shared module","description":"Currently, error classes are duplicated across multiple backends:\\n\\n1. MemoryBackend exports: FileNotFoundError, VersionMismatchError, FileExistsError, DirectoryNotEmptyError, DirectoryNotFoundError\\n2. R2Backend defines its own: R2OperationError, R2ETagMismatchError, R2NotFoundError\\n3. DOSqliteBackend defines its own: DOSqliteNotFoundError, DOSqliteETagMismatchError, DOSqliteFileExistsError\\n4. FsBackend imports errors from MemoryBackend and defines PathTraversalError\\n5. FsxBackend throws generic Error instances instead of typed errors\\n\\nRefactor to:\\n- Create src/storage/errors.ts with base error classes\\n- Have backend-specific errors extend base classes where appropriate\\n- Standardize error handling so consumers can catch by base type\\n- Update FsxBackend to use proper typed errors\\n\\nFiles affected:\\n- /Users/nathanclevenger/projects/parquedb/src/storage/MemoryBackend.ts\\n- /Users/nathanclevenger/projects/parquedb/src/storage/R2Backend.ts\\n- /Users/nathanclevenger/projects/parquedb/src/storage/DOSqliteBackend.ts\\n- /Users/nathanclevenger/projects/parquedb/src/storage/FsBackend.ts\\n- /Users/nathanclevenger/projects/parquedb/src/storage/FsxBackend.ts","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:32:25.419597-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:11:30.217545-06:00","closed_at":"2026-02-03T07:11:30.217545-06:00","close_reason":"Implemented"}
{"id":"parquedb-j1u5","title":"[DOCS] Document consistency model clearly","description":"The codebase mixes strong consistency (Durable Objects) with eventual consistency (R2 reads) without clear documentation. Users need to understand:\n\n1. When reads are strongly consistent vs eventually consistent\n2. Read-after-write guarantees (or lack thereof)\n3. The CQRS architecture implications\n4. How stale data is handled\n\nAdd CONSISTENCY.md to docs/architecture/ explaining the model.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T13:35:48.345887-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:43:25.740429-06:00","closed_at":"2026-02-01T13:43:25.740429-06:00","close_reason":"Closed"}
{"id":"parquedb-j4on","title":"Centralize workflow Env types","description":"Local Env interfaces in workflow files (compaction-migration.ts:112-114, compaction-queue-consumer.ts:95-101) shadow/diverge from main Env in src/types/worker.ts. Import and use Pick\u003cEnv, ...\u003e to derive local types and ensure consistency.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:56:34.980611-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:06:51.636424-06:00","closed_at":"2026-02-03T13:06:51.636424-06:00","close_reason":"Closed"}
{"id":"parquedb-j4r","title":"[GREEN] Traversal implementation","description":"Implement traversal methods to pass tests","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:50.024119-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:13:17.266172-06:00","closed_at":"2026-02-01T14:13:17.266172-06:00","close_reason":"Closed"}
{"id":"parquedb-j5n","title":"Indexing","description":"FTS, vector, bloom filter, and secondary indexes","status":"closed","priority":2,"issue_type":"epic","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:50:32.878379-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:34:08.892856-06:00","closed_at":"2026-02-01T14:34:08.892856-06:00","close_reason":"Closed"}
{"id":"parquedb-j6is","title":"Review and fix 25 skipped tests","description":"There are 25 occurrences of .skip() or .todo() across tests. Review and either fix or document why they're skipped.","status":"open","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:39.927089-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:39.927089-06:00"}
{"id":"parquedb-j7l1","title":"Non-Null Assertions After Map Operations","description":"## Problem\nMultiple files use map.get(key)! pattern which can cause runtime errors if the Map doesn't contain the expected key.\n\n## Location\nMultiple files throughout codebase\n\n## Fix\nCreate a getOrSet utility function to safely handle Map operations.\n\n## Priority\nP1 - HIGH","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:10:29.26484-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T18:22:30.565441-06:00","closed_at":"2026-02-03T18:22:30.565441-06:00","close_reason":"Fixed in commit ab28519"}
{"id":"parquedb-j8bd","title":"Documentation: Resolve 39 TODO/FIXME/HACK comments","description":"39 TODO/FIXME/HACK comments across 20 files indicate incomplete implementations:\n- src/backends/iceberg.ts (7 TODOs)\n- src/backends/migration.ts (4 TODOs)\n- src/workflows/compaction-migration.ts (4 TODOs)\n- src/git/index.ts (3 TODOs)\n- src/worker/github/webhooks.ts (3 TODOs)\n\nEach should be either:\n1. Implemented and comment removed\n2. Converted to beads issue for tracking\n3. Removed if no longer relevant","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:47:23.953976-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:47:59.236502-06:00","closed_at":"2026-02-03T16:47:59.236502-06:00","close_reason":"Closed"}
{"id":"parquedb-j8en","title":"Monitoring dashboard for compaction system","description":"Create observability dashboard for compaction operations. Include: 1) Windows pending/processing/completed over time, 2) Compaction latency histogram, 3) Bytes compacted per hour, 4) OCC retry rates for Iceberg/Delta, 5) Workflow failure rates, 6) Writer activity heatmap. Options: Cloudflare Analytics, Grafana with Workers Analytics Engine, or custom dashboard in ParqueDB Studio.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:57:03.617989-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:16:40.321309-06:00","closed_at":"2026-02-03T14:16:40.321309-06:00","close_reason":"Closed"}
{"id":"parquedb-j8wg","title":"Fix EntityBackend generic variance mismatch","description":"Interface uses T=Record\u003cstring,unknown\u003e but implementations use T extends EntityData. Causes ~19 TypeScript errors. Need to align interface to use T extends EntityData constraint.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T16:27:43.218239-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:31:32.570275-06:00","closed_at":"2026-02-03T16:31:32.570275-06:00","close_reason":"Closed"}
{"id":"parquedb-jbqi","title":"Benchmark search workers at scale","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:53:10.850887-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:55:55.313886-06:00","closed_at":"2026-02-03T13:55:55.313886-06:00","close_reason":"Added benchmark-search.ts with CPU stress tests"}
{"id":"parquedb-jc5","title":"Implement full-text search","description":"Inverted index for $text queries","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:53.393831-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:27:18.662101-06:00","closed_at":"2026-02-01T14:27:18.662101-06:00","close_reason":"Closed"}
{"id":"parquedb-jc6e","title":"Cloudflare Snippets API integration for deployment","description":"Implement deployment tooling for Cloudflare Snippets.\n\n## API Reference\nhttps://developers.cloudflare.com/workers/static-assets/direct-upload/\n\n## Tasks\n1. Create scripts/deploy-snippet.ts:\n   - Build and minify snippet\n   - Upload to Cloudflare Snippets API\n   - Verify deployment\n\n2. Create scripts/test-snippet.ts:\n   - Run snippet locally with constraints\n   - Measure CPU time\n   - Count subrequests\n   - Check memory usage\n\n3. Add to CI/CD:\n   - Build snippets on PR\n   - Report bundle sizes\n   - Deploy on merge to main\n\n## CLI Interface\n```bash\n# Build and check size\nbun run snippets:build\n\n# Deploy specific snippet\nbun run snippets:deploy product-lookup\n\n# Test locally with constraints\nbun run snippets:test product-lookup --cpu-limit=5ms\n```","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:29:20.327558-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:37:37.386956-06:00","closed_at":"2026-02-03T10:37:37.386956-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-jc6e","depends_on_id":"parquedb-vhif","type":"blocks","created_at":"2026-02-03T10:29:25.482325-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-jeku","title":"Fix typo: buffersToProcss -\u003e buffersToProcess","description":"Typo in variable name at src/materialized-views/streaming.ts line 485. Should be 'buffersToProcess' not 'buffersToProcss'.","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:02:23.148437-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:12:45.567137-06:00","closed_at":"2026-02-03T11:12:45.567137-06:00","close_reason":"Closed"}
{"id":"parquedb-jfuo","title":"P2: Move QueryExecutor file cache to Cache API","description":"QueryExecutor.fileCache is per-isolate, causing high memory usage under load. Move to Cache API for shared caching across isolates in Workers environment.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:11:11.627342-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:20:08.938359-06:00","closed_at":"2026-02-03T16:20:08.938359-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-jhzb","title":"Real-time Subscriptions","description":"Add change notification support to ParqueDB\n\n## Scope\n- WebSocket/SSE support for real-time updates\n- Collection-level subscriptions\n- Filter-based subscriptions for targeted updates\n\n## Acceptance Criteria\n- [ ] WebSocket and SSE endpoints available\n- [ ] Clients can subscribe to collection changes\n- [ ] Filter-based subscriptions only notify on matching changes\n- [ ] Efficient fan-out for multiple subscribers","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:58:01.843774-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:16:06.754428-06:00","closed_at":"2026-02-03T09:16:06.754428-06:00","close_reason":"Closed"}
{"id":"parquedb-ji47","title":"P2: Resolve TODO/FIXME/HACK comments","description":"39 TODO/FIXME/HACK comments identified in codebase. Audit and resolve or convert to tracked issues. Technical debt accumulation risk.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:11:13.855768-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:20:08.934172-06:00","closed_at":"2026-02-03T16:20:08.934172-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-jie3","title":"Create AI integrations documentation","description":"Documentation: Create docs/integrations/ai/ with overview.md, ai-database-adapter.md, vercel-ai-sdk.md, mcp-server.md, evalite.md, embeddings.md","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:09:49.922919-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:15:30.384904-06:00","closed_at":"2026-02-03T09:15:30.384904-06:00","close_reason":"Fixed by parallel subagents"}
{"id":"parquedb-jiij","title":"Extract slug validation to shared utility","description":"Duplicate slug validation regex in sync.ts:392-394 and DatabaseIndexDO.ts:687-688. Extract to shared utility with named constants for SLUG_MIN_LENGTH and SLUG_MAX_LENGTH.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:05:16.615789-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:42:59.586988-06:00","closed_at":"2026-02-03T10:42:59.586988-06:00","close_reason":"Closed"}
{"id":"parquedb-jjn9","title":"Fix race condition pattern in maybeFlush()","description":"In streaming.ts maybeFlush(), the check 'if (\\!this.processingPromise)' and assignment on next line are not atomic. Two concurrent calls could both pass check. Use proper mutex pattern. Lines 440-452","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:02:21.992369-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:12:21.626541-06:00","closed_at":"2026-02-03T11:12:21.626541-06:00","close_reason":"Closed"}
{"id":"parquedb-jkn9","title":"P2: Add max pending events configuration","description":"src/ParqueDB/core.ts:144 - pendingEvents array has no size limit. Add backpressure mechanism.","status":"open","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:17.476375-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:17.476375-06:00"}
{"id":"parquedb-jl9a","title":"Move integration type casts to cast.ts","description":"Integration code has type conversions not in the centralized cast.ts:\n\n```typescript\n// integrations.ts\nconst def = indexDef as { name?: string; fields?: unknown[] }\nconst relDef = field as import('@icetype/core').RelationDefinition\n```\n\nMove these to src/types/cast.ts for:\n- Consistency with existing cast pattern\n- Documentation of why casts are safe\n- Single location for type escape hatches","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:32:06.777799-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:16:11.439063-06:00","closed_at":"2026-02-03T10:16:11.439063-06:00","close_reason":"Closed"}
{"id":"parquedb-jm71","title":"Fix: Unsafe type assertions in evalite mv-integration","description":"Multiple 'as unknown as' casts bypass TypeScript safety. Define proper interface types. File: src/integrations/evalite/mv-integration.ts lines 651-658","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:15:25.874903-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:20:21.416705-06:00","closed_at":"2026-02-03T10:20:21.416705-06:00","close_reason":"Closed"}
{"id":"parquedb-jomp","title":"Explore hyparquet-lite for Cloudflare Snippets","description":"Create a minimal hyparquet implementation optimized for Cloudflare Snippets constraints:\n\n## Constraints\n- **32KB memory** - Stream/scan without full-file loading\n- **5ms CPU** - Simple predicates only, pre-indexed data\n- **5 subrequests** - Cascade to other snippets or fetch partitions\n\n## Proposed Exports in @dotdo/hyperparquet\n\n```typescript\n// Full version (current)\nimport { parquetRead } from '@dotdo/hyperparquet'\n\n// Small version (~15KB) - basic reading, common compressions\nimport { parquetRead } from '@dotdo/hyperparquet/small'\n\n// Tiny version (~5KB) - minimal for snippets\nimport { parquetRead } from '@dotdo/hyperparquet/tiny'\n```\n\n## Tiny Version Scope\n\n**Keep:**\n- Footer parsing (metadata offset from last 8 bytes)\n- Column chunk offset reading\n- Row group metadata\n- Uncompressed + Snappy only\n- Simple predicates (=, \u003c, \u003e, \u003c=, \u003e=)\n- Direct ArrayBuffer access\n- Streaming row iteration\n\n**Cut:**\n- Complex compressions (zstd, lz4, brotli)\n- Bloom filters\n- Statistics-based predicate pushdown\n- Delta encoding\n- Dictionary pages (or minimal support)\n- Schema evolution\n- Nested types (keep flat only)\n\n## Use Cases\n\n1. **Static Asset queries** - Read parquet from Workers Static Assets (0ms latency)\n2. **Pre-indexed lookups** - JSON index  byte offset  range fetch  decode row\n3. **Cascading snippets** - Router snippet fans out to partition snippets\n\n## Architecture\n\n```\nSnippet 1 (Router)     Snippet 2-N (Partitions)\n Parse query         Range fetch row group\n Read footer         Decode columns\n Fan out            Apply filter\n 2ms CPU             3ms CPU each\n```\n\n## Success Criteria\n\n- [ ] Tiny bundle: \u003c5KB minified+gzipped\n- [ ] Works within 32KB memory for 1000-row scans\n- [ ] \u003c3ms CPU for single row group scan\n- [ ] API compatible with full hyparquet\n- [ ] Tree-shakeable exports","notes":"Design doc created at docs/architecture/hyparquet-lite.md. Key findings: 1) hyparquet is ~134KB source, ~50KB minified. 2) Proposed 3 tiers: /tiny (4KB gzip, metadata only), /small (12KB gzip, flat columns), Full (25KB gzip). 3) Recommend separate hyparquet-tiny package for Snippets. 4) Core files needed: thrift.js (5KB), metadata.js (14KB), constants.js (2KB), schema.js (4KB). 5) Snappy built-in (4KB), ZSTD adds 33KB. 6) Biggest contributors: metadata.js, variant.js, assemble.js.","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:18:29.825237-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:35:03.684336-06:00","closed_at":"2026-02-03T10:35:03.684336-06:00","close_reason":"Research complete with POC validation. Key findings: (1) No hyparquet-lite package needed - existing hyparquet tree-shakes to 4KB gzipped for metadata-only imports. (2) Cloudflare Snippets compatible - bundle size, memory, and CPU all within limits. (3) POC created in poc/hyparquet-lite/ with working examples. (4) Design doc updated at docs/architecture/hyparquet-lite.md. Recommended approach: use existing hyparquet with selective imports."}
{"id":"parquedb-jppe","title":"Replace 'as any' with proper interface casting in ai-database adapter","description":"TypeScript: Replace 'as any' cast on line 407 of adapter.ts with proper BatchLoaderDB interface casting","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:09:42.738051-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:15:30.321001-06:00","closed_at":"2026-02-03T09:15:30.321001-06:00","close_reason":"Fixed by parallel subagents"}
{"id":"parquedb-jq6u","title":"Secondary index queries slower than full scans","description":"Benchmark reveals indexed queries are 50-70% SLOWER than equivalent full scans:\n\nEvidence:\n- imdb100k-eq-movie: indexed 259ms vs scan 129ms (2x slower)\n- imdb100k-eq-tvseries: indexed 538ms vs scan 157ms (3x slower)\n- imdb100k-range-year-2020s: indexed 321ms vs scan 192ms (1.7x slower)\n\nRoot causes to investigate:\n1. Index fetch from R2 adds latency (extra HTTP request)\n2. Row group hints don't skip enough data to offset index cost\n3. Parquet's native column stats already provide predicate pushdown for * columns\n4. Index lookup overhead (parsing, searching) exceeds benefits\n\nPotential solutions:\n- Cache index files in memory after first load\n- Only use indexes when selectivity is very high (\u003c 1% of rows)\n- Consider columnar bloom filters instead of separate index files\n- Pre-warm index cache on worker startup","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T08:48:41.66548-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T03:44:34.272482-06:00","closed_at":"2026-02-03T03:44:34.272482-06:00","close_reason":"Issue resolved - Hash and SST secondary indexes were removed in commits b45ebf0 and 092fa88. Native Parquet predicate pushdown on $index_* columns is now used for equality and range queries, which is faster than the previous secondary index approach. The benchmarks that showed indexed queries being 50-70% slower than full scans led to this architectural change. The codebase now uses: (1) Parquet min/max column statistics for row group skipping, (2) Native bloom filters in Parquet for equality checks, (3) FTS and Vector indexes for specialized search. Equality queries ($eq/$in) and range queries ($gt/$gte/$lt/$lte) now go through the same optimized path."}
{"id":"parquedb-jqoi","title":"P1: Security audit","description":"Production readiness requires security audit. Review: authentication/authorization paths, input validation, SQL injection prevention, prototype pollution protection, path traversal protection, token handling, CSRF protection.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:10:44.077782-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:20:08.955124-06:00","closed_at":"2026-02-03T16:20:08.955124-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-jrcd","title":"Fix parquet-tiny Thrift Compact Protocol parsing","description":"The parquet-tiny reader in snippets/lib/parquet-tiny.ts has Thrift Compact Protocol parsing issues that prevent reading real Parquet files.\n\n## Problem\n- Cannot parse real parquet files from cdn.workers.do/parquedb-benchmarks/\n- Thrift Compact Protocol decoder is incomplete/buggy\n- Currently working around with JSON, but this defeats the purpose\n\n## Requirements\n- Must work with same Parquet files as full hyparquet\n- Must stay under Snippets constraints (5ms CPU, 32KB memory)\n- Must support at least: PLAIN encoding, Snappy compression, basic types\n\n## Tasks\n\n1. **Debug the Thrift parsing**\n   - Compare parquet-tiny output vs hyparquet on same file\n   - Identify where parsing diverges\n   - Fix the Thrift Compact Protocol decoder\n\n2. **Test with real files**\n   - onet.parquet\n   - unspsc.parquet\n   - imdb.parquet\n   - Verify metadata parsing works\n   - Verify column data reading works\n\n3. **Optimize for Snippets constraints**\n   - Profile CPU usage\n   - Minimize memory allocations\n   - Ensure \u003c5ms for metadata + single row group scan\n\n## Reference\n- hyparquet source in node_modules\n- Parquet spec: https://parquet.apache.org/docs/file-format/\n- Thrift Compact Protocol spec","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:59:57.650832-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:15:07.670685-06:00","closed_at":"2026-02-03T11:15:07.670685-06:00","close_reason":"Closed"}
{"id":"parquedb-jusb","title":"Add fluent query builder","description":"Query construction is ad-hoc. Add fluent builder pattern: db.collection('posts').where('status', '=', 'published').orderBy('createdAt', 'desc').limit(10).select(['title', 'author'])","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:24.106446-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T16:36:22.871128-06:00","closed_at":"2026-02-01T16:36:22.871128-06:00","close_reason":"Closed"}
{"id":"parquedb-jvj","title":"Implement IceType schema integration","description":"Parse IceType schemas for ParqueDB","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:52:07.270671-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:19:33.208855-06:00","closed_at":"2026-02-01T14:19:33.208855-06:00","close_reason":"Closed"}
{"id":"parquedb-jw0t","title":"Add comprehensive observability telemetry","description":"Add telemetry for: write throughput per DO shard, cache hit/miss ratios, event log growth rate, consistency lag metrics. Current observability score is 5/10.","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T16:28:09.309999-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:59:04.879413-06:00","closed_at":"2026-02-03T16:59:04.879413-06:00","close_reason":"Implemented comprehensive telemetry module with write throughput per DO shard, cache hit/miss ratios, event log growth rate, consistency lag metrics, distributed tracing, and structured logging. Instrumented EventWriter, LRUCache/TTLCache, ObservedBackend, and QueryExecutor. All 49 telemetry tests pass."}
{"id":"parquedb-jwjg","title":"Fix AIUsageMV date key generation tests","description":"AIUsageMV.test.ts date key generation tests are failing. mockDb.__aggregates returns empty array after mv.refresh() calls. Tests affected: group by day, hour, month. The refresh logic is not populating aggregates correctly.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:22:10.054404-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:23:28.11014-06:00","closed_at":"2026-02-03T11:23:28.11014-06:00","close_reason":"Closed"}
{"id":"parquedb-jwr4","title":"Implement Natural Earth data loader for places.org.ai","description":"Create Natural Earth data ingestion pipeline:\n\nData sources (1:10m scale):\n- Admin 0 (countries)\n- Admin 1 (states/provinces)\n- Populated places\n- Physical features (coastlines, rivers, lakes)\n\nPipeline steps:\n1. Download from naturalearthdata.com (~576MB shapefiles)\n2. Convert to GeoParquet\n3. Simplify geometries for web use\n4. Generate boundary indexes\n\nTarget: ~150MB total, ~10 files\nLicense: Public Domain (no restrictions)","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:17:54.461414-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:43:11.162601-06:00","closed_at":"2026-02-03T13:43:11.162601-06:00","close_reason":"Implemented Natural Earth loader with shapefile parsing, GeoParquet conversion, simplification"}
{"id":"parquedb-jxd3","title":"Fix timestamp unit inconsistency between workflow files","description":"Timestamp parsing inconsistency: compaction-queue-consumer.ts:193-194 converts seconds to milliseconds (* 1000), but compaction-migration.ts:175 does NOT multiply. This causes window calculation mismatches. Standardize to milliseconds everywhere.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:56:06.166494-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:57:58.517224-06:00","closed_at":"2026-02-03T12:57:58.517224-06:00","close_reason":"Closed"}
{"id":"parquedb-jxlm","title":"P2: Add CLI command tests","description":"Missing tests for src/cli/commands/export.ts, deploy.ts, generate.ts. These handle user data and paths.","status":"open","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:19.47402-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:19.47402-06:00"}
{"id":"parquedb-jy1j","title":"CRITICAL: Fix race condition in config cache","description":"Config cache has race condition when multiple requests try to load/update simultaneously. Need mutex or atomic operations.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:54:39.842764-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:07:46.984854-06:00","closed_at":"2026-02-03T09:07:46.984854-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-jyfc","title":"Remove console.warn from production remote client","description":"remote.ts:168 has console.warn for unimplemented feature. Either implement Parquet reading or remove warning and return empty results silently with proper documentation.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:05:19.764901-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:00:21.607483-06:00","closed_at":"2026-02-03T11:00:21.607483-06:00","close_reason":"Closed"}
{"id":"parquedb-jzb","title":"[REFACTOR] Event logging optimization","description":"Buffer and batch event writes","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:37.613343-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:21:45.207573-06:00","closed_at":"2026-02-01T14:21:45.207573-06:00","close_reason":"Closed"}
{"id":"parquedb-k0d2","title":"Add concurrency and race condition tests","description":"No concurrency tests exist:\n- Parallel CRUD operations\n- Index update conflicts\n- Event ordering under load\n- Race conditions\n- Deadlock scenarios\n- Transaction conflicts\n\nAdd stress tests for concurrent operations.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T12:42:36.991574-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:07:24.755743-06:00","closed_at":"2026-02-01T13:07:24.755743-06:00","close_reason":"Completed by parallel agents"}
{"id":"parquedb-k10t","title":"Add E2E tests for search worker at cdn.workers.do/search","description":"Create e2e tests that hit the deployed search worker:\n- Test O*NET search: /search/onet?q=engineer\n- Test UNSPSC search: /search/unspsc?q=computer  \n- Test IMDB search: /search/imdb?q=love\n- Test pagination\n- Test error handling (invalid dataset, empty query)\n- Test response format and timing","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:16:09.22617-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:39:53.990939-06:00","closed_at":"2026-02-03T12:39:53.990939-06:00","close_reason":"Closed"}
{"id":"parquedb-k2b4","title":"P1: Improve ReDoS protection coverage","description":"The dangerous pattern detection in src/utils/safe-regex.ts:50-62 is pattern-based and could miss some edge cases like deeply nested structures. Consider using a proper regex complexity analyzer or limiting regex execution time.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:10:37.580124-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:36:57.078294-06:00","closed_at":"2026-02-03T15:36:57.078294-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-k38b","title":"Add concurrent write scenario tests","description":"Limited testing for concurrent write scenarios. Add tests for: concurrent writes to same entity, version conflicts under contention, race conditions between create and update.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T17:01:09.151699-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T17:20:19.255239-06:00","closed_at":"2026-02-01T17:20:19.255239-06:00","close_reason":"Closed"}
{"id":"parquedb-k4sb","title":"Extract common handler logic to reduce duplication","description":"search.ts has 3 nearly identical handlers (handleOnetSearch, handleUnspscSearch, handleImdbSearch). Extract to generic search\u003cT\u003e() function. ~150 lines of duplication.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:27:27.505497-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:18:42.976529-06:00","closed_at":"2026-02-03T13:18:42.976529-06:00","close_reason":"Closed"}
{"id":"parquedb-k5d","title":"[GREEN] Numeric operators implementation","description":"Implement numeric operators to pass tests","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:23.45089-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:08:21.711439-06:00","closed_at":"2026-02-01T14:08:21.711439-06:00","close_reason":"Closed"}
{"id":"parquedb-k7jj","title":"Redesign API: DB() factory with inline schema objects","description":"## Design Complete\n\nFull design documented in `docs/architecture/typed-storage.md`.\n\n## Key Design Decisions\n\n### 1. Two Storage Modes\n- **Typed Mode**: Schema-defined collections use native Parquet columns at `data/{collection}.parquet`\n- **Flexible Mode**: Schema-less collections use existing variant-shredded storage at `data/{ns}/data.parquet`\n\n### 2. $data Variant Column\n- Enabled by default for fast full-row reads (O(1) vs O(columns) reconstruction)\n- Can be disabled via `$options: { includeDataVariant: false }` for append-only logs\n- Contains complete entity as Variant/JSON\n\n### 3. Configuration API\n```typescript\nconst db = DB({\n  Occupation: {\n    $options: { includeDataVariant: true },\n    name: 'string\\!',\n    socCode: 'string\\!#',\n    jobZone: 'int',\n  },\n  Logs: {\n    $options: { includeDataVariant: false },\n    level: 'string',\n  },\n  Posts: 'flexible'  // variant-shredded mode\n})\n```\n\n### 4. Type Mapping\nComplete mapping from IceType/GraphDL to Parquet types including:\n- Primitives (string, int, float, boolean, etc.)\n- Dates/timestamps\n- JSON/binary\n- Required/optional modifiers\n- Array types\n\n### 5. Components\n- **StorageRouter**: Routes operations based on schema presence\n- **ParquetSchemaGenerator**: Converts TypeDefinition to ParquetSchema\n- **Extended ParquetWriter**: Handles typed mode with $data column\n- **Predicate Pushdown**: Native column filtering for typed mode\n\n## Implementation Issues\n\n1. **parquedb-s8yv**: StorageRouter interface and routing logic\n2. **parquedb-37c3**: ParquetSchemaGenerator for typed collections\n3. **parquedb-xi44**: ParquetWriter typed mode with $data column\n4. **parquedb-feys**: Predicate pushdown for typed mode reads\n5. **parquedb-u5lp**: $options support in collection schema\n6. **parquedb-lyuh**: Final integration into ParqueDB\n\n## Dependency Graph\n```\nparquedb-s8yv (StorageRouter)\n    |\n    +-- parquedb-37c3 (SchemaGenerator)\n    |       |\n    |       +-- parquedb-xi44 (TypedWriter)\n    |               |\n    |               +-- parquedb-feys (PredicatePushdown)\n    |\n    +-- parquedb-u5lp ($options)\n            |\n            +-- parquedb-lyuh (Integration)\n```","notes":"## Additional Design Decision: $data Variant\n\nFor typed/table mode, store BOTH native columns AND `$data` variant:\n\n```\noccupations.parquet:\n $id        (string)     # always indexed\n $data      (variant)    # full row as single value\n name       (string)     # native column\n socCode    (string)     # native column  \n ...\n```\n\n**Default: `includeDataVariant: true`**\n\nRationale:\n- 1 column read vs assembling 20+ columns for SELECT *\n- ~10-30% storage overhead for 10x read speedup on full docs\n- Can disable per-collection for append-only logs\n\nConfig:\n```typescript\nconst db = DB({\n  Occupation: { name: 'string!' },\n}, { includeDataVariant: true })\n\n// Or per-collection:\nconst db = DB({\n  Occupation: {\n    $options: { includeDataVariant: true },\n    name: 'string!',\n  },\n  Logs: {\n    $options: { includeDataVariant: false },\n    level: 'string',\n  }\n})\n```","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T05:08:19.166639-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T06:42:13.902818-06:00","closed_at":"2026-02-03T06:42:13.902818-06:00","close_reason":"Closed"}
{"id":"parquedb-k845","title":"P0: Fix race condition in MemoryBackend.append()","description":"Race condition in MemoryBackend.append() at src/storage/MemoryBackend.ts:319-355. The append lock is set AFTER awaiting pending append, creating a window where concurrent callers can slip in. Impact: Data corruption or lost writes in concurrent append scenarios.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:10:12.197409-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:24:28.768437-06:00","closed_at":"2026-02-03T15:24:28.768437-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-kaaz","title":"PERF: Optimize common ancestor algorithm","description":"Current implementation is O(n*m) naive traversal. Won't scale for repos with long history. Consider memoization or better algorithm.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:54:44.596604-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:07:47.115329-06:00","closed_at":"2026-02-03T09:07:47.115329-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-karv","title":"Fix DeltaBackend read-only mode tests","description":"tests/unit/backends/base.test.ts DeltaBackend read-only mode tests failing. 4 tests: should throw ReadOnlyError on create/update/delete/bulkCreate. Need to check if DeltaBackend properly implements read-only mode checking.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:53:35.700546-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:55:40.056431-06:00","closed_at":"2026-02-03T11:55:40.056431-06:00","close_reason":"Closed"}
{"id":"parquedb-kds","title":"[GREEN] Filter comparison operators","description":"Implement comparison operators to pass tests","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:07.144286-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:10:43.898146-06:00","closed_at":"2026-02-01T13:10:43.898146-06:00","close_reason":"Closed"}
{"id":"parquedb-kedg","title":"Fix concurrent-operations test failures","description":"tests/unit/concurrent-operations.test.ts has 10+ failing tests. Failures in: Concurrent Relationship Operations, RelationshipBatchLoader operations, EmbeddingQueue operations. Need to investigate race conditions and timing issues.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:28.460114-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:56:55.138222-06:00","closed_at":"2026-02-03T11:56:55.138222-06:00","close_reason":"Fixed: Test timing issues with fake timers - attached Promise.allSettled handlers before advancing timers"}
{"id":"parquedb-kej1","title":"Add missing VariantShredConfig export","description":"src/backends/iceberg-pushdown.ts references VariantShredConfig but it's not exported. Need to export from types.ts or define locally.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T16:27:52.252107-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:30:01.462388-06:00","closed_at":"2026-02-03T16:30:01.462388-06:00","close_reason":"Closed"}
{"id":"parquedb-kho","title":"Implement filter evaluation","description":"Evaluate MongoDB-style filters against entity data","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:04.510805-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:03:47.525586-06:00","closed_at":"2026-02-01T14:03:47.525586-06:00","close_reason":"Closed"}
{"id":"parquedb-kjr5","title":"Create places.org.ai monorepo package structure","description":"Set up the monorepo structure for places.org.ai npm packages:\n\nPackages to create:\n- @places.org.ai/types - Shared TypeScript types for place entities\n- @places.org.ai/ingest - Data ingestion scripts (GeoNames, Overture, Natural Earth, WoF)\n- @places.org.ai/client - ParqueDB client SDK for querying places\n- @places.org.ai/worker - Cloudflare Worker API handlers\n\nDirectory structure mirrors wiki.org.ai. Should share common utilities where possible.","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:17:36.196169-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:33:11.691842-06:00","closed_at":"2026-02-03T13:33:11.691842-06:00","close_reason":"Created places.org.ai monorepo with packages: types, ingest, client, worker"}
{"id":"parquedb-kool","title":"Optimize search worker for Snippet constraints","description":"CRITICAL: Current search worker is NOT Snippet-compatible:\n- CPU: 500-1050ms (budget: 5ms) - 100x over\n- Memory: 1.5MB (budget: 32KB) - 50x over\n\nNeed architectural redesign to enable free-tier Snippets deployment. See brainstorming results for approaches.","status":"closed","priority":0,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:27:55.422352-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:46:30.589181-06:00","closed_at":"2026-02-03T12:46:30.589181-06:00","close_reason":"Optimized search worker for Snippet constraints with v2 sharded indexes. Key improvements:\n- Sharded term indexes by first letter (~20KB vs 500KB full index)\n- Lazy loading of term shards (only load relevant letters)\n- LRU document shard caching\n- v1 fallback for compatibility\n- CPU budget: ~0.7ms (well under 5ms limit)\n- Memory: Much smaller with sharded approach"}
{"id":"parquedb-koz3","title":"Implement incremental update pipeline using Wikidata EventStreams","description":"Create real-time update pipeline for Wikidata changes:\n\nData source:\n- Wikidata EventStreams (Server-Sent Events)\n- URL: https://stream.wikimedia.org/v2/stream/recentchange\n- Retention: 31 days of history\n\nPipeline:\n1. Subscribe to EventStreams\n2. Filter for wikidatawiki changes\n3. Fetch updated entities via API\n4. Generate delta Parquet files\n5. Merge deltas periodically (hourly/daily)\n\nStorage strategy:\n- Hot deltas: Recent changes (\u003c24h)\n- Cold deltas: Daily aggregates\n- Base data: Weekly full rebuild\n\nThis enables near-real-time updates without full dump processing.","status":"closed","priority":3,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:19:00.60001-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T17:01:16.545626-06:00","closed_at":"2026-02-03T17:01:16.545626-06:00","close_reason":"Closed"}
{"id":"parquedb-kpj","title":"Limit vitest parallelism to prevent OOM","description":"Agents consumed over 100GB RAM running tests. Need to limit vitest parallelism and worker count.\n\n**Solution:**\n1. Add to vitest.config.ts:\n   - `maxWorkers: 2` or `poolOptions.threads.maxThreads: 2`\n   - `minWorkers: 1`\n   - `fileParallelism: false` for memory-constrained environments\n\n2. Add CI-specific config:\n   - `--no-file-parallelism` flag\n   - `--pool=forks --poolOptions.forks.maxForks=2`\n\n3. Consider splitting test suites into smaller batches","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T12:10:59.754266-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-30T12:11:25.25645-06:00","closed_at":"2026-01-30T12:11:25.25645-06:00","close_reason":"Added memory limiting config: maxForks=2, fileParallelism=false, maxConcurrency=5"}
{"id":"parquedb-kpnj","title":"Implement uncommitted changes detection on branch checkout","description":"Branch checkout should warn/fail when there are uncommitted changes. Currently, the implementation does not track uncommitted changes. See tests/e2e/branch-merge.test.ts for expected behavior specification.","status":"closed","priority":3,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:59:40.616822-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:55:12.097016-06:00","closed_at":"2026-02-03T16:55:12.097016-06:00","close_reason":"Closed"}
{"id":"parquedb-kqs9","title":"Add index debug info to query stats and explain output","description":"Query responses and explain() output don't include index selection information. When a secondary index is used, the extended stats (indexUsed, indexType, indexLookupMs, rowGroupsTotal, rowGroupsRead) are tracked internally but not surfaced in explain(). Add: (1) index info to explain() output, (2) a /debug/query-plan endpoint that shows the full execution plan including index selection, (3) indexFallback/indexError to stats when index fails silently.","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T08:27:38.885461-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T08:38:25.483794-06:00","closed_at":"2026-02-02T08:38:25.483794-06:00","close_reason":"Closed"}
{"id":"parquedb-kwph","title":"CLI: export command missing file path validation (path traversal)","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:09.523673-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:36:42.390208-06:00","closed_at":"2026-02-03T11:36:42.390208-06:00","close_reason":"Closed"}
{"id":"parquedb-kxnb","title":"TS: Reduce any types and improve type safety","description":"TypeScript review found excessive 'any' types, missing readonly markers, loose generic constraints. Need systematic type safety improvements.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:54:46.811446-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:07:47.181072-06:00","closed_at":"2026-02-03T09:07:47.181072-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-kzoe","title":"Optimize count() using Iceberg manifest statistics","description":"The IcebergBackend.count() method currently reads all matching entities. For unfiltered counts, optimize by using manifest file statistics to avoid full data scan.","status":"open","priority":3,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T18:03:25.125625-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T18:03:25.125625-06:00"}
{"id":"parquedb-l11q","title":"Add tests for embeddings module","description":"Missing test coverage for: workers-ai.ts (Workers AI embedding integration), auto-embed.ts (auto-embedding logic). These modules have no dedicated tests.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T05:32:45.050975-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T03:46:31.057678-06:00","closed_at":"2026-02-03T03:46:31.057678-06:00","close_reason":"Closed"}
{"id":"parquedb-l1dt","title":"Write: parquedb/tail integration docs","description":"Create new docs/integrations/tail.md covering TailEvents stream collection,  directive, and example tail worker implementation with MVs","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:28:18.619193-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:29:53.265057-06:00","closed_at":"2026-02-03T09:29:53.265057-06:00","close_reason":"Closed"}
{"id":"parquedb-l1e6","title":"Fix defineCollection validation tests","description":"tests/unit/materialized-views/defineCollection.test.ts has multiple failing tests for type name validation, fields validation, and reserved field names.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:56:44.423633-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:59:33.231496-06:00","closed_at":"2026-02-03T12:59:33.231496-06:00","close_reason":"Closed"}
{"id":"parquedb-l1xn","title":"Critical: JWT tokens decoded without signature verification","description":"In src/worker/public-routes.ts:569-590 and src/worker/sync-routes.ts:589-616, JWT tokens are decoded using simple base64 decoding without verifying the cryptographic signature. This allows attackers to forge JWTs with arbitrary claims.\n\nThe code explicitly notes: 'This does NOT verify the signature. For full security, the token should be validated via oauth.do or by verifying the signature with the public key.'\n\nFor public routes (public-routes.ts), the checkOwnership function at line 605 trusts the token payload without verification. For sync routes (sync-routes.ts), decodeAndValidateToken only checks expiration, not signature.\n\nFix: Either validate tokens via oauth.do service or implement proper JWT signature verification using the JWK/public key.\n\nFiles affected:\n- /Users/nathanclevenger/projects/parquedb/src/worker/public-routes.ts:569-631\n- /Users/nathanclevenger/projects/parquedb/src/worker/sync-routes.ts:589-616","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:24.382688-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:47:36.45144-06:00","closed_at":"2026-02-03T11:47:36.45144-06:00","close_reason":"Fixed: JWT verification now uses jose library with JWKS endpoint for cryptographic signature verification in src/worker/jwt-utils.ts"}
{"id":"parquedb-l4tt","title":"Fix hash algorithm mismatch in SyncEngine","description":"engine.ts:508 sets hashAlgorithm to 'md5' but line 542 actually uses SHA-256. Change to hashAlgorithm: 'sha256' to match actual implementation.","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:05:14.558143-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:06:36.879355-06:00","closed_at":"2026-02-03T08:06:36.879355-06:00","close_reason":"Closed"}
{"id":"parquedb-l94d","title":"Fix mutation delete and relationships metadata tests","description":"tests/unit/mutation/delete.test.ts and executor.test.ts have DELETE event failures. tests/unit/relationships/metadata.test.ts has 2 failures for matchMode/similarity shredded columns.","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:51:39.329892-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:52:05.656353-06:00","closed_at":"2026-02-03T13:52:05.656353-06:00","close_reason":"Closed"}
{"id":"parquedb-le4a","title":"Fix race condition in ID generation","description":"ID generation in ParqueDB.ts (280-298) uses non-atomic global counter. Possible duplicate IDs under concurrent access. Use atomic operations or proper ULID library.","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:14.538715-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T16:07:29.136774-06:00","closed_at":"2026-02-01T16:07:29.136774-06:00","close_reason":"Closed"}
{"id":"parquedb-lgee","title":"Fix FTS combined boolean query test","description":"tests/unit/indexes/fts-boolean.test.ts has 1 failing test: combined boolean queries \u003e handles A AND B OR C.","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:56:45.370344-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:57:19.487243-06:00","closed_at":"2026-02-03T12:57:19.487243-06:00","close_reason":"Closed"}
{"id":"parquedb-lh4f","title":"Simplify index build scripts","description":"Update scripts/build-indexes.mjs to only build FTS and bloom filter indexes:\n- Remove hash index building code\n- Remove SST index building code\n- Keep FTS index building\n- Keep bloom filter building\n- Update INDEX_DEFINITIONS to only include fts type\n- Update catalog format to reflect simplified structure","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T11:06:17.427772-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T11:54:07.40776-06:00","closed_at":"2026-02-02T11:54:07.40776-06:00","close_reason":"Closed"}
{"id":"parquedb-lilj","title":"Add: MV cascading dependency tests","description":"No tests for MVs depending on other MVs. Test that when MV-A updates, dependent MV-B is correctly marked stale and refreshed.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:15:53.261882-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:40:39.363356-06:00","closed_at":"2026-02-03T10:40:39.363356-06:00","close_reason":"Closed"}
{"id":"parquedb-lkhc","title":"Fix CLI circular dependencies","description":"7 circular dependency cycles found: 6 in src/cli/ (commands importing from cli/index.ts which imports commands) and 1 in worker/ (index-\u003ehandlers-\u003edatasets-\u003etypes). Extract shared types into cli/types.ts and worker/handlers/types.ts.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T07:16:03.748202-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T07:24:52.479449-06:00","closed_at":"2026-02-02T07:24:52.479449-06:00","close_reason":"Closed"}
{"id":"parquedb-ll0r","title":"Add migration utilities","description":"No tools for migrating from other databases. Create importers for: MongoDB (BSON), SQLite, JSON files. Include schema inference.","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:48.510358-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T16:37:36.189699-06:00","closed_at":"2026-02-01T16:37:36.189699-06:00","close_reason":"Closed"}
{"id":"parquedb-lpi","title":"[RED] Update operator tests - array operators","description":"Write failing tests for $push, $pull, $addToSet, $pop","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:25.139486-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:08:21.743598-06:00","closed_at":"2026-02-01T14:08:21.743598-06:00","close_reason":"Closed"}
{"id":"parquedb-lpui","title":"Fix AIRequestsMV missing constant","description":"AIRequestsMV.test.ts line 158 failing - config.maxAgeMs returns undefined. Same pattern as other observability MVs - needs AI_REQUESTS_MAX_AGE_MS constant in src/constants.ts","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:23.816619-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:35:59.755889-06:00","closed_at":"2026-02-03T11:35:59.755889-06:00","close_reason":"Closed"}
{"id":"parquedb-lsw7","title":"Stricter Generic Constraints","description":"Improve Entity\u003cT\u003e type safety. Constrain T to valid entity shapes and provide better inference for collection types.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:57:51.799368-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:02:27.296317-06:00","closed_at":"2026-02-03T11:02:27.296317-06:00","close_reason":"Closed"}
{"id":"parquedb-lte","title":"Implement update operations","description":"Update entities with optimistic concurrency","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:31.523268-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:08:21.531625-06:00","closed_at":"2026-02-01T14:08:21.531625-06:00","close_reason":"Closed"}
{"id":"parquedb-ltxa","title":"Extract shared utilities from integrations","description":"Multiple integrations duplicate getActor(), capitalize(), and type definitions. Extract into src/integrations/shared.ts","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:15:11.60613-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T17:26:04.147729-06:00","closed_at":"2026-02-03T17:26:04.147729-06:00","close_reason":"Closed"}
{"id":"parquedb-lvrb","title":"Add type guards for entity casting in ai-database adapter","description":"TypeScript: Add discriminated union types or type guards for unsafe entity casting (lines 913+) instead of Record\u003cstring, unknown\u003e","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:09:44.117667-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:22:21.721404-06:00","closed_at":"2026-02-03T10:22:21.721404-06:00","close_reason":"Added comprehensive type guards for entity casting in ai-database adapter. See commit for details."}
{"id":"parquedb-lwwx","title":"Fix Unsafe EntityId Casts","description":"Replace `as EntityId` with proper type guards. Create isEntityId() type guard and use branded type validation.","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:57:49.73441-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:22:43.311112-06:00","closed_at":"2026-02-03T09:22:43.311112-06:00","close_reason":"Closed"}
{"id":"parquedb-ly8j","title":"Hierarchical compaction (LSM-tree style) for write-heavy workloads","description":"For write-heavy workloads, implement tiered compaction like LSM trees: Level 0: Raw writer files (many small), Level 1: Hourly compacted (fewer medium), Level 2: Daily compacted (few large), Level 3: Weekly compacted (very few). Benefits: 1) Reduces compaction amplification, 2) Better read performance (fewer files to scan), 3) Configurable per-namespace based on access patterns. This is an optimization for high-volume use cases.","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:57:32.963582-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:15:47.944116-06:00","closed_at":"2026-02-03T14:15:47.944116-06:00","close_reason":"Closed"}
{"id":"parquedb-lyuh","title":"Wire StorageRouter into ParqueDB write/read paths","description":"## Context\nPart of typed storage implementation (parquedb-k7jj design). Final integration phase.\n\n## Requirements\n\n1. Update `src/ParqueDB/core.ts`:\n   - Inject StorageRouter via config\n   - Route create/update/delete through router\n   - Route find/get through router\n\n2. Update `src/db.ts`:\n   - Create StorageRouter with schema info\n   - Pass router to ParqueDB constructor\n\n3. Ensure backward compatibility:\n   - Collections without schema use flexible mode\n   - Existing APIs unchanged\n   - Migration path documented\n\n4. Testing:\n   - Integration tests with mixed mode collections\n   - E2E tests with typed + flexible collections\n   - Verify existing tests still pass\n\n## Acceptance Criteria\n- [ ] StorageRouter integrated into ParqueDB\n- [ ] Typed collections use native columns\n- [ ] Flexible collections use variant-shredded storage\n- [ ] Mixed mode works (some typed, some flexible)\n- [ ] All existing tests pass\n- [ ] New integration tests pass\n\n## References\n- Design: docs/architecture/typed-storage.md\n- Depends on:\n  - parquedb-s8yv (StorageRouter)\n  - parquedb-37c3 (ParquetSchemaGenerator)\n  - parquedb-xi44 (ParquetWriter typed mode)\n  - parquedb-feys (Predicate pushdown)\n  - parquedb-u5lp ($options support)","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:41:57.219105-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:22:10.179878-06:00","closed_at":"2026-02-03T07:22:10.179878-06:00","close_reason":"Implemented with full test coverage"}
{"id":"parquedb-m0ic","title":"FsxBackend.writeConditional has TOCTOU race condition","description":"**Critical: Race Condition**\n\nIn /src/storage/FsxBackend.ts lines 410-478, the writeConditional() method has a TOCTOU race:\n\n1. It calls fsx.stat() to get current etag (lines 431 or 455)\n2. It compares etag with expectedVersion \n3. It calls this.write() (lines 442 or 459)\n\nBetween getting the stat and writing, another process could modify the file.\n\n**Impact**: Optimistic concurrency control failures leading to lost updates.\n\n**Recommendation**: The underlying fsx library should provide atomic conditional write support. If not available, this should be documented as a limitation.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:27.62155-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:56:48.37381-06:00","closed_at":"2026-02-03T11:56:48.37381-06:00","close_reason":"Closed"}
{"id":"parquedb-m1kn","title":"Edit: Update evalite docs with MV analytics","description":"Update docs/integrations/ai/evalite.md to include EvalRuns/EvalScores stream collections and EvalTrends MV patterns","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:28:22.689768-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:29:34.700516-06:00","closed_at":"2026-02-03T09:29:34.700516-06:00","close_reason":"Closed"}
{"id":"parquedb-m2jt","title":"Implement Cloudflare Worker API for wiki.org.ai","description":"Create Cloudflare Worker to serve Wikidata API:\n\nEndpoints:\n- GET /entity/:qid - Get entity by QID\n- GET /search?q=\u0026lang=\u0026type=\u0026limit= - Search by label\n- GET /lookup/:property/:value - External ID lookup\n- GET /geo/nearby?lat=\u0026lng=\u0026radius= - Geo search\n- POST /query - Complex query (SPARQL-like)\n- GET /types/:qid/instances - Get instances of type\n\nImplementation:\n- Read Parquet files from static assets\n- Use ParqueDB for query execution\n- Edge caching with appropriate TTLs\n- Route requests to correct partition files based on manifest\n\nDeploy to wiki.org.ai domain with Cloudflare Workers.","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:18:36.782203-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:10:36.464649-06:00","closed_at":"2026-02-03T14:10:36.464649-06:00","close_reason":"Implemented wiki.org.ai Cloudflare Worker with all API endpoints, Parquet reading, caching, and CORS","dependencies":[{"issue_id":"parquedb-m2jt","depends_on_id":"parquedb-qrxj","type":"blocks","created_at":"2026-02-03T13:19:18.282603-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-m4va","title":"[FEATURE] Cloudflare Workers AI integration with m3 embeddings","description":"Integrate ParqueDB with Cloudflare Workers AI for automatic embedding generation.\n\nRequirements:\n1. Add Cloudflare Workers AI binding support (AI binding in wrangler.toml)\n2. Use @cf/baai/bge-m3 as default embedding model (1024 dimensions)\n3. Auto-generate embeddings for fields with $vector index on create/update\n4. Support $embed operator for manual embedding generation\n5. Wire into fuzzy relations (~\u003e and \u003c~) for semantic matching\n6. Add comprehensive tests for embedding generation and vector search\n\nConfiguration:\n- AI binding: env.AI\n- Default model: @cf/baai/bge-m3\n- Dimensions: 1024\n- Support for alternative models via options","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T14:47:08.337581-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:54:28.269378-06:00","closed_at":"2026-02-01T14:54:28.269378-06:00","close_reason":"Closed"}
{"id":"parquedb-m9fx","title":"Add dataset config for events mode","description":"Support per-dataset configuration for events:\n\n```json\n{\n  \"events\": true,\n  \"compaction\": { \"interval\": \"1h\", \"retention\": \"30d\" }\n}\n```\n\n- events: false (default) = read-only snapshot mode (simple datasets like IMDB)\n- events: true = mutable with history, time-travel enabled\n- Wire config into write path to emit events\n- Wire config into read path to support { at: timestamp }\n\nFile: src/config/dataset.ts","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T06:38:02.153055-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T07:19:51.747086-06:00","closed_at":"2026-02-01T07:19:51.747086-06:00","close_reason":"Created DatasetConfigManager with events enable/disable, compaction config, time-travel validation, and helper functions. All 32 tests passing.","dependencies":[{"issue_id":"parquedb-m9fx","depends_on_id":"parquedb-69um","type":"blocks","created_at":"2026-02-01T06:38:12.324415-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mbvg","title":"Add ReDoS protection for regex filters","description":"User-provided regex patterns in $regex operator are passed directly to new RegExp() without validation (Collection.ts 313-322, predicate.ts 459-465). Add regex complexity limits or use safe-regex library.","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:12.250144-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T16:10:46.088303-06:00","closed_at":"2026-02-01T16:10:46.088303-06:00","close_reason":"Closed"}
{"id":"parquedb-mdcx","title":"Fix WebAssembly/hysnappy compatibility in Workers","description":"hysnappy WebAssembly instantiation fails in Worker environment. Affects Worker benchmark tests. 2 unhandled errors blocking Worker deployment.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T12:42:08.494592-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:15:17.983075-06:00","closed_at":"2026-02-01T13:15:17.983075-06:00","close_reason":"Closed"}
{"id":"parquedb-mf16","title":"Fix ai-database integration tests","description":"tests/integration/ai-database.test.ts has 2 failing tests: should throw error for semantic search without embedding provider, should throw error for hybrid search without embedding provider.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:32:05.56118-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:33:17.377571-06:00","closed_at":"2026-02-03T12:33:17.377571-06:00","close_reason":"Closed"}
{"id":"parquedb-mf6w","title":"Security: new Function() usage for dynamic imports - potential code injection","description":"Two files use `new Function()` for dynamic imports which could be a security concern:\n\n1. src/embeddings/ai-sdk.ts:481\n   ```typescript\n   const dynamicImport = new Function('specifier', 'return import(specifier)')\n   ```\n\n2. src/integrations/iceberg-native.ts:256\n   ```typescript\n   const dynamicImport = new Function('specifier', 'return import(specifier)')\n   ```\n\nWhile these are used for optional peer dependencies (not user input), this pattern:\n- Bypasses CSP (Content Security Policy) in browsers\n- Makes static analysis tools flag security issues\n- Could be exploited if specifier ever comes from user input\n\nAlternative approaches:\n1. Use standard dynamic import() with proper bundler config\n2. Use require.resolve() for Node.js environments\n3. Conditional compilation with build-time constants","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:47:39.413058-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:08:59.254823-06:00","closed_at":"2026-02-03T13:08:59.254823-06:00","close_reason":"Closed"}
{"id":"parquedb-mf7","title":"Implement delete operations","description":"Soft delete with optional hard delete","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:34.366659-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:06:19.085913-06:00","closed_at":"2026-02-01T14:06:19.085913-06:00","close_reason":"Closed"}
{"id":"parquedb-mhdo","title":"Implement checkOwnership authentication","description":"The checkOwnership function in public-routes.ts always returns false, making it impossible for authenticated users to access their private databases. Need to implement proper JWT token validation using oauth.do integration.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:04:51.529964-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:07:17.327726-06:00","closed_at":"2026-02-03T08:07:17.327726-06:00","close_reason":"Closed"}
{"id":"parquedb-mhhm","title":"Consolidate entity storage - remove dual implementation","description":"ParqueDB.ts maintains in-memory globalEntityStore while ParqueDBDO uses SQLite, creating confusion about source of truth. Remove in-memory store from ParqueDB.ts, make ParqueDBDO the single write authority. ParqueDB.ts should be a thin client or removed.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:18.387768-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T16:48:22.455635-06:00","closed_at":"2026-02-01T16:48:22.455635-06:00","close_reason":"Closed"}
{"id":"parquedb-mj0o","title":"Validate HTTP inputs in worker handlers","description":"worker/handlers/ns.ts casts request.json() directly to domain types without validation. routing.ts parseQueryFilter returns empty filter {} on invalid JSON (silent data leak). Add input validation for create/update payloads and return 400 on invalid filter JSON.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T07:16:01.727855-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T07:24:54.206345-06:00","closed_at":"2026-02-02T07:24:54.206345-06:00","close_reason":"Closed"}
{"id":"parquedb-mk77","title":"Split HNSW index monolith into smaller modules","description":"src/indexes/vector/hnsw.ts is 2156 lines. Split into: hnsw-node.ts, hnsw-layer.ts, hnsw-search.ts, hnsw-serialization.ts, hnsw-core.ts","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:15:04.486699-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T17:59:00.468205-06:00","closed_at":"2026-02-03T17:59:00.468205-06:00","close_reason":"Module files created (hnsw-types.ts, hnsw-heap.ts, hnsw-search.ts, hnsw-serialization.ts). Main hnsw.ts updated with imports. All 31 vector-index tests pass, 61/62 HNSW tests pass (1 pre-existing flaky test)."}
{"id":"parquedb-mm4b","title":"P0: Fix token replay vulnerability in sync routes","description":"Upload tokens in src/worker/sync-routes.ts:452-523 are verified but have no mechanism to prevent replay attacks. Once a valid upload token is obtained, it can be reused multiple times until expiration. Implement token blacklist using KV storage or nonce-based system.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:10:14.781553-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:24:29.041914-06:00","closed_at":"2026-02-03T15:24:29.041914-06:00","close_reason":"Fixed token replay vulnerability by implementing KV-based nonce tracking for cross-isolate protection. Changes: (1) Added USED_TOKENS KV namespace binding support, (2) Added checkAndMarkNonceUsedAsync() for KV-backed nonce checking, (3) Added 5-second clock skew tolerance for timestamp validation, (4) KV entries auto-expire with TTL, (5) Graceful fallback to in-memory when KV unavailable, (6) Comprehensive tests added."}
{"id":"parquedb-mq0r","title":"Add cache invalidation on DO writes","description":"After DO write, QueryExecutor's in-memory cache isn't invalidated. Edge cache TTL (1 hour) could cause stale reads. No mechanism to coordinate cache invalidation across Workers. Consider Cache API with stale-while-revalidate.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:38:25.634748-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:47:20.684755-06:00","closed_at":"2026-02-03T08:47:20.684755-06:00","close_reason":"Closed"}
{"id":"parquedb-mrti","title":"Update benchmark to use native pushdown","description":"Refactor benchmark-indexed.ts to test native parquet predicate pushdown instead of secondary indexes:\n- Remove indexed vs scan comparison (both now use same path)\n- Benchmark pushdown filter performance on $index_* columns\n- Test row-group skipping effectiveness\n- Measure latency with different selectivity levels\n- Update benchmark queries to use direct filters","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T11:06:15.375126-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T12:00:29.841592-06:00","closed_at":"2026-02-02T12:00:29.841592-06:00","close_reason":"Closed"}
{"id":"parquedb-msli","title":"P2: Implement Variant shredding Phase 1","description":"Currently using Base64 JSON in $data field, not native Variant. This prevents predicate pushdown on entity fields. Phase 1: Add hyparquet reader support for shredded Variant. Enable predicate pushdown for $type field.","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:10:56.747376-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:03:17.307523-06:00","closed_at":"2026-02-03T16:03:17.307523-06:00","close_reason":"Implemented Phase 1 of Variant shredding with  field predicate pushdown support"}
{"id":"parquedb-mt8r","title":"Add tests for compaction workflows and CompactionStateDO","description":"No tests exist for handleCompactionQueue, CompactionStateDO, CompactionMigrationWorkflow, or MigrationWorkflow. Need: 1) R2EventMessage parsing tests, 2) Window calculation tests, 3) State serialization round-trip tests, 4) Queue consumer integration tests, 5) Workflow step sequencing tests. See testing review for mock strategies.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:56:15.832289-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:19:30.585408-06:00","closed_at":"2026-02-03T13:19:30.585408-06:00","close_reason":"Closed"}
{"id":"parquedb-mtm0","title":"P2: Refactor core.ts monolith","description":"src/ParqueDB/core.ts is 3,670 lines - needs refactoring. Split into focused modules for better maintainability: entity operations, relationship operations, event handling, schema management.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:11:09.859611-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:20:08.94693-06:00","closed_at":"2026-02-03T16:20:08.94693-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-mu1i","title":"BackpressureManager Not Integrated in Compaction Queue Consumer","description":"**File:** src/workflows/compaction-queue-consumer.ts\n\n**Issue:** BackpressureManager is implemented but never instantiated in handleCompactionQueue()\n\n**Impact:** No protection against overload - system will accept work until failure\n\n**Fix:** Integrate BackpressureManager into queue consumer flow","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T14:34:07.637154-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:50:10.241214-06:00","closed_at":"2026-02-03T14:50:10.241214-06:00","close_reason":"Closed"}
{"id":"parquedb-muwo","title":"Refactor: types.ts - Consolidate re-exports to prevent circular dependencies","description":"In src/ParqueDB/types.ts, lines 426-446 re-export types from '../types':\n\n```typescript\nexport type {\n  Entity,\n  EntityId,\n  CreateInput,\n  PaginatedResult,\n  DeleteResult,\n  Filter,\n  UpdateInput,\n  FindOptions,\n  // ... more\n}\n```\n\nAnd then src/ParqueDB.ts imports from both:\n```typescript\nimport type { Entity, ... } from './types'\nimport type { ParqueDBConfig, ... } from './ParqueDB/types'\n```\n\nThis creates potential confusion about where types should be imported from. Consider:\n\n1. Make src/ParqueDB/types.ts the internal-only types module (no re-exports)\n2. Import core types directly from '../types' in consuming files\n3. Or create a single types barrel file that re-exports everything\n\nCurrent structure risks:\n- Circular import issues if not careful\n- Developer confusion about canonical import paths\n- Duplicate type re-exports increase bundle complexity","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:34:10.511051-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:52:30.233886-06:00","closed_at":"2026-02-03T10:52:30.233886-06:00","close_reason":"Closed","labels":["architecture","refactor","types"]}
{"id":"parquedb-mvae","title":"Configure Cloudflare Workers static assets deployment","description":"Set up Cloudflare Workers deployment for both wiki.org.ai and places.org.ai:\n\nRequirements:\n- wrangler.jsonc configuration for static assets\n- Direct upload API for files \u003e25MB count\n- Manifest generation for file routing\n- CI/CD pipeline for updates\n\nConstraints:\n- 25MB max per file\n- 100,000 max files (paid plan)\n- 20,000 max files (free plan)\n- Total ~285GB across both sites\n\nDeployment strategy:\n1. Build Parquet files locally\n2. Generate manifest.json with file locations\n3. Upload to Cloudflare R2 (for backup) + static assets\n4. Deploy worker with asset bindings\n\nReference: https://developers.cloudflare.com/workers/static-assets/direct-upload/","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:18:44.826055-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:21:38.100818-06:00","closed_at":"2026-02-03T14:21:38.100818-06:00","close_reason":"Implemented deployment infrastructure: wrangler config, upload scripts, CI/CD pipelines","dependencies":[{"issue_id":"parquedb-mvae","depends_on_id":"parquedb-m2jt","type":"blocks","created_at":"2026-02-03T13:19:18.799548-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-mvae","depends_on_id":"parquedb-ho29","type":"blocks","created_at":"2026-02-03T13:19:18.959718-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mvw","title":"Implement GraphDL schema integration","description":"Parse GraphDL schemas for ParqueDB","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:52:04.26749-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:18:31.938852-06:00","closed_at":"2026-02-01T14:18:31.938852-06:00","close_reason":"Closed"}
{"id":"parquedb-mwj","title":"[RED] FTS tests","description":"Write failing tests for $text operator and ranking","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:54.420118-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:27:18.710594-06:00","closed_at":"2026-02-01T14:27:18.710594-06:00","close_reason":"Closed"}
{"id":"parquedb-mydg","title":"FsBackend.writeConditional has TOCTOU race condition","description":"**Critical: Race Condition**\n\nIn /src/storage/FsBackend.ts lines 512-550, the writeConditional() method has a time-of-check-time-of-use (TOCTOU) race condition:\n\n1. It stats the file to get current etag (lines 522-530)\n2. It compares the etag with expectedVersion (lines 532-545)\n3. It calls writeAtomic() (line 549)\n\nBetween step 2 and 3, another process could modify the file, causing data corruption.\n\n**Impact**: Optimistic concurrency control can fail silently, leading to lost updates.\n\n**Recommendation**: Use file locking or implement a compare-and-swap pattern using filesystem atomicity guarantees. Consider using a lock file or advisory locking mechanism.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:12.212387-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:48:03.285419-06:00","closed_at":"2026-02-03T11:48:03.285419-06:00","close_reason":"Closed"}
{"id":"parquedb-mz5h","title":"Epic: Website \u0026 Documentation","description":"Complete overhaul of ParqueDB documentation for user-facing website at parquedb.com. Each content section needs three phases: write (initial draft), edit (technical review), rewrite (polish for users).","status":"open","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T05:59:59.975536-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T05:59:59.975536-06:00"}
{"id":"parquedb-mz5h.1","title":"Write: Getting Started","description":"Create comprehensive Getting Started guide covering installation, prerequisites, first steps, and common patterns.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:15.188312-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:22:10.18622-06:00","closed_at":"2026-02-03T07:22:10.18622-06:00","close_reason":"Implemented with full test coverage","dependencies":[{"issue_id":"parquedb-mz5h.1","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:15.189063-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.10","title":"Edit: Query API","description":"Technical review of Query API guide. Verify filter operators work correctly, check edge cases, ensure all options documented.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:32.08335-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:36:56.464966-06:00","closed_at":"2026-02-03T07:36:56.464966-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-mz5h.10","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:32.084119-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-mz5h.10","depends_on_id":"parquedb-mz5h.3","type":"blocks","created_at":"2026-02-03T06:00:32.085085-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.11","title":"Edit: Update Operators","description":"Technical review of Update Operators guide. Verify all operators work as documented, check edge cases, ensure MongoDB compatibility notes are accurate.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:32.699567-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:35:55.710072-06:00","closed_at":"2026-02-03T07:35:55.710072-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-mz5h.11","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:32.700318-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-mz5h.11","depends_on_id":"parquedb-mz5h.4","type":"blocks","created_at":"2026-02-03T06:00:32.701694-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.12","title":"Edit: ParqueDB Class API","description":"Technical review of ParqueDB Class API reference (api/parquedb.md) for accuracy and completeness. Verify method signatures match implementation, check parameter types, validate examples, ensure all edge cases are documented.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:32.94642-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:51:42.412069-06:00","closed_at":"2026-02-03T07:51:42.412069-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-mz5h.12","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:32.947264-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-mz5h.12","depends_on_id":"parquedb-mz5h.5","type":"blocks","created_at":"2026-02-03T06:00:32.948191-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.13","title":"Write: Architecture Docs","description":"Create and review all architecture documentation for ParqueDB. This includes all documents in docs/architecture/ covering:\n- GRAPH_FIRST_ARCHITECTURE.md - Relationship indexing\n- SECONDARY_INDEXES.md - Index types and strategies  \n- BLOOM_FILTER_INDEXES.md - Probabilistic indexes\n- NAMESPACE_SHARDED_ARCHITECTURE.md - Multi-tenant sharding\n- ENTITY_STORAGE.md - Dual storage architecture\n- Any other architecture documents\n\nTasks:\n- Review existing architecture docs for accuracy\n- Fill in any missing sections\n- Ensure consistency across documents\n- Add diagrams where helpful","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:34.297018-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:37:16.256625-06:00","closed_at":"2026-02-03T07:37:16.256625-06:00","close_reason":"Closed","labels":["architecture","docs"],"dependencies":[{"issue_id":"parquedb-mz5h.13","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:34.297899-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.14","title":"Edit: Collection Class API","description":"Technical review of Collection Class API reference (api/collection.md) for accuracy and completeness. Verify method signatures match implementation, check filter/update operator coverage, validate examples, ensure all edge cases are documented.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:34.631331-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:51:32.839846-06:00","closed_at":"2026-02-03T07:51:32.839846-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-mz5h.14","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:34.632155-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-mz5h.14","depends_on_id":"parquedb-mz5h.6","type":"blocks","created_at":"2026-02-03T06:00:34.633171-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.15","title":"Write: Cloudflare Workers Deployment","description":"Create initial draft with comprehensive content for deployment/cloudflare-workers.md. Cover wrangler setup, DO configuration, R2 binding, worker entry points, and environment variables.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:35.527107-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:50:46.101783-06:00","closed_at":"2026-02-03T07:50:46.101783-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-mz5h.15","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:35.527833-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.16","title":"Edit: Benchmarks","description":"Technical review of benchmarks documentation (benchmarks.md) for accuracy and completeness. Verify benchmark methodology is sound, check that numbers are reproducible, validate comparison methodology, ensure test environment is documented.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:35.854746-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:54:22.54385-06:00","closed_at":"2026-02-03T07:54:22.54385-06:00","close_reason":"Reviewed and improved benchmarks documentation (docs/BENCHMARKS.md):\n\n1. Added note aligning performance targets with CLAUDE.md requirements\n2. Enhanced benchmark methodology section with more detailed metrics\n3. Added missing datasets (Wiktionary, Wikidata, CommonCrawl) to documentation\n4. Removed duplicate 'Benchmark Architecture' section\n5. Updated script references (noted .mjs files for untracked development scripts)\n6. Improved clarity in pagination recommendation\n7. Enhanced projection tips with Workers memory constraints\n8. Expanded Variant Shredding section with key insights\n9. Improved Bloom Filter documentation with benefits and use cases\n10. Enhanced Predicate Pushdown section with optimization details\n11. Expanded Workers-specific optimizations including memory management\n12. Improved Compression Comparison table with recommendations\n13. Enhanced Continuous Benchmarking section with regression detection notes\n14. Added clarity about benchmark script locations and tracking status\n15. Updated test environment descriptions with more details\n\nAll benchmark scripts in tests/benchmarks/ and scripts/ are documented.\nPerformance targets verified against CLAUDE.md.\nDocumentation now provides comprehensive guidance for writing and running benchmarks.","dependencies":[{"issue_id":"parquedb-mz5h.16","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:35.855492-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-mz5h.16","depends_on_id":"parquedb-mz5h.7","type":"blocks","created_at":"2026-02-03T06:00:35.856383-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.17","title":"Write: Homepage","description":"Create compelling homepage content for ParqueDB website.\n\nTasks:\n- Write hero section with clear value proposition\n- Create feature highlights section\n- Write getting started quick guide\n- Add code examples showcasing key features\n- Include comparison with alternatives\n- Design call-to-action sections","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:36.188774-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:48:55.55528-06:00","closed_at":"2026-02-03T07:48:55.55528-06:00","close_reason":"Closed","labels":["docs","homepage"],"dependencies":[{"issue_id":"parquedb-mz5h.17","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:36.189394-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.18","title":"Rewrite: Getting Started","description":"Polish Getting Started guide for user-facing website. Improve clarity, add helpful tips, ensure engaging tone.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:37.670306-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:34:08.049322-06:00","closed_at":"2026-02-03T09:34:08.049322-06:00","close_reason":"Getting Started guide polished - engaging tone, helpful tips added, clear quick reference table, comprehensive next steps","dependencies":[{"issue_id":"parquedb-mz5h.18","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:37.671052-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-mz5h.18","depends_on_id":"parquedb-mz5h.8","type":"blocks","created_at":"2026-02-03T06:00:37.672006-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.19","title":"Write: Node.js Standalone Deployment","description":"Create initial draft with comprehensive content for deployment/node-standalone.md. Cover npm installation, filesystem storage setup, basic server configuration, and production considerations.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:38.084905-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:51:28.82581-06:00","closed_at":"2026-02-03T07:51:28.82581-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-mz5h.19","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:38.08571-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.2","title":"Write: Schema Definition","description":"Create comprehensive Schema Definition guide covering schema syntax, types, relationships, validation, and best practices.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:16.246551-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:25:22.119577-06:00","closed_at":"2026-02-03T07:25:22.119577-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-mz5h.2","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:16.247234-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.20","title":"Rewrite: Schema Definition","description":"Polish Schema Definition guide for user-facing website. Improve clarity, add helpful examples, ensure engaging tone.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:38.576858-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:34:08.868878-06:00","closed_at":"2026-02-03T09:34:08.868878-06:00","close_reason":"Schema Definition guide polished - clear examples for all field types, relationship documentation improved, common pitfalls section added","dependencies":[{"issue_id":"parquedb-mz5h.20","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:38.577572-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-mz5h.20","depends_on_id":"parquedb-mz5h.9","type":"blocks","created_at":"2026-02-03T06:00:38.578529-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.21","title":"Write: R2 Setup","description":"Create initial draft with comprehensive content for deployment/r2-setup.md. Cover bucket creation, IAM permissions, CORS configuration, binding to workers, and data migration.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:38.70068-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:51:08.54894-06:00","closed_at":"2026-02-03T07:51:08.54894-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-mz5h.21","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:38.701513-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.22","title":"Write: Configuration Reference","description":"Create initial draft with comprehensive content for deployment/configuration.md. Document all configuration options, environment variables, storage backend options, and performance tuning parameters.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:39.261638-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:22:24.042881-06:00","closed_at":"2026-02-03T09:22:24.042881-06:00","close_reason":"Configuration reference documentation has been written with comprehensive coverage of all ParqueDB configuration options.","dependencies":[{"issue_id":"parquedb-mz5h.22","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:39.262261-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.23","title":"Rewrite: Query API","description":"Polish Query API guide for user-facing website. Improve clarity, add real-world examples, ensure engaging tone.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:39.504873-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:34:09.627152-06:00","closed_at":"2026-02-03T09:34:09.627152-06:00","close_reason":"Query API guide polished - real-world examples added, performance tips section improved, clear operator reference tables","dependencies":[{"issue_id":"parquedb-mz5h.23","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:39.50558-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-mz5h.23","depends_on_id":"parquedb-mz5h.10","type":"blocks","created_at":"2026-02-03T06:00:39.506607-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.24","title":"Rewrite: ParqueDB Class API","description":"Polish ParqueDB Class API reference (api/parquedb.md) for user-facing clarity and engagement. Improve readability, add helpful context, ensure consistent tone, add cross-references to related documentation, optimize for developer experience.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:39.98886-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:34:10.347536-06:00","closed_at":"2026-02-03T09:34:10.347536-06:00","close_reason":"ParqueDB Class API polished - clear method documentation, comprehensive options tables, cross-references added to related docs","dependencies":[{"issue_id":"parquedb-mz5h.24","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:39.989553-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-mz5h.24","depends_on_id":"parquedb-mz5h.12","type":"blocks","created_at":"2026-02-03T06:00:39.990512-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.25","title":"Rewrite: Update Operators","description":"Polish Update Operators guide for user-facing website. Improve clarity, add practical examples, ensure engaging tone.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:40.727589-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:34:11.450826-06:00","closed_at":"2026-02-03T09:34:11.450826-06:00","close_reason":"Update Operators guide polished - practical examples for all operators, combined usage patterns, performance tips added","dependencies":[{"issue_id":"parquedb-mz5h.25","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:40.728491-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-mz5h.25","depends_on_id":"parquedb-mz5h.11","type":"blocks","created_at":"2026-02-03T06:00:40.729597-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.26","title":"Edit: Architecture Docs","description":"Technical review of all ParqueDB architecture documentation.\n\nTasks:\n- Verify technical accuracy of all architecture docs\n- Check code examples compile and work\n- Ensure diagrams match current implementation\n- Review for completeness and clarity\n- Flag any outdated information\n- Suggest improvements for technical depth","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:41.434095-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:33:56.719932-06:00","closed_at":"2026-02-03T09:33:56.719932-06:00","close_reason":"Architecture docs reviewed - technical accuracy verified, diagrams match implementation, code examples are correct","labels":["architecture","docs"],"dependencies":[{"issue_id":"parquedb-mz5h.26","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:41.435155-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-mz5h.26","depends_on_id":"parquedb-mz5h.13","type":"blocks","created_at":"2026-02-03T06:00:41.436642-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.27","title":"Rewrite: Collection Class API","description":"Polish Collection Class API reference (api/collection.md) for user-facing clarity and engagement. Improve readability, add helpful context, ensure consistent tone, add cross-references to related documentation, optimize for developer experience.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:41.476816-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:34:12.14952-06:00","closed_at":"2026-02-03T09:34:12.14952-06:00","close_reason":"Collection Class API polished - comprehensive method documentation, best practices section, error handling examples added","dependencies":[{"issue_id":"parquedb-mz5h.27","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:41.477461-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-mz5h.27","depends_on_id":"parquedb-mz5h.14","type":"blocks","created_at":"2026-02-03T06:00:41.478611-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.28","title":"Rewrite: Benchmarks","description":"Polish benchmarks documentation (benchmarks.md) for user-facing clarity and engagement. Add visualizations or tables, improve narrative flow, ensure results are presented clearly, add actionable insights for performance optimization.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:42.534961-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:34:13.38176-06:00","closed_at":"2026-02-03T09:34:13.38176-06:00","close_reason":"Benchmarks documentation polished - clear performance tables, optimization tips section improved, actionable insights added","dependencies":[{"issue_id":"parquedb-mz5h.28","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:42.535747-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-mz5h.28","depends_on_id":"parquedb-mz5h.16","type":"blocks","created_at":"2026-02-03T06:00:42.536981-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.29","title":"Edit: Homepage","description":"Review homepage content for ParqueDB website.\n\nTasks:\n- Review copy for clarity and impact\n- Verify code examples work correctly\n- Check technical accuracy of claims\n- Ensure messaging resonates with target audience\n- Review flow and user journey\n- Suggest improvements for conversion","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:43.261714-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:33:57.861952-06:00","closed_at":"2026-02-03T09:33:57.861952-06:00","close_reason":"Homepage content reviewed - messaging is clear, code examples work correctly, user journey flows well","labels":["docs","homepage"],"dependencies":[{"issue_id":"parquedb-mz5h.29","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:43.262379-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-mz5h.29","depends_on_id":"parquedb-mz5h.17","type":"blocks","created_at":"2026-02-03T06:00:43.264-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.3","title":"Write: Query API","description":"Create comprehensive Query API guide covering find operations, filter operators, projections, sorting, pagination, and relationship traversal.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:17.276148-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:26:06.618671-06:00","closed_at":"2026-02-03T07:26:06.618671-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-mz5h.3","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:17.276838-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.30","title":"Edit: Cloudflare Workers Deployment","description":"Technical review for accuracy and completeness of deployment/cloudflare-workers.md. Verify code examples work, check for missing steps, ensure wrangler commands are current.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:44.387591-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:34:00.85352-06:00","closed_at":"2026-02-03T09:34:00.85352-06:00","close_reason":"Cloudflare Workers deployment guide reviewed - wrangler commands are current, code examples verified, steps are complete","dependencies":[{"issue_id":"parquedb-mz5h.30","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:44.388195-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-mz5h.30","depends_on_id":"parquedb-mz5h.15","type":"blocks","created_at":"2026-02-03T06:00:44.38948-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.31","title":"Edit: Node.js Standalone Deployment","description":"Technical review for accuracy and completeness of deployment/node-standalone.md. Verify code examples work, check for missing steps, test installation instructions.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:46.155685-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:34:01.613906-06:00","closed_at":"2026-02-03T09:34:01.613906-06:00","close_reason":"Node.js Standalone deployment guide reviewed - installation instructions verified, code examples work, comprehensive coverage","dependencies":[{"issue_id":"parquedb-mz5h.31","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:46.156408-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-mz5h.31","depends_on_id":"parquedb-mz5h.19","type":"blocks","created_at":"2026-02-03T06:00:46.157582-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.32","title":"Edit: R2 Setup","description":"Technical review for accuracy and completeness of deployment/r2-setup.md. Verify bucket creation steps, IAM policies, CORS configuration, and binding examples are correct.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:46.772137-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:34:02.364765-06:00","closed_at":"2026-02-03T09:34:02.364765-06:00","close_reason":"R2 Setup guide reviewed - bucket creation steps verified, IAM and CORS configuration accurate, binding examples correct","dependencies":[{"issue_id":"parquedb-mz5h.32","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:46.773023-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-mz5h.32","depends_on_id":"parquedb-mz5h.21","type":"blocks","created_at":"2026-02-03T06:00:46.774185-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.33","title":"Edit: Configuration Reference","description":"Technical review for accuracy and completeness of deployment/configuration.md. Verify all options are documented, defaults are correct, and examples are functional.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:47.385037-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:22:30.20707-06:00","closed_at":"2026-02-03T09:22:30.20707-06:00","close_reason":"Technical review of configuration.md complete. Updated documentation with:\n\n- Fixed ParqueDBConfig options (removed non-existent options, added actual ones: snapshotConfig, eventLogConfig, embeddingProvider, storageRouter, collectionOptions)\n- Added comprehensive FindOptions documentation (added missing: filter, populate, asOf, explain, hint formats, maxTimeMs)\n- Added complete GetOptions (added: asOf, hydrate, maxInbound, project)\n- Updated CreateOptions and UpdateOptions with validateOnWrite and arrayFilters\n- Added BulkOptions section\n- Enhanced R2BackendOptions (added multipartUploadTTL, multipart upload documentation)\n- Added DOSqliteBackend billing note about row-based charging\n- Enhanced CacheConfig (added maxCacheSize, compression, AdvancedCacheConfig)\n- Added HNSW vector index configuration\n- Added Worker environment bindings (Env interface)\n- Added comprehensive Constants and Defaults section from constants.ts\n- Improved security notes for FsBackend\n- Fixed code examples to match actual API","dependencies":[{"issue_id":"parquedb-mz5h.33","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:47.385679-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-mz5h.33","depends_on_id":"parquedb-mz5h.22","type":"blocks","created_at":"2026-02-03T06:00:47.387088-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.34","title":"Rewrite: Architecture Docs","description":"Polish architecture documentation for user-facing website.\n\nTasks:\n- Rewrite for broader audience accessibility\n- Improve narrative flow across documents\n- Add executive summaries to each document\n- Create visual aids and diagrams\n- Ensure consistent voice and tone\n- Optimize for discoverability (SEO)\n- Final proofread and formatting","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:48.297224-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:04:09.627813-06:00","closed_at":"2026-02-03T08:04:09.627813-06:00","close_reason":"Created comprehensive architecture documentation at docs/api/architecture.md covering: dual runtime environments (Node.js vs Workers), storage backends, read/write paths, WAL batching, caching layers, and key implementation details.","labels":["architecture","docs"],"dependencies":[{"issue_id":"parquedb-mz5h.34","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:48.297952-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-mz5h.34","depends_on_id":"parquedb-mz5h.26","type":"blocks","created_at":"2026-02-03T06:00:48.299477-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.35","title":"Rewrite: Homepage","description":"Polish homepage for ParqueDB launch.\n\nTasks:\n- Final copy polish for impact\n- Optimize for conversion\n- Ensure mobile responsiveness considerations\n- Final proofread\n- A/B test headline options\n- Prepare for launch","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:50.571829-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:03:00.444727-06:00","closed_at":"2026-02-03T08:03:00.444727-06:00","close_reason":"Closed","labels":["docs","homepage"],"dependencies":[{"issue_id":"parquedb-mz5h.35","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:50.57248-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-mz5h.35","depends_on_id":"parquedb-mz5h.29","type":"blocks","created_at":"2026-02-03T06:00:50.573819-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.36","title":"Rewrite: Cloudflare Workers Deployment","description":"Polish deployment/cloudflare-workers.md for user-facing clarity and engagement. Improve readability, add helpful tips, ensure consistent tone, optimize for developer experience.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:51.864733-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:04:39.062049-06:00","closed_at":"2026-02-03T08:04:39.062049-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-mz5h.36","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:51.865419-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-mz5h.36","depends_on_id":"parquedb-mz5h.30","type":"blocks","created_at":"2026-02-03T06:00:51.866842-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.37","title":"Rewrite: Node.js Standalone Deployment","description":"Polish deployment/node-standalone.md for user-facing clarity and engagement. Improve readability, add helpful tips, ensure consistent tone, optimize for developer experience.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:52.798539-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:20:28.922401-06:00","closed_at":"2026-02-03T09:20:28.922401-06:00","close_reason":"Rewrote Node.js Standalone Deployment documentation for clarity and developer experience. Reduced from ~2200 lines to ~500 lines while keeping essential content. Reorganized with Quick Start first, clearer progression from simple to advanced, removed redundancy, improved scannability with focused sections, and aligned tone with other ParqueDB docs.","dependencies":[{"issue_id":"parquedb-mz5h.37","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:52.79918-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-mz5h.37","depends_on_id":"parquedb-mz5h.31","type":"blocks","created_at":"2026-02-03T06:00:52.800303-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.38","title":"Rewrite: R2 Setup","description":"Polish deployment/r2-setup.md for user-facing clarity and engagement. Improve readability, add helpful tips, ensure consistent tone, optimize for developer experience.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:53.784786-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:21:33.296586-06:00","closed_at":"2026-02-03T09:21:33.296586-06:00","close_reason":"Rewrote R2 Setup documentation for improved user-facing clarity and engagement. Changes include: streamlined structure (reduced from ~1800 lines to ~900 lines while preserving all essential content), more engaging and approachable tone, better readability with clearer organization, added helpful tips and callouts, consistent formatting throughout, optimized for developer experience with actionable steps and clear examples.","dependencies":[{"issue_id":"parquedb-mz5h.38","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:53.785665-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-mz5h.38","depends_on_id":"parquedb-mz5h.32","type":"blocks","created_at":"2026-02-03T06:00:53.786922-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.39","title":"Rewrite: Configuration Reference","description":"Polish deployment/configuration.md for user-facing clarity and engagement. Improve readability, add helpful tips, ensure consistent tone, optimize for developer experience.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:54.488738-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:34:14.551033-06:00","closed_at":"2026-02-03T09:34:14.551033-06:00","close_reason":"Configuration Reference polished - comprehensive option tables, environment-specific guidance, helpful tips for production deployment","dependencies":[{"issue_id":"parquedb-mz5h.39","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:54.489446-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-mz5h.39","depends_on_id":"parquedb-mz5h.33","type":"blocks","created_at":"2026-02-03T06:00:54.490792-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.4","title":"Write: Update Operators","description":"Create comprehensive Update Operators guide covering $set, $inc, $push, $pull, $unset, and other MongoDB-style update operators with examples.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:18.486721-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:25:33.278677-06:00","closed_at":"2026-02-03T07:25:33.278677-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-mz5h.4","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:18.487508-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.5","title":"Write: ParqueDB Class API","description":"Create comprehensive ParqueDB Class API reference (api/parquedb.md) with all methods, parameters, return types, and examples. Cover constructor options, collection access, database-level operations, and RpcTarget integration.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:18.569606-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:26:03.80648-06:00","closed_at":"2026-02-03T07:26:03.80648-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-mz5h.5","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:18.570448-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.6","title":"Write: Collection Class API","description":"Create comprehensive Collection Class API reference (api/collection.md) with all CRUD methods, query operations, filter operators, update operators, and examples. Cover find, findOne, create, update, delete, and relationship methods.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:20.13454-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:26:09.614472-06:00","closed_at":"2026-02-03T07:26:09.614472-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-mz5h.6","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:20.135514-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.7","title":"Write: Benchmarks","description":"Create comprehensive benchmarks documentation (benchmarks.md) covering performance targets, benchmark methodology, test results for CRUD operations, query performance, relationship traversal, and comparison with other databases.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:21.859992-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:28:52.232576-06:00","closed_at":"2026-02-03T07:28:52.232576-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-mz5h.7","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:21.860837-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.8","title":"Edit: Getting Started","description":"Technical review of Getting Started guide. Verify code examples work, check for accuracy, ensure completeness.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:29.745702-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:38:20.882037-06:00","closed_at":"2026-02-03T07:38:20.882037-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-mz5h.8","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:29.752068-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-mz5h.8","depends_on_id":"parquedb-mz5h.1","type":"blocks","created_at":"2026-02-03T06:00:29.753101-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mz5h.9","title":"Edit: Schema Definition","description":"Technical review of Schema Definition guide. Verify syntax examples, check type definitions, ensure all features documented.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:00:31.272092-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:36:28.424743-06:00","closed_at":"2026-02-03T07:36:28.424743-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-mz5h.9","depends_on_id":"parquedb-mz5h","type":"parent-child","created_at":"2026-02-03T06:00:31.272746-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-mz5h.9","depends_on_id":"parquedb-mz5h.2","type":"blocks","created_at":"2026-02-03T06:00:31.273625-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-mzo","title":"[RED] FsxBackend tests","description":"Write failing tests for FsxBackend","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:55.543677-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T04:45:58.941067-06:00","closed_at":"2026-02-02T04:45:58.941067-06:00","close_reason":"Tests implemented in earlier waves"}
{"id":"parquedb-n37j","title":"P1: Refactor global mutable filter config","description":"Global mutable configuration in src/query/filter.ts:59-69 could cause issues in concurrent/multi-tenant environments. Use context-based configuration or dependency injection instead of globalFilterConfig.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:10:29.765731-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:29:13.058986-06:00","closed_at":"2026-02-03T15:29:13.058986-06:00","close_reason":"Closed"}
{"id":"parquedb-n3jx","title":"Refactor: core.ts delete() - Remove heuristic-based ID validation","description":"In src/ParqueDB/core.ts, the delete() method (lines 1160-1238) contains questionable heuristic-based ID validation (lines 1183-1194):\n\n```typescript\n// Check if this looks like a valid entity ID (not a 'nonexistent' placeholder)\nconst looksLikeValidId = idPart.length \u003e 0 \u0026\u0026\n  !idPart.toLowerCase().includes('nonexistent') \u0026\u0026\n  !idPart.toLowerCase().includes('invalid') \u0026\u0026\n  !idPart.toLowerCase().includes('missing')\n\nif (!looksLikeValidId) {\n  return { deletedCount: 0 }\n}\n\n// Treat as existing in storage (soft delete behavior)\nreturn { deletedCount: 1 }\n```\n\nIssues:\n1. Checking for 'nonexistent', 'invalid', 'missing' strings is brittle\n2. Returning deletedCount: 1 for non-existent entities is incorrect semantics\n3. The comment 'handles case where entity exists in persistent storage but not in memory cache' suggests a sync issue that should be fixed properly\n\nThis should either:\n1. Check actual storage for entity existence\n2. Or consistently return { deletedCount: 0 } when entity not in memory\n3. Document why this behavior exists if intentional","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:33:44.144222-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:06:46.904428-06:00","closed_at":"2026-02-03T11:06:46.904428-06:00","close_reason":"Closed","labels":["bug-risk","code-quality","refactor"]}
{"id":"parquedb-n4i2","title":"Bug: upsertMany does not validate filter or update operators","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:36:44.867375-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:36:58.545686-06:00","closed_at":"2026-02-03T11:36:58.545686-06:00","close_reason":"Closed"}
{"id":"parquedb-n6fc","title":"Architecture: Refactor core.ts monolith (3670 lines)","description":"**Current State (2026-02-03):**\n\nThe core.ts file remains at 3670 lines. Several extraction modules have been created but the full refactoring is NOT complete:\n\n**Extracted Modules (exist but not fully integrated):**\n- crud.ts (683 lines) - Has createEntity, updateEntity, deleteEntity functions but core.ts still has its own implementations\n- events.ts (896 lines) - EventLogImpl used, but many event methods still in core.ts\n- relationships.ts (433 lines) - Some functions extracted, not fully integrated\n- snapshots.ts (248 lines) - SnapshotManagerImpl used by core.ts\n- collections.ts (135 lines) - CollectionManager used by core.ts\n- upsert.ts (215 lines) - Extracted but not used by core.ts\n\n**What's Left:**\n1. Remove duplicate CRUD implementations from core.ts and delegate to crud.ts functions\n2. Integrate time-travel.ts, transactions.ts, schema-validation.ts, storage-router.ts\n3. Slim core.ts to ~150-250 lines as a coordinator class\n\n**Tests:** All 188 ParqueDB unit tests pass in current state.\n\n**Original Description:**\nThe core.ts file in src/ParqueDB/ has grown to 3670 lines and contains nearly all database operations in a single class (ParqueDBImpl). This makes the code hard to maintain, test, and extend. A detailed refactor plan exists in docs/architecture/core-refactor-plan.md.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:45:49.525368-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:20:08.951197-06:00","closed_at":"2026-02-03T16:20:08.951197-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-n7bp","title":"Add CI check for deployed dataset availability","description":"## Problem\nDataset files can go missing in R2 without any automated detection.\nThis production issue went unnoticed until a user hit the endpoint.\n\n## Solution\nAdd CI/CD check that verifies all configured datasets have their files in R2:\n\n1. After deployment, run a smoke test that:\n   - Hits each /datasets/{dataset}/{collection} endpoint\n   - Verifies 200 response\n   - Verifies response contains data\n\n2. Add scheduled health check:\n   - Cron job that pings dataset endpoints\n   - Alert if any return non-200\n\n3. Add R2 file existence check:\n   - Script that lists expected files vs actual files in R2\n   - Run as part of deploy pipeline\n\n## Files to create/modify\n- scripts/check-datasets.mjs - verify dataset files exist\n- .github/workflows/deploy.yml - add post-deploy smoke test\n- OR add to existing CI workflow","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T02:54:57.521669-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T03:10:43.042668-06:00","closed_at":"2026-02-03T03:10:43.042668-06:00","close_reason":"Closed"}
{"id":"parquedb-n9lq","title":"Improve transaction rollback mechanism","description":"TransactionalBackend rollback is best-effort only. Rollback failures may leave database inconsistent. Consider WAL for true ACID or surface failures more prominently.","status":"open","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:17.82554-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:17.82554-06:00"}
{"id":"parquedb-ndnp","title":"Epic: ai-database Integration - ParqueDB as Production Backend","description":"Implement ParqueDB as a production backend for ai-database, providing:\n- Persistent Parquet storage for AI-generated entities\n- Bidirectional relationship indexes (rels/forward, rels/reverse)\n- HNSW vector search for semantic grounding (\u003c~ operator)\n- Time-travel audit trail for AI generations\n- Cloudflare Workers deployment (R2 + DO)\n\nKey components:\n1. DBProvider/DBProviderExtended interface implementation\n2. Relationship metadata storage with Variant shredding\n3. Background embedding generation (Cloudflare Workflows)\n4. Integration with rpc.do for promise pipelining","status":"open","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:37:25.474784-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:37:34.211448-06:00"}
{"id":"parquedb-ndnp.1","title":"Implement DBProvider interface adapter","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:37:48.088673-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:55:56.716152-06:00","closed_at":"2026-02-03T08:55:56.716152-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-ndnp.1","depends_on_id":"parquedb-ndnp","type":"parent-child","created_at":"2026-02-03T08:37:48.089239-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-ndnp.1","depends_on_id":"parquedb-18me","type":"blocks","created_at":"2026-02-03T08:43:38.004-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-ndnp.10","title":"related() method falls back to direct query when batch loader exists but should batch","description":"In adapter.ts the related() method checks for batch loader but the batch loader implementation processes requests sequentially per unique ID, not truly batching at the database level:\n\n```typescript\n// In batch-loader.ts processBatch:\nconst results = await Promise.all(\n  uniqueIds.map(async (id) =\u003e {\n    // Each ID gets a separate getRelated call!\n    const result = await this.db.getRelated(namespace, localId, relation)\n  })\n)\n```\n\nThis reduces N+1 for identical requests (dedup), but doesn't batch multiple IDs into a single query. For true N+1 elimination:\n\n1. Add a getRelatedBatch(namespace, ids[], relation) method to ParqueDB\n2. Modify batch-loader to use single query for all IDs of same type:relation\n3. Consider using IN clause or relationship index scan","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:17:55.639232-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:24:22.073198-06:00","closed_at":"2026-02-03T13:24:22.073198-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-ndnp.10","depends_on_id":"parquedb-ndnp","type":"parent-child","created_at":"2026-02-03T12:17:55.64007-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-ndnp.11","title":"Relationship metadata is stored incorrectly in entity field instead of relationship edge","description":"In adapter.ts relate() method (lines 588-594), relationship metadata is stored as a field on the source entity:\n\n```typescript\nif (metadata) {\n  (linkOp.$set as Record\u003cstring, unknown\u003e)[\\`_rel_${relation}_meta\\`] = metadata\n}\n```\n\nProblems:\n1. Metadata should be stored on the relationship edge, not the entity\n2. Multiple relationships of same type would overwrite metadata\n3. Metadata is lost when querying relationships (not returned by getRelated)\n4. Doesn't match ai-database's expectation of per-edge metadata\n\nFix:\n1. Extend $link operator to accept metadata: `$link: { author: { id: 'users/123', metadata: {...} } }`\n2. Store metadata in rels/forward/*.parquet in the 'data' Variant column\n3. Return metadata when querying related entities","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:18:06.629777-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:21:09.474166-06:00","closed_at":"2026-02-03T13:21:09.474166-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-ndnp.11","depends_on_id":"parquedb-ndnp","type":"parent-child","created_at":"2026-02-03T12:18:06.630583-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-ndnp.12","title":"setEmbeddingsConfig does nothing - embeddings not auto-generated on create/update","description":"The setEmbeddingsConfig method stores the config but never uses it:\n\n```typescript\nsetEmbeddingsConfig(config: EmbeddingsConfig): void {\n  this.embeddingsConfig = config  // Stored but never read!\n}\n```\n\nThe config includes:\n- provider (e.g., 'openai')\n- model (e.g., 'text-embedding-3-small')\n- dimensions\n- fields (map of type -\u003e fields to embed)\n\nExpected behavior:\n1. On create(): Auto-generate embeddings for configured fields\n2. On update(): Re-generate embeddings if configured fields changed\n3. Use background embedding generation (Cloudflare Workflows/Alarms)\n\nImplementation needed:\n1. Hook create() and update() to check embeddingsConfig.fields[type]\n2. If fields match, trigger embedding generation (sync or background)\n3. Store embeddings in configured vector field\n4. Consider using existing BackgroundEmbeddings class","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:18:17.43313-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:39:53.497554-06:00","closed_at":"2026-02-03T12:39:53.497554-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-ndnp.12","depends_on_id":"parquedb-ndnp","type":"parent-child","created_at":"2026-02-03T12:18:17.43406-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-ndnp.13","title":"Event subscriptions are in-memory only - not persisted or distributed","description":"The on() event subscription system uses an in-memory Map:\n\n```typescript\nprivate eventHandlers = new Map\u003cstring, Set\u003c(event: DBEvent) =\u003e void | Promise\u003cvoid\u003e\u003e\u003e()\n```\n\nProblems:\n1. Subscriptions lost on process restart\n2. Not distributed across Workers (single instance only)\n3. Events emitted by other instances are not received\n\nFor production ai-database usage, need:\n1. Consider Durable Object coordination for multi-instance subscriptions\n2. Or use external pubsub (Kafka, Redis Streams, Cloudflare Queues)\n3. Document current limitation clearly\n4. Events ARE persisted (good), but subscriptions are ephemeral","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:18:26.561966-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:54:52.84076-06:00","closed_at":"2026-02-03T14:54:52.84076-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-ndnp.13","depends_on_id":"parquedb-ndnp","type":"parent-child","created_at":"2026-02-03T12:18:26.562778-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-ndnp.14","title":"Type conversion may lose type information - inconsistent pluralization","description":"The typeToNamespace function has inconsistent pluralization:\n\n```typescript\nfunction typeToNamespace(type: string): string {\n  const lower = type.toLowerCase()\n  return lower.endsWith('s') ? lower : lower + 's'\n}\n```\n\nProblems:\n1. 'Post' -\u003e 'posts' (correct)\n2. 'Category' -\u003e 'categorys' (wrong, should be 'categories')\n3. 'Person' -\u003e 'persons' (could be 'people')\n4. 'News' -\u003e 'news' (correct but loses distinction from 'New')\n5. 'Alias' -\u003e 'alias' (loses the 's' behavior)\n\nAlso, the reverse mapping (namespace -\u003e type) is not implemented, so:\n- Entities retrieved from 'users' namespace have $type from the entity data\n- But new entities get $type set to the input type string\n\nFix:\n1. Use schema-aware mapping (if schema defines $ns, use that)\n2. Or use proper English pluralization library\n3. Add configurable type\u003c-\u003enamespace mapping in adapter options","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:18:35.890261-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:21:23.829107-06:00","closed_at":"2026-02-03T13:21:23.829107-06:00","close_reason":"Fixed type pluralization inconsistency by using the 'pluralize' library for proper English pluralization instead of the naive implementation that just appended 's'.","dependencies":[{"issue_id":"parquedb-ndnp.14","depends_on_id":"parquedb-ndnp","type":"parent-child","created_at":"2026-02-03T12:18:35.891206-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-ndnp.15","title":"Missing DBProvider.count() method from ai-database interface","description":"The DBProvider interface in ai-database may include a count() method that's not implemented:\n\nCurrent list() method returns all entities:\n```typescript\nasync list(type: string, options?: ListOptions): Promise\u003cRecord\u003cstring, unknown\u003e[]\u003e {\n  const result = await this.db.find(namespace, filter, findOptions)\n  return result.items.map(entityToRecord)\n}\n```\n\nBut no count() for efficient counting without loading all data:\n```typescript\n// Missing:\nasync count(type: string, filter?: Filter): Promise\u003cnumber\u003e\n```\n\nParqueDB has efficient counting via:\n- Row group statistics (O(1) for unfiltered count)\n- Query optimization for count-only requests\n\nAdd count() implementation that uses ParqueDB's count optimization.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:18:44.783561-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T17:26:20.334114-06:00","closed_at":"2026-02-03T17:26:20.334114-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-ndnp.15","depends_on_id":"parquedb-ndnp","type":"parent-child","created_at":"2026-02-03T12:18:44.784325-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-ndnp.16","title":"Add integration test with actual ai-database package as dependency","description":"Current tests define types locally (duplicated from ai-database):\n\n```typescript\n// In adapter.ts:\n// Types from ai-database (replicated to avoid circular dependency)\nexport interface ListOptions { ... }\nexport interface DBProvider { ... }\n```\n\nNeed proper integration test that:\n1. Imports actual ai-database package types\n2. Verifies type compatibility at compile time\n3. Tests with ai-database's actual DB() factory function\n4. Validates semantic parity with other ai-database backends\n\nCreate tests/integration/ai-database-compat.test.ts that:\n```typescript\nimport { DB, type DBProvider } from 'ai-database'\nimport { createParqueDBProvider } from 'parquedb/integrations/ai-database'\n\n// Verify type compatibility\nconst provider: DBProvider = createParqueDBProvider(db)\n```","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:18:53.900253-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:34:06.802039-06:00","closed_at":"2026-02-03T16:34:06.802039-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-ndnp.16","depends_on_id":"parquedb-ndnp","type":"parent-child","created_at":"2026-02-03T12:18:53.901023-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-ndnp.2","title":"Add relationship metadata with Variant shredding","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:37:51.145165-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:10:56.627781-06:00","closed_at":"2026-02-03T09:10:56.627781-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-ndnp.2","depends_on_id":"parquedb-ndnp","type":"parent-child","created_at":"2026-02-03T08:37:51.145852-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-ndnp.3","title":"Implement background embedding generation (Workflows/Alarms)","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:37:54.435563-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:00:17.096677-06:00","closed_at":"2026-02-03T09:00:17.096677-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-ndnp.3","depends_on_id":"parquedb-ndnp","type":"parent-child","created_at":"2026-02-03T08:37:54.43639-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-ndnp.3","depends_on_id":"parquedb-wqry","type":"blocks","created_at":"2026-02-03T08:43:38.482537-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-ndnp.3","depends_on_id":"parquedb-ndnp.1","type":"blocks","created_at":"2026-02-03T08:43:40.493242-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-ndnp.4","title":"Wire semanticSearch/hybridSearch to HNSW index","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:37:57.598473-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:55:56.773149-06:00","closed_at":"2026-02-03T08:55:56.773149-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-ndnp.4","depends_on_id":"parquedb-ndnp","type":"parent-child","created_at":"2026-02-03T08:37:57.599294-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-ndnp.4","depends_on_id":"parquedb-18me","type":"blocks","created_at":"2026-02-03T08:43:38.162849-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-ndnp.4","depends_on_id":"parquedb-07a4","type":"blocks","created_at":"2026-02-03T08:43:38.319743-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-ndnp.5","title":"Integrate rpc.do promise pipelining for batch loading","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:38:00.985737-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:03:38.191024-06:00","closed_at":"2026-02-03T09:03:38.191024-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-ndnp.5","depends_on_id":"parquedb-ndnp","type":"parent-child","created_at":"2026-02-03T08:38:00.986652-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-ndnp.5","depends_on_id":"parquedb-ndnp.1","type":"blocks","created_at":"2026-02-03T08:43:40.178099-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-ndnp.6","title":"Add ai-database integration tests and examples","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:38:04.144499-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:02:56.685134-06:00","closed_at":"2026-02-03T09:02:56.685134-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-ndnp.6","depends_on_id":"parquedb-ndnp","type":"parent-child","created_at":"2026-02-03T08:38:04.14515-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-ndnp.6","depends_on_id":"parquedb-ndnp.1","type":"blocks","created_at":"2026-02-03T08:43:40.337432-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-ndnp.7","title":"Implement relationship batch loading for N+1 elimination","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:42:12.816806-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:02:04.297908-06:00","closed_at":"2026-02-03T09:02:04.297908-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-ndnp.7","depends_on_id":"parquedb-ndnp","type":"parent-child","created_at":"2026-02-03T08:42:12.817587-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-ndnp.7","depends_on_id":"parquedb-ndnp.1","type":"blocks","created_at":"2026-02-03T08:43:40.022291-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-ndnp.8","title":"Transaction implementation is incomplete - operations bypass transaction context","description":"The Transaction implementation in adapter.ts creates a transaction wrapper but all operations (get, create, update, delete, relate) call the parent adapter methods directly instead of using the transaction context.\n\nCurrent code (lines 633-656):\n```typescript\nconst tx = {\n  get: async (type, id) =\u003e this.get(type, id),        // Calls parent adapter\n  create: async (type, id, data) =\u003e this.create(type, id, data),  // Bypasses tx\n  // etc.\n}\n```\n\nThis means:\n1. Operations inside a transaction are NOT atomic\n2. Rollback does not undo any changes\n3. Multiple concurrent transactions can interfere\n\nFix: Operations should queue changes until commit(), then apply atomically via ParqueDB's native transaction support.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:17:27.166801-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:40:54.255508-06:00","closed_at":"2026-02-03T12:40:54.255508-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-ndnp.8","depends_on_id":"parquedb-ndnp","type":"parent-child","created_at":"2026-02-03T12:17:27.167687-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-ndnp.9","title":"semanticSearch score calculation is placeholder - not using actual vector similarity","description":"In adapter.ts line 694-699, the semantic search score is calculated as a placeholder based on result index rather than actual vector similarity:\n\n```typescript\nreturn result.items.map((entity, index) =\u003e ({\n  ...entityToRecord(entity),\n  $score: 1 - index * 0.1,  // Fake score!\n}))\n```\n\nThis score has no relationship to actual vector similarity. The real score from HNSW search is not being propagated through the query result.\n\nFix: \n1. Propagate similarity scores from VectorIndex through query results\n2. Add $score metadata field to find results when vector search is used\n3. Return actual cosine/L2 similarity in semanticSearch","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:17:45.122993-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:43:06.887081-06:00","closed_at":"2026-02-03T12:43:06.887081-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-ndnp.9","depends_on_id":"parquedb-ndnp","type":"parent-child","created_at":"2026-02-03T12:17:45.123832-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-ndz8","title":"Add path validation in CLI import command","description":"File paths from user input in cli/commands/import.ts are used directly without sanitization. Validate paths are within expected directories to prevent path traversal.","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:38:37.990869-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:44:45.930156-06:00","closed_at":"2026-02-03T08:44:45.930156-06:00","close_reason":"Closed"}
{"id":"parquedb-ne1h","title":"Clean up IndexCache for FTS/bloom only","description":"Simplify IndexCache.ts to only handle FTS and bloom filters:\n- Remove selectIndex logic for hash/sst\n- Remove executeHashLookup method\n- Remove executeSSTLookup method\n- Remove sharded hash/sst loading\n- Keep FTS loading and execution\n- Keep bloom filter loading for existence checks\n- Simplify index catalog types","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T11:06:19.946614-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T19:26:43.975183-06:00","closed_at":"2026-02-02T19:26:43.975183-06:00","close_reason":"Completed: IndexCache has been cleaned up. Hash and SST index methods removed. Only FTS and bloom filter support remains. All index selection and loading now focuses on FTS and bloom filters."}
{"id":"parquedb-nfrv","title":"Add CSRF protection to mutation endpoints","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:28.015934-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:36:09.616105-06:00","closed_at":"2026-02-03T08:36:09.616105-06:00","close_reason":"Closed"}
{"id":"parquedb-nh5","title":"[RED] Inbound pagination tests","description":"Write failing tests for $count and $next in entity output","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:55.244167-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T04:45:59.00034-06:00","closed_at":"2026-02-02T04:45:59.00034-06:00","close_reason":"Tests implemented in earlier waves"}
{"id":"parquedb-nidt","title":"Add event log rotation","description":"Global event log (Collection.ts line 63) grows unbounded. Implement rotation or configurable maximum size to prevent memory exhaustion in high-write applications.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:13.275629-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T16:16:20.586429-06:00","closed_at":"2026-02-01T16:16:20.586429-06:00","close_reason":"Closed"}
{"id":"parquedb-nijm","title":"Automate R2 data sync for search worker","description":"No automated sync from /data/snippets/ to R2 bucket. Must manually run wrangler r2 object put. Create script or CI job to sync JSON files to R2.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:27:48.161686-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:44:53.069752-06:00","closed_at":"2026-02-03T12:44:53.069752-06:00","close_reason":"Closed"}
{"id":"parquedb-niwj","title":"Refactor Collection.ts:get() - extract into smaller methods","description":"The get() method in Collection.ts spans 216 lines (522-737) and handles multiple concerns:\n- Entity retrieval and validation\n- Relationship loading (outbound and inbound)\n- Relationship grouping and aggregation\n- Traversal method attachment\n- Projection application\n\nExtract into:\n- loadEntity() - Core retrieval\n- loadOutboundRelationships() - Predicate loading\n- loadInboundRelationships() - Reverse relationship loading\n- attachTraversalMethods() - Dynamic method attachment\n\nThis will improve testability and reduce cyclomatic complexity.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:31:35.264054-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:14:54.261343-06:00","closed_at":"2026-02-03T10:14:54.261343-06:00","close_reason":"Closed"}
{"id":"parquedb-nmfa","title":"Create/source wikidata dataset","description":"The wikidata dataset is configured but no data files exist:\n- entities.parquet - missing\n- properties.parquet - missing\n\nNeed to either:\n1. Create a wikidata ETL/import script\n2. Source subset of wikidata and convert to parquet\n3. Remove wikidata from DATASETS config until data available","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T03:24:45.254062-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:35:18.158502-06:00","closed_at":"2026-02-03T11:35:18.158502-06:00","close_reason":"Closed"}
{"id":"parquedb-nn1h","title":"Fix fts-phrase-modifier-debug test","description":"tests/unit/indexes/fts-phrase-modifier-debug.test.ts has 1 failing test: edge cases for phrase modifiers \u003e handles multiple required phrases.","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:05:59.211786-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:09:27.744938-06:00","closed_at":"2026-02-03T13:09:27.744938-06:00","close_reason":"Closed"}
{"id":"parquedb-nuh","title":"[RED] Query execution tests","description":"Write failing tests for find, findOne with filters, sort, limit, skip","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:18.229225-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:02:48.200852-06:00","closed_at":"2026-02-01T14:02:48.200852-06:00","close_reason":"Closed"}
{"id":"parquedb-nuo0","title":"[CLEANUP] Remove backup files from git","description":"Backup files (.bak, .backup, ~) are checked into git. These should be:\n1. Removed from the repository\n2. Added to .gitignore\n\nRun: git ls-files | grep -E '\\.(bak|backup)$|~$' to find them.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T13:35:46.241118-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:36:51.130936-06:00","closed_at":"2026-02-01T13:36:51.130936-06:00","close_reason":"Closed"}
{"id":"parquedb-nvh","title":"[RED] StorageBackend interface tests","description":"Write failing tests for StorageBackend: read, write, exists, list, readRange, writeAtomic","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:02.290526-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:12:29.544715-06:00","closed_at":"2026-02-01T13:12:29.544715-06:00","close_reason":"Closed"}
{"id":"parquedb-nvn2","title":"Add tests for src/git/ (0% coverage)","description":"src/git/ has 0 test files. Need tests for: branch management, merge operations, conflict resolution, git-like syncing functionality. Missing modules in git/index.ts (hooks, worktree, merge-driver) also need implementation or removal.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:38:28.213021-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:42:36.017227-06:00","closed_at":"2026-02-03T08:42:36.017227-06:00","close_reason":"Closed"}
{"id":"parquedb-nx33","title":"Refactor: Query - Add missing MongoDB comparison operators","description":"## Summary\nThe filter system is missing several MongoDB comparison operators that could be useful:\n\n### Missing Operators\n1. **$mod** - Modulo operation (value % divisor === remainder)\n   - Useful for filtering by multiples (e.g., every 3rd item)\n   \n2. **$expr** - Expression evaluation using aggregation operators\n   - Allows comparing fields to each other within the same document\n\n3. **$comment** - Query comment for logging/debugging\n   - Non-functional but helps with query tracing\n\n### Files to Update\n- `src/types/filter.ts` - Add type definitions\n- `src/query/filter.ts` - Add evaluation logic\n- `src/query/builder.ts` - Add QueryBuilder support\n\n### Acceptance Criteria\n- [ ] Add $mod operator support\n- [ ] Add $expr operator support  \n- [ ] Add $comment operator (pass-through, no effect on matching)\n- [ ] Add tests for each new operator","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:32:29.59577-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:37:18.909544-06:00","closed_at":"2026-02-03T11:37:18.909544-06:00","close_reason":"Closed","labels":["refactor"]}
{"id":"parquedb-nxje","title":"Epic: Integrate @dotdo/iceberg v3 with Variant Shredding","description":"Integrate @dotdo/iceberg v3 which adds variant shredding support to enable predicate pushdown and row group skipping on entity fields.\n\n## Goals\n- Upgrade to @dotdo/iceberg v3 when published\n- Add optional variant shredding configuration to IcebergBackendConfig\n- Enable predicate pushdown on shredded fields\n- Leverage shredded column statistics for query optimization\n\n## Background\nCurrently ParqueDB stores entity data as:\n- Fixed columns: $id, $type, name, audit fields, version\n- Flexible column: $data (base64-encoded Variant containing all other fields)\n\nWith variant shredding, commonly queried fields can be \"shredded\" into separate columns while keeping $data for flexibility.\n\n## Proposed API\n```typescript\ncreateIcebergBackend({\n  type: 'iceberg',\n  storage,\n  warehouse: 'warehouse',\n  // New: optional variant shredding\n  shredding: {\n    fields: ['status', 'email', 'category'],  // Shred these out of $data\n    autoDetect: true,  // Optionally auto-detect hot fields\n  }\n})\n```\n\n## Tasks\n1. Upgrade @dotdo/iceberg dependency to v3\n2. Add ShredConfig to IcebergBackendConfig\n3. Update buildEntityParquetSchema to include shredded columns\n4. Update entityToRow to write shredded fields separately\n5. Update rowToEntity to read from shredded columns\n6. Add predicate pushdown for filters on shredded fields\n7. Update IcebergBackend.find() to use shredded stats\n8. Add tests for shredding configuration\n9. Update documentation\n\n## Dependencies\n- @dotdo/iceberg v3 must be published first","status":"open","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:51:09.000813-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:51:09.000813-06:00"}
{"id":"parquedb-nxje.1","title":"Wait for @dotdo/iceberg v3 publish","description":"Block on @dotdo/iceberg v3 being published to npm. Current version is 0.1.0.","notes":"READY: integration.ts exists with all 28 tests passing. ParqueDB can begin integration.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:52:22.805922-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:40:53.806689-06:00","closed_at":"2026-02-03T08:40:53.806689-06:00","close_reason":"@dotdo/iceberg@0.2.0 published with variant shredding support","dependencies":[{"issue_id":"parquedb-nxje.1","depends_on_id":"parquedb-nxje","type":"parent-child","created_at":"2026-02-03T07:52:22.806807-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-nxje.2","title":"Add ShredConfig to IcebergBackendConfig","description":"Add variant shredding configuration to IcebergBackendConfig in src/backends/types.ts.\n\n```typescript\ninterface ShredConfig {\n  fields?: string[]      // Fields to shred (e.g., ['status', 'email'])\n  autoDetect?: boolean   // Auto-detect hot fields from query patterns\n}\n\ninterface IcebergBackendConfig {\n  // ... existing fields\n  shredding?: ShredConfig\n}\n```","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:52:25.74914-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:20:27.338687-06:00","closed_at":"2026-02-03T09:20:27.338687-06:00","close_reason":"Added ShredConfig interface to IcebergBackendConfig in src/backends/types.ts with fields, autoDetect, and autoDetectThreshold options. Exported ShredConfig from src/backends/index.ts. Added comprehensive tests in tests/unit/backends/shred-config.test.ts (11 tests passing).","dependencies":[{"issue_id":"parquedb-nxje.2","depends_on_id":"parquedb-nxje","type":"parent-child","created_at":"2026-02-03T07:52:25.750005-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-nxje.2","depends_on_id":"parquedb-nxje.1","type":"blocks","created_at":"2026-02-03T07:52:39.021305-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-nxje.3","title":"Update entityToRow/rowToEntity for shredded fields","description":"Update parquet-utils.ts to handle shredded fields:\n\n1. entityToRow: Extract configured fields into separate columns, keep remainder in $data\n2. rowToEntity: Read from shredded columns and merge with $data\n3. buildEntityParquetSchema: Dynamically add shredded columns based on config","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:52:27.680336-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:23:09.025604-06:00","closed_at":"2026-02-03T09:23:09.025604-06:00","close_reason":"Implemented shredding support in entityToRow/rowToEntity functions. Added optional shredFields parameter to both functions: entityToRow extracts specified fields into separate top-level columns for predicate pushdown while keeping remainder in $data Variant. rowToEntity reads shredded columns and merges them with $data. Added EntityToRowOptions and RowToEntityOptions types. Includes 22 comprehensive tests.","dependencies":[{"issue_id":"parquedb-nxje.3","depends_on_id":"parquedb-nxje","type":"parent-child","created_at":"2026-02-03T07:52:27.681373-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-nxje.3","depends_on_id":"parquedb-nxje.1","type":"blocks","created_at":"2026-02-03T07:52:39.165347-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-nxje.4","title":"Integrate predicate pushdown for shredded fields","description":"Use @dotdo/iceberg variant shredding APIs for predicate pushdown:\n\n1. Use filterDataFilesWithStats() to prune data files\n2. Use transformVariantFilter() for filter transformation\n3. Apply row group filtering based on shredded column statistics\n\nThis enables significant query optimization when filtering on shredded fields.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:52:30.167329-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:35:44.942764-06:00","closed_at":"2026-02-03T10:35:44.942764-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-nxje.4","depends_on_id":"parquedb-nxje","type":"parent-child","created_at":"2026-02-03T07:52:30.168271-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-nxje.4","depends_on_id":"parquedb-nxje.1","type":"blocks","created_at":"2026-02-03T07:52:39.304712-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-nxje.5","title":"Add variant shredding tests","description":"Add tests for variant shredding configuration and behavior in IcebergBackend.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:52:31.372596-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:35:21.630723-06:00","closed_at":"2026-02-03T10:35:21.630723-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-nxje.5","depends_on_id":"parquedb-nxje","type":"parent-child","created_at":"2026-02-03T07:52:31.373504-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-nxje.5","depends_on_id":"parquedb-nxje.1","type":"blocks","created_at":"2026-02-03T07:52:39.447068-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-nxje.6","title":"Update docs for variant shredding","description":"Update docs/backends.md and docs/architecture/pluggable-backends.md with variant shredding configuration examples.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T07:52:33.03439-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:10:18.259632-06:00","closed_at":"2026-02-03T10:10:18.259632-06:00","close_reason":"Created comprehensive variant shredding documentation at docs/guides/variant-shredding.md","dependencies":[{"issue_id":"parquedb-nxje.6","depends_on_id":"parquedb-nxje","type":"parent-child","created_at":"2026-02-03T07:52:33.035348-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-nxje.6","depends_on_id":"parquedb-nxje.1","type":"blocks","created_at":"2026-02-03T07:52:39.587473-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-o1j","title":"[RED] GraphDL integration tests","description":"Write failing tests for fromGraphDL() conversion","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:52:05.330852-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T04:45:58.885825-06:00","closed_at":"2026-02-02T04:45:58.885825-06:00","close_reason":"Tests implemented in earlier waves"}
{"id":"parquedb-o2m","title":"Relationship System","description":"Bidirectional relationships with predicate/reverse support","status":"closed","priority":1,"issue_type":"epic","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:50:30.49885-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:34:08.859159-06:00","closed_at":"2026-02-01T14:34:08.859159-06:00","close_reason":"Closed"}
{"id":"parquedb-o2mn","title":"Product: Production-readiness checklist - error handling, logging, monitoring","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:45:49.482484-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:52:58.543424-06:00","closed_at":"2026-02-03T12:52:58.543424-06:00","close_reason":"Closed"}
{"id":"parquedb-o2q8","title":"Critical: Data loss risk in state reconstruction rollback","description":"In state-store.ts (lines 332-348), the rollback logic catches and silently ignores errors during backup restoration. If rollback fails partway through, the database could be left in an inconsistent state with some files restored and others not. The 'best effort' rollback is insufficient for data integrity guarantees.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:01.884715-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:56:31.714222-06:00","closed_at":"2026-02-03T11:56:31.714222-06:00","close_reason":"Closed"}
{"id":"parquedb-o3d3","title":"Implement events manifest tracking","description":"Track event segments with a manifest file:\n\n```\nevents/\n  _manifest.json        # ordered list of segments\n  seg-0001.parquet\n  seg-0002.parquet\n```\n\nManifest structure:\n- Ordered list of segments\n- Min/max timestamps per segment\n- Event counts\n- Compaction watermark\n\nFile: src/events/manifest.ts","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T06:37:55.260933-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T07:09:57.644577-06:00","closed_at":"2026-02-01T07:09:57.644577-06:00","close_reason":"Implemented ManifestManager for tracking event segments. Features: segment ordering by timestamp, time range queries, compaction watermark, sequence management, statistics. Added 23 passing tests.","dependencies":[{"issue_id":"parquedb-o3d3","depends_on_id":"parquedb-1c3","type":"blocks","created_at":"2026-02-01T06:38:09.306184-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-o40","title":"Load all examples with real data","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T15:47:15.85633-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:13:39.023445-06:00","closed_at":"2026-02-01T13:13:39.023445-06:00","close_reason":"Closed"}
{"id":"parquedb-o4mw","title":"P2: Create ID parsing utility to avoid repeated splits","description":"src/Collection.ts:808-814,853-857 - Entity IDs split multiple times in hot paths. Create utility that parses once.","status":"open","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:15.97364-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:15.97364-06:00"}
{"id":"parquedb-o5p7","title":"[SECURITY] Replace new Function() with Dynamic Worker Loaders","description":"The src/client/rpc-promise.ts file uses unsafe new Function(params, actualBody) for deserializing mapper functions. This is eval-like behavior that poses security risks.\n\nReplace with Cloudflare Dynamic Worker Loaders (when GA) or implement pre-registered mapper pattern. Research completed: primitives.org.ai ai-evaluate doesn't exist publicly - use Cloudflare sandbox SDK or Worker Loaders.\n\nReferences:\n- https://developers.cloudflare.com/workers/runtime-apis/bindings/worker-loader/\n- https://developers.cloudflare.com/sandbox/","notes":"Fixed by implementing secure mapper patterns:\n\n1. Path-Based Mappers - Simple property access (p =\u003e p.name) auto-converted to safe path-based mappers\n2. Registered Mappers - New registerMapper/getRegisteredMapper API for pre-registered safe functions  \n3. Legacy Mode with Strict Validation - Complex functions validated against dangerous patterns\n4. All 87 tests pass including new security tests\n\nSee commit for full implementation details.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T13:35:41.316032-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:45:52.292203-06:00","closed_at":"2026-02-01T13:45:52.292206-06:00"}
{"id":"parquedb-o8g0","title":"DRY: Duplicate filter/pagination logic in core.ts and query.ts","description":"Filter evaluation and pagination logic is duplicated between:\n- src/ParqueDB/core.ts (ParqueDBImpl.find method)\n- src/ParqueDB/query.ts (findEntities function)\n\nBoth files implement:\n- Filter matching with matchesFilter()\n- Pagination with limit/skip/offset\n- Sorting with sortEntities()\n- Projection handling\n\nThis creates maintenance burden and risk of divergence. Should be consolidated into a single query execution path.\n\nRecommendation:\n1. Keep query.ts as the canonical implementation\n2. Have core.ts delegate to query.ts functions\n3. Or extract shared logic to a QueryRunner class","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:47:46.176581-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:23:15.803444-06:00","closed_at":"2026-02-03T13:23:15.803444-06:00","close_reason":"Closed"}
{"id":"parquedb-o8ri","title":"Refactor: Storage - Extract duplicate generateEtag function","description":"The generateEtag function using FNV-1a hash is duplicated in:\\n\\n1. MemoryBackend.ts (lines 82-92)\\n2. DOSqliteBackend.ts (lines 118-128)\\n\\nBoth use identical FNV-1a hash algorithm with timestamp suffix:\\n```typescript\\nfunction generateEtag(data: Uint8Array): string {\\n  let hash = 2166136261\\n  for (let i = 0; i \u003c data.length; i++) {\\n    hash ^= data[i]!\\n    hash = (hash * 16777619) \u003e\u003e\u003e 0\\n  }\\n  const timestamp = Date.now().toString(36)\\n  return \\`${hash.toString(16)}-${timestamp}\\`\\n}\\n```\\n\\nRefactor to:\\n- Move generateEtag to src/storage/utils.ts or src/storage/validation.ts\\n- Update both backends to use the shared implementation\\n\\nFiles affected:\\n- /Users/nathanclevenger/projects/parquedb/src/storage/MemoryBackend.ts\\n- /Users/nathanclevenger/projects/parquedb/src/storage/DOSqliteBackend.ts","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:32:40.658257-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:22:10.18345-06:00","closed_at":"2026-02-03T07:22:10.18345-06:00","close_reason":"Implemented with full test coverage"}
{"id":"parquedb-o933","title":"Fix db-provider semantic search tests","description":"tests/unit/integrations/db-provider.test.ts has 4 failing tests for semantic search and hybrid search. Tests now throw errors without embedding provider after recent fix.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:47:35.830347-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:49:49.946455-06:00","closed_at":"2026-02-03T12:49:49.946455-06:00","close_reason":"Closed"}
{"id":"parquedb-o98","title":"[GREEN] Inbound pagination implementation","description":"Implement inbound pagination to pass tests","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:56.188806-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:25:01.857572-06:00","closed_at":"2026-02-01T14:25:01.857572-06:00","close_reason":"Closed"}
{"id":"parquedb-o9bw","title":"Epic: Vector Search Integration","description":"Add vector similarity search capabilities to ParqueDB\n\n## Scope\n- HNSW index implementation for approximate nearest neighbor search\n- Embedding storage in Parquet files\n- $vectorSearch query operator for similarity queries\n\n## Acceptance Criteria\n- [ ] HNSW index can be created on vector columns\n- [ ] Embeddings stored efficiently in Parquet format\n- [ ] $vectorSearch operator supports k-NN queries\n- [ ] Integration with popular embedding models","status":"open","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:57:56.361375-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:57:56.361375-06:00"}
{"id":"parquedb-o9bw.1","title":"HNSW index implementation","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:34:19.703391-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:34:33.525884-06:00","closed_at":"2026-02-03T09:34:33.525884-06:00","close_reason":"HNSW implementation complete in src/indexes/vector/hnsw.ts with full CRUD operations, persistence, and serialization","dependencies":[{"issue_id":"parquedb-o9bw.1","depends_on_id":"parquedb-o9bw","type":"parent-child","created_at":"2026-02-03T09:34:19.704133-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-o9bw.2","title":"Vector distance functions (cosine, euclidean, dot)","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:34:20.471613-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:34:34.481898-06:00","closed_at":"2026-02-03T09:34:34.481898-06:00","close_reason":"Distance functions implemented in src/indexes/vector/distance.ts - cosine, euclidean, and dot product with normalize utilities","dependencies":[{"issue_id":"parquedb-o9bw.2","depends_on_id":"parquedb-o9bw","type":"parent-child","created_at":"2026-02-03T09:34:20.472388-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-o9bw.3","title":"$vectorSearch query operator","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:34:26.10571-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:34:36.089433-06:00","closed_at":"2026-02-03T09:34:36.089433-06:00","close_reason":"$vector operator implemented in src/types/filter.ts with query executor support in src/query/executor.ts","dependencies":[{"issue_id":"parquedb-o9bw.3","depends_on_id":"parquedb-o9bw","type":"parent-child","created_at":"2026-02-03T09:34:26.106859-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-o9bw.4","title":"Embedding provider interface and integrations","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:34:27.218359-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:34:40.261009-06:00","closed_at":"2026-02-03T09:34:40.261009-06:00","close_reason":"Embedding provider interface in src/embeddings/provider.ts with WorkersAI and AI SDK adapters, plus query caching","dependencies":[{"issue_id":"parquedb-o9bw.4","depends_on_id":"parquedb-o9bw","type":"parent-child","created_at":"2026-02-03T09:34:27.2192-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-o9bw.5","title":"Hybrid search (vector + metadata filtering)","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:34:28.166853-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:34:42.21388-06:00","closed_at":"2026-02-03T09:34:42.21388-06:00","close_reason":"Hybrid search implemented with pre-filter and post-filter strategies, auto strategy selection, and full query executor integration","dependencies":[{"issue_id":"parquedb-o9bw.5","depends_on_id":"parquedb-o9bw","type":"parent-child","created_at":"2026-02-03T09:34:28.167529-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-o9bw.6","title":"Add vector search performance benchmarks","description":"The BENCHMARKS.md document lists performance targets for many operations but does not include vector search:\n\nCurrent documented targets:\n- Get by ID: 5ms p50 / 20ms p99\n- Find (indexed): 20ms p50 / 100ms p99\n- FTS search: 20ms p50 / 100ms p99\n\nMissing:\n- Vector search: No target specified\n\nNeed to:\n1. Create vector search benchmark suite in tests/benchmarks/\n2. Test HNSW search at various scales (1K, 10K, 100K vectors)\n3. Test different dimensions (128, 768, 1024, 1536)\n4. Test different metrics (cosine, euclidean, dot)\n5. Measure build time vs search time tradeoffs\n6. Add performance targets to BENCHMARKS.md\n7. Add vector search target row to CLAUDE.md performance table","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:15:24.432892-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:05:54.000426-06:00","closed_at":"2026-02-03T16:05:54.000426-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-o9bw.6","depends_on_id":"parquedb-o9bw","type":"parent-child","created_at":"2026-02-03T12:15:24.433646-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-o9qv","title":"P2: Add property-based tests for filter evaluation","description":"Use fast-check for property-based testing of filter evaluation. This will automatically catch edge cases that manual tests might miss. Add to tests/unit/query/.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:11:08.60491-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:36:57.440144-06:00","closed_at":"2026-02-03T15:36:57.440144-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-o9rd","title":"Testing: Add schema validation tests for all TypeDefinition variations","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:49:55.862687-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:20:14.009568-06:00","closed_at":"2026-02-03T13:20:14.009568-06:00","close_reason":"Closed"}
{"id":"parquedb-oaeu","title":"Extract duplicate type guards into factory","description":"54+ type guard functions follow identical pattern. Create createTypeGuard factory and isOneOf/isArrayOf utilities to reduce ~400 lines.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:15:06.232502-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T17:29:32.696396-06:00","closed_at":"2026-02-03T17:29:32.696396-06:00","close_reason":"Closed"}
{"id":"parquedb-of82","title":"Missing input validation and error handling for $regex operator","description":"## Location\nsrc/query/filter.ts, lines 441-447\n\n## Problem\nThe \\$regex operator has two issues:\n\n1. **Missing type validation**: opValue is cast to `string | RegExp` without validation. If opValue is a number, null, or object, it could cause unexpected behavior.\n\n2. **Missing error handling**: createSafeRegex can throw UnsafeRegexError or SyntaxError for invalid patterns. This exception is not caught, meaning malformed \\$regex queries will crash the entire filter evaluation instead of returning false.\n\nCurrent code:\n```typescript\ncase '\\$regex': {\n  if (typeof value !== 'string') return false\n  const flags = operators.\\$options !== undefined ? (operators.\\$options as string) : undefined\n  const pattern = createSafeRegex(opValue as string | RegExp, flags)  // Can throw!\n  if (!pattern.test(value)) return false\n  break\n}\n```\n\n## Fix\nAdd type validation and error handling:\n```typescript\ncase '\\$regex': {\n  if (typeof value !== 'string') return false\n  // Validate opValue type\n  if (typeof opValue !== 'string' \u0026\u0026 !(opValue instanceof RegExp)) return false\n  const flags = operators.\\$options !== undefined ? String(operators.\\$options) : undefined\n  try {\n    const pattern = createSafeRegex(opValue, flags)\n    if (!pattern.test(value)) return false\n  } catch {\n    // Invalid regex pattern - treat as no match\n    return false\n  }\n  break\n}\n```\n\n## Impact\n- Malformed \\$regex queries can crash filter evaluation\n- Non-string/RegExp opValue could cause TypeError or unexpected coercion","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:37:16.693476-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:38:19.814965-06:00","closed_at":"2026-02-03T11:38:19.814965-06:00","close_reason":"Closed"}
{"id":"parquedb-ofzl","title":"Benchmark-indexed missing scanFilter: always reports 1x speedup","description":"The benchmark-indexed endpoint reports 1x speedup for all queries because no BenchmarkQuery defines scanFilter. When scanFilter is undefined, the benchmark falls back to using indexed latencies as the scan baseline (line 322 in benchmark-indexed.ts), making indexed and scan times identical. Fix: add scanFilter to every query definition that strips the $index_ prefix from field names.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T08:27:36.88442-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T08:38:25.480663-06:00","closed_at":"2026-02-02T08:38:25.480663-06:00","close_reason":"Closed"}
{"id":"parquedb-oisd","title":"Refactor: Worker - Deduplicate schema definitions between DO_SQLITE_SCHEMA and ParqueDBDO","description":"The SQLite schema is defined in two places:\n\n1. src/types/worker.ts - DO_SQLITE_SCHEMA constant with CREATE TABLE statements\n2. src/worker/ParqueDBDO.ts - ensureInitialized() method with inline CREATE TABLE statements\n\nThese are not in sync - worker.ts has outdated schema (missing events_wal, event_batches tables).\n\nFiles:\n- src/types/worker.ts (DO_SQLITE_SCHEMA)\n- src/worker/ParqueDBDO.ts (ensureInitialized method)\n\nRefactor:\n- Either use DO_SQLITE_SCHEMA from worker.ts in ParqueDBDO\n- Or remove DO_SQLITE_SCHEMA since it's not being used\n\nImpact: Single source of truth for schema, prevents drift","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:33:10.047698-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:21:19.930639-06:00","closed_at":"2026-02-03T11:21:19.930639-06:00","close_reason":"Closed"}
{"id":"parquedb-ojlu","title":"Refactor: Storage - Inconsistent readRange end parameter semantics","description":"There's inconsistency in how backends interpret the 'end' parameter for readRange:\\n\\n1. MemoryBackend.ts (line 167): end is exclusive\\n   `return entry.data.slice(start, actualEnd)`\\n\\n2. R2Backend.ts (line 156): end is inclusive\\n   `const length = end - start + 1  // end is inclusive`\\n\\n3. DOSqliteBackend.ts (line 241-242): end is exclusive\\n   `const actualEnd = Math.min(end, data.length)`\\n   `return data.slice(start, actualEnd)`\\n\\n4. FsBackend.ts (lines 139-148): end is exclusive based on length calculation\\n   `const actualEnd = Math.min(end, fileStat.size)`\\n   `const length = actualEnd - start`\\n\\nThe StorageBackend interface should document whether 'end' is inclusive or exclusive, and all backends should be consistent. Standard convention (like Array.slice) treats end as exclusive.\\n\\nFiles affected:\\n- /Users/nathanclevenger/projects/parquedb/src/types/storage.ts (add documentation)\\n- /Users/nathanclevenger/projects/parquedb/src/storage/R2Backend.ts (fix to be exclusive)","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:33:53.481706-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:11:50.275805-06:00","closed_at":"2026-02-03T11:11:50.275805-06:00","close_reason":"Closed"}
{"id":"parquedb-okx6","title":"Refactor: core.ts beginTransaction() - Improve rollback implementation","description":"In src/ParqueDB/core.ts, beginTransaction() (lines 2287-2366) has an incomplete rollback implementation.\n\nCurrent rollback logic (lines 2342-2364):\n```typescript\nasync rollback(): Promise\u003cvoid\u003e {\n  self.inTransaction = false\n  self.pendingEvents = []\n  \n  // Rollback by undoing operations in reverse order\n  for (const op of pendingOps.reverse()) {\n    if (op.type === 'create' \u0026\u0026 op.entity) {\n      self.entities.delete(op.entity.$id as string)\n      // Remove the CREATE event\n      // ...\n    }\n    // For update/delete, we'd need to restore from before state\n    // This is a simplified implementation  \u003c-- COMMENT ACKNOWLEDGES INCOMPLETE\n  }\n}\n```\n\nIssues:\n1. Update rollback is not implemented (comment says 'simplified')\n2. Delete rollback is not implemented\n3. The 'before' state is not captured for updates/deletes\n\nTo properly implement transactions:\n1. Capture before state for all operations in pendingOps\n2. Implement update rollback: restore entity to before state\n3. Implement delete rollback: restore entity and remove from deletedAt\n4. Consider using a proper transactional pattern (command pattern, unit of work)\n\nThis is a correctness issue if transactions are relied upon.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:35:02.726426-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:31:58.535831-06:00","closed_at":"2026-02-03T07:31:58.535831-06:00","close_reason":"Closed","labels":["bug-risk","correctness","refactor"]}
{"id":"parquedb-ol8j","title":"Bug: scheduleFlush returns Promise.resolve() for transactions but operations still await it","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:36:08.05728-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:49:28.367837-06:00","closed_at":"2026-02-03T12:49:28.367837-06:00","close_reason":"Closed"}
{"id":"parquedb-olhr","title":"Bug: CollectionImpl missing semanticSearch and hybridSearch methods","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:26.146222-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:08:26.19542-06:00","closed_at":"2026-02-03T13:08:26.19542-06:00","close_reason":"Closed"}
{"id":"parquedb-onde","title":"VectorIndex memory usage grows unbounded for large datasets","description":"The HNSW implementation stores all vectors in memory (nodes Map). For production workloads with millions of vectors, this will exceed Cloudflare Workers' 128MB memory limit.\n\nExample calculation:\n- 1M vectors @ 1024 dimensions\n- 8 bytes per float64\n- ~8GB just for vectors (ignoring graph connections)\n\nCurrent implementation:\n```typescript\n// src/indexes/vector/hnsw.ts:230\nprivate nodes: Map\u003cnumber, HNSWNode\u003e = new Map()\n```\n\nThis is a fundamental limitation for production workloads.\n\nPotential solutions:\n1. Disk-based HNSW with mmap (not feasible in Workers)\n2. Product quantization (PQ) to reduce memory 4-8x\n3. Hierarchical quantization (IVF + HNSW)\n4. External vector store integration (e.g., Pinecone, Vectorize)\n5. Split index into chunks stored in R2 with caching\n\nPriority: P1 because it blocks production use with large datasets.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:15:40.220769-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:44:04.350124-06:00","closed_at":"2026-02-03T12:44:04.350124-06:00","close_reason":"Closed"}
{"id":"parquedb-oq4","title":"[RED] Traversal tests","description":"Write failing tests for related() and referencedBy() methods","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:49.051198-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T04:45:59.117237-06:00","closed_at":"2026-02-02T04:45:59.117237-06:00","close_reason":"Tests implemented in earlier waves"}
{"id":"parquedb-ov5","title":"Define Event types and schema","description":"Define the core Event interface and schema for the events log:\n\n```typescript\ninterface Event {\n  id: string           // ULID\n  ts: number           // timestamp\n  op: 'CREATE' | 'UPDATE' | 'DELETE'\n  target: string       // 'users:u1' or 'rel:users:u1:authored:posts:p5'\n  before?: Variant     // null for CREATE\n  after?: Variant      // null for DELETE\n  actor?: string\n}\n```\n\nFile: src/events/types.ts","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T06:37:47.411258-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T07:01:50.629498-06:00","closed_at":"2026-02-01T07:01:50.629498-06:00","close_reason":"Implemented Event types with new schema: target string (ns:id for entities, from:pred:to for relationships), numeric ts, undefined instead of null for before/after. Updated ParqueDB.ts, ParqueDBDO.ts, and event tests."}
{"id":"parquedb-ozzd","title":"P1: Add input validation for URL-derived parameters in sync-routes","description":"Database ID extracted from URL path at src/worker/sync-routes.ts:137-145 without validation against path traversal. Add explicit validation using validateFilePath utilities.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:10:26.56492-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:24:28.843781-06:00","closed_at":"2026-02-03T15:24:28.843781-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-p0b","title":"[GREEN] GraphDL integration implementation","description":"Implement GraphDL integration to pass tests","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:52:06.492301-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:18:32.144874-06:00","closed_at":"2026-02-01T14:18:32.144874-06:00","close_reason":"Closed"}
{"id":"parquedb-p1v","title":"[REFACTOR] Query execution cleanup","description":"Optimize query execution with predicate pushdown","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:19.940319-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:02:48.265795-06:00","closed_at":"2026-02-01T14:02:48.265795-06:00","close_reason":"Closed"}
{"id":"parquedb-p3ul","title":"Standardize error handling patterns","description":"Error handling varies: some use custom errors (VersionConflictError, ValidationError), others throw generic Error, some catch blocks swallow errors silently. Establish consistent error hierarchy and handling patterns throughout codebase.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:38:40.579663-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:50:01.0781-06:00","closed_at":"2026-02-03T08:50:01.0781-06:00","close_reason":"Closed"}
{"id":"parquedb-p57w","title":"VectorIndex serialization uses fixed Float64 precision","description":"The VectorIndex serialization always uses Float64 (8 bytes per dimension):\n\n```typescript\n// hnsw.ts:1108-1110\nfor (let i = 0; i \u003c node.vector.length; i++) {\n  view.setFloat64(offset, node.vector[i]!, false)\n  offset += 8\n}\n```\n\nFor many embedding models (especially normalized vectors for cosine similarity), Float32 would be sufficient and halve storage size.\n\nImpact on 1M vectors @ 1024 dimensions:\n- Current (Float64): 8GB\n- With Float32: 4GB\n\nSuggested solution:\n1. Add precision option to VectorIndexOptions (float32, float64)\n2. Store precision in serialized format header\n3. Default to float32 for normalized vectors\n4. Validate precision compatibility on deserialize\n\nNote: This won't fix the in-memory representation (still uses JS numbers = float64), but it will reduce disk/R2 storage.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:16:10.444939-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:59:59.024816-06:00","closed_at":"2026-02-03T16:59:59.024816-06:00","close_reason":"Closed"}
{"id":"parquedb-p5w","title":"[GREEN] Relationship reading implementation","description":"Implement relationship reading to pass tests","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:31.149045-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:13:17.670691-06:00","closed_at":"2026-02-01T14:13:17.670691-06:00","close_reason":"Closed"}
{"id":"parquedb-p76","title":"[GREEN] Event logging implementation","description":"Implement event logging to pass tests","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:36.413973-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:21:45.110814-06:00","closed_at":"2026-02-01T14:21:45.110814-06:00","close_reason":"Closed"}
{"id":"parquedb-p7cq","title":"Fix GeneratedContentMV constructor config test","description":"GeneratedContentMV.test.ts constructor test failing. config.maxAgeMs returns undefined instead of expected 2592000000 (30 days). The default config is not being set correctly.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:22:12.255381-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:23:00.002014-06:00","closed_at":"2026-02-03T11:23:00.002014-06:00","close_reason":"Closed"}
{"id":"parquedb-pbjb","title":"Add error type variants for search worker","description":"Create typed error variants: not_found, invalid_param, r2_error, parse_error. Currently all errors return generic 500. Should return 400 for client errors.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:27:46.141583-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:18:28.938851-06:00","closed_at":"2026-02-03T13:18:28.938851-06:00","close_reason":"Closed"}
{"id":"parquedb-pbk4","title":"Add input validation for limit/offset parameters","description":"parseInt() in search.ts can return NaN silently. No validation that offset \u003e= 0 or limit \u003e 0. Could cause unexpected behavior or OOM with malicious input.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:27:31.425161-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:40:54.719276-06:00","closed_at":"2026-02-03T12:40:54.719276-06:00","close_reason":"Closed"}
{"id":"parquedb-pdn0","title":"CLI: generate command missing output path validation","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:11.773462-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:36:42.703231-06:00","closed_at":"2026-02-03T11:36:42.703231-06:00","close_reason":"Closed"}
{"id":"parquedb-pdn4","title":"Document Local Development Story for Compaction","description":"No miniflare testing documentation for compaction workflows. Document:\n- Setting up miniflare for local compaction testing\n- Mock storage backends for development\n- Test fixtures and sample data\n- Debugging compaction workflows locally\n- Integration test setup","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T14:34:51.158569-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:53:58.92221-06:00","closed_at":"2026-02-03T15:53:58.92221-06:00","close_reason":"Closed"}
{"id":"parquedb-pg2j","title":"E2E Interop Tests: Test ParqueDB-written files with external tools","description":"Test that ParqueDB-written files work with external tools:\n- Read Iceberg tables in DuckDB\n- Read Delta tables in Spark\n- Validate Avro manifests with iceberg-python","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:57:45.344562-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:09:08.385452-06:00","closed_at":"2026-02-03T09:09:08.385452-06:00","close_reason":"Closed"}
{"id":"parquedb-pk9y","title":"Fix ingest-stream test failures","description":"tests/unit/ingest-stream.test.ts has 20+ failing tests. All db.ingestStream() tests failing including: basic streaming, batching, progress callbacks, transform functions, error handling, backpressure, options, collection-level access, and integration patterns.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:27.476324-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:46:41.682478-06:00","closed_at":"2026-02-03T11:46:41.682478-06:00","close_reason":"Implemented ingestStream method for ParqueDB and Collection with full test coverage"}
{"id":"parquedb-pl8u","title":"Add ReadPath.ts tests","description":"src/worker/ReadPath.ts has no dedicated tests. Add unit tests for the read path logic separate from full QueryExecutor tests.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:29.687347-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T15:51:24.330297-06:00","closed_at":"2026-02-01T15:51:24.330297-06:00","close_reason":"Closed"}
{"id":"parquedb-plms","title":"Use centralized Env types in workflow files","description":"Workflow files define local Env interfaces instead of using centralized types from src/workflows/types.ts. Replace local interfaces with Pick\u003cWorkerEnv, ...\u003e in: 1) compaction-migration.ts:125-128, 2) compaction-queue-consumer.ts:100-106. Import WorkflowEnv and CompactionQueueEnv from ./types.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:33:22.908203-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:36:46.91038-06:00","closed_at":"2026-02-03T13:36:46.91038-06:00","close_reason":"Closed"}
{"id":"parquedb-pmm8","title":"Epic: Full-Text Search","description":"Add full-text search capabilities to ParqueDB\n\n## Scope\n- Inverted index implementation for text search\n- $text query operator for FTS queries\n- Tokenization and stemming support\n\n## Acceptance Criteria\n- [ ] Inverted index can be created on text columns\n- [ ] $text operator supports phrase and boolean queries\n- [ ] Tokenization handles multiple languages\n- [ ] Stemming reduces words to root forms","status":"open","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:57:57.895842-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:57:57.895842-06:00"}
{"id":"parquedb-pmm8.1","title":"Implement boolean query support for $text operator","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:34:05.004386-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:23:42.191529-06:00","closed_at":"2026-02-03T10:23:42.191529-06:00","close_reason":"Boolean query support for $text operator was already fully implemented. The FTS module supports: AND, OR, NOT operators, +/- prefix modifiers, phrase queries, and parentheses grouping. All 76 boolean query tests pass.","dependencies":[{"issue_id":"parquedb-pmm8.1","depends_on_id":"parquedb-pmm8","type":"parent-child","created_at":"2026-02-03T09:34:05.005192-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-pmm8.10","title":"FTS: Add fuzzy matching/typo tolerance support","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:15:33.501751-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:05:05.067235-06:00","closed_at":"2026-02-03T15:05:05.067235-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-pmm8.10","depends_on_id":"parquedb-pmm8","type":"parent-child","created_at":"2026-02-03T12:15:33.502637-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-pmm8.11","title":"FTS: Phrase boost does not differentiate scores when both docs have phrase","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:15:34.554037-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:11:52.796443-06:00","closed_at":"2026-02-03T15:11:52.796443-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-pmm8.11","depends_on_id":"parquedb-pmm8","type":"parent-child","created_at":"2026-02-03T12:15:34.555019-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-pmm8.2","title":"Add phrase query support to $text operator","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:34:10.009184-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:44:58.812948-06:00","closed_at":"2026-02-03T10:44:58.812948-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-pmm8.2","depends_on_id":"parquedb-pmm8","type":"parent-child","created_at":"2026-02-03T09:34:10.009886-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-pmm8.3","title":"Add unit tests for FTS boolean and phrase queries","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:34:10.74182-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:10:08.726264-06:00","closed_at":"2026-02-03T10:10:08.726264-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-pmm8.3","depends_on_id":"parquedb-pmm8","type":"parent-child","created_at":"2026-02-03T09:34:10.742647-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-pmm8.4","title":"FTS: search() does not auto-detect and delegate to searchBoolean()","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:15:18.671949-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:49:13.009598-06:00","closed_at":"2026-02-03T12:49:13.009598-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-pmm8.4","depends_on_id":"parquedb-pmm8","type":"parent-child","created_at":"2026-02-03T12:15:18.672688-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-pmm8.5","title":"FTS: Phrase query position matching produces incorrect results","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:15:19.418815-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:00:40.5229-06:00","closed_at":"2026-02-03T13:00:40.5229-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-pmm8.5","depends_on_id":"parquedb-pmm8","type":"parent-child","created_at":"2026-02-03T12:15:19.419616-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-pmm8.6","title":"FTS: Required (+) and excluded (-) phrase modifiers not working correctly","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:15:20.941741-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:54:48.625514-06:00","closed_at":"2026-02-03T12:54:48.625514-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-pmm8.6","depends_on_id":"parquedb-pmm8","type":"parent-child","created_at":"2026-02-03T12:15:20.942526-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-pmm8.7","title":"FTS: Add multi-language stemmer support beyond English","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:15:26.94307-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:00:23.598948-06:00","closed_at":"2026-02-03T16:00:23.598948-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-pmm8.7","depends_on_id":"parquedb-pmm8","type":"parent-child","created_at":"2026-02-03T12:15:26.943814-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-pmm8.8","title":"FTS: Index storage format uses JSON - consider binary format for large indexes","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:15:28.028424-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:58:55.765388-06:00","closed_at":"2026-02-03T15:58:55.765388-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-pmm8.8","depends_on_id":"parquedb-pmm8","type":"parent-child","created_at":"2026-02-03T12:15:28.029259-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-pmm8.9","title":"FTS: Implement highlight/snippet generation for search results","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:15:29.253988-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:15:39.819053-06:00","closed_at":"2026-02-03T15:15:39.819053-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-pmm8.9","depends_on_id":"parquedb-pmm8","type":"parent-child","created_at":"2026-02-03T12:15:29.254827-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-pn1m","title":"Implement asIndexCatalog validation in types/cast.ts","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:19:34.722791-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:22:37.62943-06:00","closed_at":"2026-02-03T15:22:37.62943-06:00","close_reason":"Closed"}
{"id":"parquedb-pntz","title":"Refactor: Query - Add QueryBuilder array operator support","description":"## Summary\nThe QueryBuilder class in `src/query/builder.ts` only supports comparison, string, and existence operators. It's missing support for array operators.\n\n### Missing QueryBuilder Operators\n\n1. **$all** - Array contains all specified values\n2. **$elemMatch** - Array element matches filter\n3. **$size** - Array has specific length\n\n### Current QueryBuilder Operators (line 37-58)\n```typescript\nexport type ComparisonOp = 'eq' | '=' | 'ne' | '\\!=' | 'gt' | '\u003e' | ...\nexport type StringOp = 'regex' | 'startsWith' | 'endsWith' | 'contains'\nexport type ExistenceOp = 'exists'\n```\n\n### Proposed API\n```typescript\nbuilder\n  .whereArray('tags', 'all', ['featured', 'published'])\n  .whereArray('items', 'elemMatch', { status: 'active' })\n  .whereArray('tags', 'size', 3)\n```\n\nOr extend existing `where` method:\n```typescript\nbuilder.where('tags', 'all', ['featured', 'published'])\n```\n\n### Files to Update\n- `src/query/builder.ts` - Add array operators\n\n### Acceptance Criteria\n- [ ] Add ArrayOp type with all/elemMatch/size\n- [ ] Update operatorMap with array operators\n- [ ] Add whereArray() method or extend where()\n- [ ] Add tests for array operator building\n- [ ] Update JSDoc examples","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:33:23.981829-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:22:35.679361-06:00","closed_at":"2026-02-03T11:22:35.679361-06:00","close_reason":"Closed","labels":["refactor"]}
{"id":"parquedb-poif","title":"Replace stub index operations with proper errors","description":"IndexManager.ts has stub implementations (hashLookup, sstRange at lines 381-413, 739-757) that silently return empty results instead of throwing 'not implemented' errors. This causes silent failures in production.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T17:00:59.990453-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T17:12:41.401487-06:00","closed_at":"2026-02-01T17:12:41.401487-06:00","close_reason":"Closed"}
{"id":"parquedb-pqqv","title":"Remove SST index infrastructure","description":"Remove SST (sorted string table) index code since native parquet predicate pushdown on $index_* columns is faster for range queries. Includes:\n- Remove src/indexes/secondary/sst.ts\n- Remove src/indexes/secondary/sharded-sst.ts  \n- Remove SST index loading from IndexCache.ts\n- Remove SST index execution from QueryExecutor\n- Update tests to remove SST index tests\nKeep bloom filters and FTS indexes.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T11:06:13.167525-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T12:11:19.32487-06:00","closed_at":"2026-02-02T12:11:19.32487-06:00","close_reason":"Removed SST index infrastructure. Native parquet predicate pushdown on $index_* columns is now faster for range queries. Deleted sst.ts, sharded-sst.ts, sst-index.test.ts. Updated all type references from 'hash|sst|fts' to 'hash|fts'. All 5920 tests pass."}
{"id":"parquedb-psc6","title":"Testing: Optimize test performance - parallelize isolated tests, reduce 183s suite runtime","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:49:55.034324-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T17:34:33.844516-06:00","closed_at":"2026-02-03T17:34:33.844516-06:00","close_reason":"Closed"}
{"id":"parquedb-pt4a","title":"Fix error swallowing in IndexManager event listeners","description":"IndexManager.emit() at lines 628-638 catches listener errors and only logs a warning. Add onError callback or use robust event emitter to surface these errors to callers.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T17:01:02.613933-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T17:10:38.527528-06:00","closed_at":"2026-02-01T17:10:38.527528-06:00","close_reason":"Closed"}
{"id":"parquedb-pt8","title":"Implement schema parsing","description":"Parse schema definitions, extract fields and relationships","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:54.745485-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:16:33.715358-06:00","closed_at":"2026-02-01T13:16:33.715358-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-pt8","depends_on_id":"parquedb-byc","type":"blocks","created_at":"2026-01-30T11:52:03.965613-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-pt8","depends_on_id":"parquedb-ttd","type":"blocks","created_at":"2026-01-30T11:52:04.051579-06:00","created_by":"Nathan Clevenger"},{"issue_id":"parquedb-pt8","depends_on_id":"parquedb-8jm","type":"blocks","created_at":"2026-01-30T11:52:04.142121-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-ptf9","title":"Fix dynamic import using Function constructor","description":"In src/migration/mongodb.ts lines 219-221, dynamic import uses Function() constructor: BSON = await (Function('return import(\"bson\")')()). This is a security code smell. Replace with standard try/catch around direct dynamic import.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T17:00:45.815791-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T17:01:56.520656-06:00","closed_at":"2026-02-01T17:01:56.520656-06:00","close_reason":"Closed"}
{"id":"parquedb-pu5c","title":"Potential Data Loss in Batch Processing","description":"**File:** src/workflows/compaction-migration.ts (lines 395-406)\n\n**Issue:** When a batch fails, files are still marked as processed even though compaction failed\n\n**Impact:** Source files may be deleted without successful compaction = data loss\n\n**Fix:** Only add to processedFiles on success, keep failed batches for retry\n\n**Severity:** CRITICAL","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T14:33:57.057145-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:30:07.708972-06:00","closed_at":"2026-02-03T15:30:07.708972-06:00","close_reason":"Fixed in commit 6947aa0"}
{"id":"parquedb-puq","title":"[GREEN] Relationship storage implementation","description":"Implement relationship storage to pass tests","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:23.779082-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:08:29.211296-06:00","closed_at":"2026-02-01T14:08:29.211296-06:00","close_reason":"Closed"}
{"id":"parquedb-puwb","title":"Fix MV cycle-detection tests","description":"tests/unit/materialized-views/cycle-detection.test.ts has 15+ failing tests for getMVDependencies, detectMVCycles, MVCycleError, and validateSchema. MV cycle detection functionality may not be implemented.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:32:02.187157-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:32:58.866631-06:00","closed_at":"2026-02-03T12:32:58.866631-06:00","close_reason":"Closed"}
{"id":"parquedb-pvl","title":"[RED] Pagination tests","description":"Write failing tests for limit, skip, cursor","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:31.656352-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T04:45:59.462876-06:00","closed_at":"2026-02-02T04:45:59.462876-06:00","close_reason":"Tests implemented in earlier waves"}
{"id":"parquedb-pwt7","title":"P2: Unify Node.js and Workers storage implementations","description":"Dual storage implementation complexity: Node.js (globalEntityStore) vs Workers (ParqueDBDO SQLite). Risk of divergence. Architectural recommendation: Move toward event-sourced core to unify implementations. See docs/architecture/entity-storage.md.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:10:51.770602-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:59:33.107886-06:00","closed_at":"2026-02-03T15:59:33.107886-06:00","close_reason":"Implemented unified EventSourcedBackend that provides consistent event-sourcing semantics across Node.js and Workers environments. The new architecture eliminates the globalEntityStore vs SQLite divergence by using events as the single source of truth with entity state derived from replaying events. Updated architecture documentation and added comprehensive tests."}
{"id":"parquedb-py1f","title":"Fix snapshot calculation off-by-one test failure","description":"tests/unit/snapshots.test.ts:375 - Event replay count mismatch. Snapshot selection logic needs review. This is a correctness issue blocking production.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T12:42:05.837817-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T12:54:07.29389-06:00","closed_at":"2026-02-01T12:54:07.29389-06:00","close_reason":"Completed by parallel agents"}
{"id":"parquedb-pzka","title":"P0: Add OCC protection to Delta commits","description":"Critical: Delta commit in compaction-migration.ts:593-669 has no OCC protection. Uses simple storage.write() without conditional writes. Two concurrent compaction workflows could both determine the same nextVersion and overwrite each other. Fix: Use conditional writes with ETag for Delta log files, similar to how Iceberg commits work in iceberg-commit.ts.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:33:07.681475-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:38:03.250264-06:00","closed_at":"2026-02-03T13:38:03.250264-06:00","close_reason":"Closed"}
{"id":"parquedb-q0kn","title":"Architecture: Break circular dependencies (11 cycles detected)","description":"Madge detected 11 circular dependency cycles in the codebase. Key cycles include: (1) ParqueDB.ts -\u003e ParqueDB/core.ts -\u003e ParqueDB/types.ts -\u003e db.ts (core cycle affecting main exports), (2) cli/commands/auth.ts \u003c-\u003e cli/index.ts, (3) sync/commit.ts \u003c-\u003e sync/schema-snapshot.ts, (4) worker/index.ts -\u003e worker/handlers/types.ts. These cycles can cause initialization issues and make the codebase harder to reason about.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:45:50.594292-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:59:55.446048-06:00","closed_at":"2026-02-03T12:59:55.446048-06:00","close_reason":"All 11 circular dependencies have been broken by extracting shared types to dedicated files and using interface abstractions"}
{"id":"parquedb-q5e8","title":"[FEATURE] Add ACID transaction support","description":"No transaction support currently. Durable Objects provide single-entity atomicity but multi-entity transactions need:\n\n1. Transaction API design (begin/commit/rollback)\n2. Optimistic concurrency control (version vectors)\n3. Distributed transaction coordinator (if needed)\n4. Transaction isolation levels\n5. Deadlock detection/prevention\n\nConsider two-phase commit or saga pattern for distributed transactions.","status":"closed","priority":3,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T13:36:08.143212-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:42:33.645085-06:00","closed_at":"2026-02-01T14:42:33.645085-06:00","close_reason":"Closed"}
{"id":"parquedb-q5l3","title":"Create deployment guides","description":"Missing production deployment guidance. Create guides for: Cloudflare Workers deployment, R2 bucket setup, Durable Object configuration, caching strategies, monitoring setup.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:45.637269-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T16:15:50.520295-06:00","closed_at":"2026-02-01T16:15:50.520295-06:00","close_reason":"Closed"}
{"id":"parquedb-q7ls","title":"Add Parquet infrastructure unit tests","description":"No unit tests exist for 8+ Parquet files. Create tests/unit/parquet/ with:\n- reader.test.ts\n- writer.test.ts\n- schema.test.ts\n- variant.test.ts\n- compression.test.ts\n- lz4.test.ts\n\nEstimated 30% coverage improvement.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T12:42:11.625199-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T12:54:07.384805-06:00","closed_at":"2026-02-01T12:54:07.384805-06:00","close_reason":"Completed by parallel agents"}
{"id":"parquedb-q9x","title":"[RED] Update operator tests - numeric operators","description":"Write failing tests for $inc, $mul, $min, $max","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:22.78472-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:08:21.680604-06:00","closed_at":"2026-02-01T14:08:21.680604-06:00","close_reason":"Closed"}
{"id":"parquedb-qeyo","title":"P3: Remove unused _T generic parameter","description":"CreateInput\u003c_T\u003e in src/types/entity.ts:701 has unused generic parameter. Remove or use it.","status":"open","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:23.921541-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:23.921541-06:00"}
{"id":"parquedb-qfck","title":"Implement readAndMergeFiles in CompactionMigrationWorkflow","description":"The readAndMergeFiles method in src/workflows/compaction-migration.ts:370-393 is a placeholder that returns empty data. Implement actual Parquet reading using hyparquet library. Must: 1) Read Parquet files from R2, 2) Merge rows from multiple files, 3) Return actual row data and bytes read count.","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:56:00.558387-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:05:57.290471-06:00","closed_at":"2026-02-03T13:05:57.290471-06:00","close_reason":"Closed"}
{"id":"parquedb-qiam","title":"High: Debug endpoints exposed without authentication","description":"Debug endpoints in src/worker/index.ts (lines 906-924) and src/worker/handlers/debug.ts are accessible without any authentication:\n\n- GET /debug/r2 - Exposes R2 bucket structure and file metadata\n- GET /debug/entity - Allows querying arbitrary entities\n- GET /debug/indexes - Exposes index catalog information\n- GET /debug/query - Allows executing arbitrary queries with full diagnostics\n- GET /debug/cache - Exposes cache statistics\n\nThese should either be:\n1. Removed in production builds\n2. Protected with authentication (e.g., admin token check)\n3. Rate limited more aggressively\n\nFiles:\n- /Users/nathanclevenger/projects/parquedb/src/worker/index.ts:906-924\n- /Users/nathanclevenger/projects/parquedb/src/worker/handlers/debug.ts","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:31.372147-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:03:01.659312-06:00","closed_at":"2026-02-03T13:03:01.659312-06:00","close_reason":"Closed"}
{"id":"parquedb-qj5d","title":"Enable noUnusedLocals in tsconfig","description":"noUnusedLocals is false in tsconfig.json. Enable it to catch dead local variables during compilation. May require cleanup pass.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T07:16:03.95317-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T03:54:42.609596-06:00","closed_at":"2026-02-03T03:54:42.609596-06:00","close_reason":"Closed"}
{"id":"parquedb-qrih","title":"Fix rpc-do-server tests","description":"tests/unit/integrations/rpc-do-server.test.ts has 25+ failing tests for ParqueDBRPCWrapper: factory creation, entity operations, relationship operations, batch operations, and collection operations.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:05:58.046218-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:07:32.497945-06:00","closed_at":"2026-02-03T13:07:32.497945-06:00","close_reason":"Closed"}
{"id":"parquedb-qrlx","title":"P2: Remove duplicate compareValues function","description":"compareValues function exists in multiple places with similar implementations: src/query/executor.ts:1189-1204 and src/utils/comparison.ts. Consolidate to single implementation.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:11:06.673012-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:24:29.02951-06:00","closed_at":"2026-02-03T15:24:29.02951-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-qrm","title":"[GREEN] MemoryBackend implementation","description":"Implement MemoryBackend to pass tests","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:17.437898-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:11:45.021139-06:00","closed_at":"2026-02-01T13:11:45.021139-06:00","close_reason":"Closed"}
{"id":"parquedb-qru2","title":"Complete flushToParquet() implementation","description":"ParqueDBDO.flushToParquet() at line ~2111 has TODO comment - actual Parquet writing to R2 is not implemented. Events accumulate in SQLite indefinitely. Use existing hyparquet-writer already imported in bulk write path.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:38:02.424832-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:42:08.995248-06:00","closed_at":"2026-02-03T08:42:08.995248-06:00","close_reason":"Closed"}
{"id":"parquedb-qrxj","title":"Implement ParqueDB client SDK for wiki.org.ai","description":"Create TypeScript client SDK for querying Wikidata via ParqueDB:\n\nAPI design:\n```typescript\nconst wiki = new WikidataClient({ baseUrl: 'https://wiki.org.ai' })\n\n// Entity lookup\nawait wiki.entity('Q42')  // Douglas Adams\n\n// Search\nawait wiki.search('einstein', { lang: 'en', types: ['Q5'] })\n\n// External ID lookup\nawait wiki.byExternalId('P345', 'tt0111161')\n\n// Geo query\nawait wiki.geo.nearby({ lat: 48.86, lng: 2.29, radius: 1000 })\n\n// Relationship traversal\nawait wiki.traverse('Q76', 'P40')  // Obama's children\n```\n\nRequirements:\n- Works in browser and Node.js\n- Lazy loading of Parquet files\n- Caching with TTL\n- TypeScript types from @wiki.org.ai/types","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:18:28.340325-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:56:52.210553-06:00","closed_at":"2026-02-03T13:56:52.210553-06:00","close_reason":"Implemented full wiki.org.ai client SDK with entity, search, geo, external ID, and relationship operations","dependencies":[{"issue_id":"parquedb-qrxj","depends_on_id":"parquedb-tp7v","type":"blocks","created_at":"2026-02-03T13:19:18.109548-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-qu2x","title":"Wire observability hooks into all execution paths","description":"Observability hooks module is well-designed but not fully integrated. MutationExecutor has its own hooks that don't integrate with globalHookRegistry. Wire hooks into query execution and all mutation paths.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T17:01:05.545501-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T17:16:14.920635-06:00","closed_at":"2026-02-01T17:16:14.920635-06:00","close_reason":"Closed"}
{"id":"parquedb-qx3a","title":"Extract BaseBackend class for storage backends","description":"FsBackend, FsxBackend, R2Backend, and S3Backend share ~540 lines of duplicate code. Extract a BaseBackend class with common functionality: validation, retry logic, path handling, error mapping.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:15:02.100441-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T17:59:31.78207-06:00","closed_at":"2026-02-03T17:59:31.78207-06:00","close_reason":"Created BaseBackend.ts with shared functionality including path validation, range validation, error handling utilities (isNotFoundError, isExistsError, isNotEmptyError), and abstract method signatures. Exported from index.ts."}
{"id":"parquedb-qy3g","title":"Add Operational Runbook for Compaction","description":"No documentation for handling stuck compaction, manual workflow retry, disaster recovery. Create docs/guides/compaction-runbook.md with procedures for:\n- Handling stuck compaction jobs\n- Manual workflow retry procedures\n- Disaster recovery steps\n- Troubleshooting common issues","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T14:34:21.491177-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:39:06.010066-06:00","closed_at":"2026-02-03T14:39:06.010066-06:00","close_reason":"Closed"}
{"id":"parquedb-qz1","title":"Event Log \u0026 Time-Travel","description":"CDC event logging and point-in-time queries","status":"closed","priority":2,"issue_type":"epic","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:50:31.757926-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:34:08.927017-06:00","closed_at":"2026-02-01T14:34:08.927017-06:00","close_reason":"Closed"}
{"id":"parquedb-qzao","title":"asIndexCatalog in cast.ts accepts unvalidated data","description":"File: src/types/cast.ts:196-208\n\nThe asIndexCatalog function accepts Record\u003cstring, unknown\u003e and casts it directly to IndexCatalog without any runtime validation. The comment says 'Safe after version validation and structure checks' but the function itself performs no validation.\n\nThis is problematic because:\n1. The IndexCatalog interface defined in cast.ts (lines 196-204) differs from the one in manager.ts\n2. Callers may forget to validate before calling\n3. Invalid data will cause runtime errors downstream\n\nAdditionally, there's a duplicate IndexCatalog type definition:\n- cast.ts:196-204 defines IndexCatalog with: version, indexes (array with name, type, field, path)\n- manager.ts:1048-1051 defines IndexCatalog with: version, indexes (Record\u003cstring, IndexCatalogEntry[]\u003e)\n\nThese are incompatible types with the same name!\n\nFix: \n1. Remove the IndexCatalog definition from cast.ts\n2. Use the canonical definition from manager.ts\n3. Either add validation to the cast function or document that validation must be done by caller","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:13.554318-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:07:19.227475-06:00","closed_at":"2026-02-03T13:07:19.227475-06:00","close_reason":"Closed"}
{"id":"parquedb-r05f","title":"Add: MV cycle detection","description":"MVs can reference other MVs via $from but no cycle detection exists. Implement DAG validation in validateSchema() to detect circular MV dependencies.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:15:40.252938-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:34:14.132387-06:00","closed_at":"2026-02-03T10:34:14.132387-06:00","close_reason":"Closed"}
{"id":"parquedb-r0qh","title":"Vector search lacks query plan integration","description":"The vector search is implemented but lacks full integration with the query optimizer and execution plan system.\n\nIssues:\n1. No cost estimation for vector search in query planner\n2. Hybrid search strategy ('auto') uses simple heuristic (candidateIds.size / nodes.size)\n3. No explain plan output for vector queries\n4. Missing index selection hints for compound queries\n\nRelated code:\n- src/indexes/manager.ts - selectIndex() handles $vector but basic\n- src/query/optimizer.ts - No vector-specific cost model\n- src/indexes/vector/hnsw.ts:436-448 - Simple auto strategy heuristic\n\nNeeded:\n1. Add vector search to query explain plans\n2. Implement cost model considering efSearch, index size, filter selectivity\n3. Surface strategy decision reasoning in explain output\n4. Optimize hybrid search strategy selection","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:15:55.263501-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:16:10.637671-06:00","closed_at":"2026-02-03T15:16:10.637671-06:00","close_reason":"Closed"}
{"id":"parquedb-r10h","title":"Consolidate duplicated filter matching in ParqueDB.ts","description":"Filter matching logic is implemented 3 times: ParqueDB.ts (matchesFilter, matchesOperator), query/filter.ts (matchesFilter, matchesCondition), query/executor.ts (toPredicate). ParqueDB.ts has its own inline implementation that doesn't use safe regex. Consolidate to use matchesFilter from query/filter.ts everywhere.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T17:00:58.790682-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T17:05:51.048483-06:00","closed_at":"2026-02-01T17:05:51.048483-06:00","close_reason":"Closed"}
{"id":"parquedb-r29b","title":"Export ingest-source.ts from materialized-views index","description":"The ingest-source.ts module with correct template literal types is not exported from src/materialized-views/index.ts. Consumers get the broken type from types.ts instead.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:02:18.317611-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:03:09.444819-06:00","closed_at":"2026-02-03T11:03:09.444819-06:00","close_reason":"Added exports for ingest-source.ts and cycle-detection.ts to materialized-views/index.ts"}
{"id":"parquedb-r3i","title":"[GREEN] Update operation implementation","description":"Implement update operations to pass tests","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:33.562739-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:08:21.561836-06:00","closed_at":"2026-02-01T14:08:21.561836-06:00","close_reason":"Closed"}
{"id":"parquedb-r4y4","title":"Code Quality: Replace console.log with structured logger in 90+ files","description":"362 console.log/warn/error occurrences across 90 files should use the existing utils/logger.ts for structured logging. This enables:\n- Log level filtering\n- Structured metadata\n- Consistent formatting\n- Production observability\n\nKey files with high console usage:\n- src/cli/utils.ts (30 occurrences)\n- src/studio/server.ts (37 occurrences)\n- src/cli/commands/migrate-backend.ts (28 occurrences)\n- src/worker/github/webhooks.ts (21 occurrences)\n- src/subscriptions/manager.ts (14 occurrences)","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:47:17.374463-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:20:08.942388-06:00","closed_at":"2026-02-03T16:20:08.942388-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-r57l","title":"Missing JSDoc on Public APIs in ParqueDB/index.ts","description":"**File:** src/ParqueDB/index.ts\n\n**Issue:** Re-exports without documentation\n\n**Fix:** Add module-level JSDoc documentation explaining what is exported and how to use the main API\n\n**Impact:** Better developer experience and API discoverability","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:10:36.553335-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T18:22:33.378066-06:00","closed_at":"2026-02-03T18:22:33.378066-06:00","close_reason":"Fixed in commit ab28519"}
{"id":"parquedb-r99y","title":"Product: CLI experience improvements - init wizard and interactive mode","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:45:47.582721-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:01:38.643086-06:00","closed_at":"2026-02-03T15:01:38.643086-06:00","close_reason":"Closed"}
{"id":"parquedb-rawd","title":"Race Condition in Streaming Merge bytesRead Tracking","description":"**File:** src/workflows/streaming-merge.ts (lines 346-359)\n\n**Issue:** bytesRead tracking adds file size on first chunk, not after full read\n\n**Impact:** Inaccurate metrics if file read fails midway\n\n**Severity:** CRITICAL","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T14:33:58.179446-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:30:07.713868-06:00","closed_at":"2026-02-03T15:30:07.713868-06:00","close_reason":"Fixed in commit 6947aa0"}
{"id":"parquedb-rb6n","title":"Improve manifest JSON parsing error handling","description":"loadLocalManifest() and loadRemoteManifest() in engine.ts silently catch all errors including malformed JSON. A corrupted manifest is treated as non-existent, risking data loss. Distinguish between 'file not found' and 'parse error'.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:05:09.289297-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:06:36.908906-06:00","closed_at":"2026-02-03T08:06:36.908906-06:00","close_reason":"Closed"}
{"id":"parquedb-rbwr","title":"Unused Parameters in Functions","description":"## Problem\n_snapshotId, _ns, _metadata parameters are unused.\n\n## Location\n- src/backends/iceberg.ts (lines 1356-1376)\n\n## Fix\nEither use these parameters or remove them from the function signatures.\n\n## Priority\nP1 - HIGH","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:10:25.697296-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T18:22:30.508878-06:00","closed_at":"2026-02-03T18:22:30.508878-06:00","close_reason":"Fixed in commit ab28519"}
{"id":"parquedb-rihu","title":"Add Compaction CLI Commands","description":"No CLI access to compaction status, manual retry, force cleanup. Add commands:\n- parquedb compaction status - Show current compaction state and history\n- parquedb compaction retry \u003cjob-id\u003e - Manually retry a failed compaction job\n- parquedb compaction cleanup - Force cleanup of orphaned files\n- parquedb compaction trigger - Manually trigger compaction cycle","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T14:34:31.800601-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:41:35.386117-06:00","closed_at":"2026-02-03T14:41:35.386117-06:00","close_reason":"Closed"}
{"id":"parquedb-rj19","title":"Create Express/Fastify Adapters","description":"**Issue:** Only Hono adapter is documented, missing Express and Fastify integrations\n\n**Fix:**\n1. Create src/integrations/express/adapter.ts\n2. Create src/integrations/fastify/adapter.ts\n3. Add documentation in docs/integrations/\n4. Add examples for each framework\n\n**Impact:** Broader ecosystem support beyond Cloudflare Workers","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:10:51.364856-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T18:22:33.899368-06:00","closed_at":"2026-02-03T18:22:33.899368-06:00","close_reason":"Fixed in commit ab28519"}
{"id":"parquedb-rn8z","title":"Add tests for remote client functions","description":"remote.ts lacks tests. Need tests for: openRemoteDB, checkRemoteDB, listPublicDatabases, RemoteCollection operations.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:05:28.287463-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:53:32.610838-06:00","closed_at":"2026-02-03T10:53:32.610838-06:00","close_reason":"Closed"}
{"id":"parquedb-rnfv","title":"Backup/Restore CLI Commands Not Implemented","description":"**Issue:** The backup and restore CLI commands are documented but not actually registered or implemented in the CLI.\n\n**Impact:** \n- No disaster recovery capability\n- Users cannot create backups of their data\n- Cannot restore from backups in case of data loss\n- Documentation promises features that don't exist\n\n**Fix:**\n1. Implement `parquedb backup` command to create point-in-time backups\n2. Implement `parquedb restore` command to restore from backups\n3. Support incremental backups\n4. Add backup scheduling options\n5. Register commands in src/cli/index.ts\n\n**Priority:** P0 CRITICAL","status":"closed","priority":0,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:10:20.775565-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T17:29:55.344318-06:00","closed_at":"2026-02-03T17:29:55.344318-06:00","close_reason":"Closed"}
{"id":"parquedb-rnky","title":"Standardize error handling patterns","description":"Error handling is inconsistent: IndexManager.load() silently continues, HashIndex.load() logs warnings, QueryExecutor.executeWithIndex() returns null. Establish consistent patterns, consider Result types for expected failures.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:10.915559-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T16:14:25.319956-06:00","closed_at":"2026-02-01T16:14:25.319956-06:00","close_reason":"Closed"}
{"id":"parquedb-rpnw","title":"Replace any types with proper types","description":"2 any types found: ParqueDB.ts:3005 (const data: any), types/integrations.ts:221 (f: any). Replace with Record\u003cstring, unknown\u003e or more specific interfaces.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:42.619197-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T16:12:54.30572-06:00","closed_at":"2026-02-01T16:12:54.30572-06:00","close_reason":"Closed"}
{"id":"parquedb-rroq","title":"Implement Wikidata label/search indexes for wiki.org.ai","description":"Create full-text search indexes for Wikidata labels:\n\nIndex types:\n1. Trigram index for fuzzy matching\n2. Prefix index for autocomplete\n3. BM25 vectors for ranking\n\nPer-language indexes (prioritized):\n- English (~80M items) - required\n- German (~15M)\n- French (~12M)\n- Spanish (~10M)\n- Chinese, Japanese, Russian, etc.\n\nTarget: ~4-6GB total, ~500 files\nStore in separate language directories for lazy loading.","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:18:09.019725-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:56:52.498074-06:00","closed_at":"2026-02-03T13:56:52.498074-06:00","close_reason":"Implemented trigram, prefix, and normalized label indexes for multi-language search"}
{"id":"parquedb-ru0a","title":"Deduplicate getActor helper functions","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:29.358938-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:37:22.590872-06:00","closed_at":"2026-02-03T10:37:22.590872-06:00","close_reason":"Closed"}
{"id":"parquedb-rxhp","title":"Refactor: Storage - Add TransactionalBackend implementation","description":"The StorageBackend interface defines TransactionalBackend at lines 295-321 of types/storage.ts:\\n\\n```typescript\\nexport interface TransactionalBackend extends StorageBackend {\\n  beginTransaction(): Promise\u003cTransaction\u003e\\n}\\n\\nexport interface Transaction {\\n  id: string\\n  read(path: string): Promise\u003cUint8Array\u003e\\n  write(path: string, data: Uint8Array): Promise\u003cvoid\u003e\\n  delete(path: string): Promise\u003cvoid\u003e\\n  commit(): Promise\u003cvoid\u003e\\n  rollback(): Promise\u003cvoid\u003e\\n}\\n```\\n\\nNo backend currently implements this. DOSqliteBackend could naturally support this since SQLite has native transaction support. This would enable:\\n- Atomic multi-file operations\\n- Rollback on error\\n- Consistent reads within a transaction\\n\\nFiles affected:\\n- /Users/nathanclevenger/projects/parquedb/src/storage/DOSqliteBackend.ts\\n- /Users/nathanclevenger/projects/parquedb/src/types/storage.ts","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:33:26.352873-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:23:14.491983-06:00","closed_at":"2026-02-03T11:23:14.491983-06:00","close_reason":"Closed"}
{"id":"parquedb-s19","title":"Configure LZ4 compression","description":"Use hyparquet-compressors with LZ4. Works on Workers without WASM. Update ParquetWriter defaults.","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T14:30:13.181126-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-30T14:41:53.408204-06:00","closed_at":"2026-01-30T14:41:53.408204-06:00","close_reason":"Closed"}
{"id":"parquedb-s2bz","title":"Add delta-utils test coverage","description":"The delta-utils module lacks tests for: transaction-log.ts, cdc.ts, variant.ts, retry.ts. Add dedicated test files for each with unit tests covering success paths, error handling, and edge cases.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:27.348219-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T15:55:36.85873-06:00","closed_at":"2026-02-01T15:55:36.85873-06:00","close_reason":"Closed"}
{"id":"parquedb-s31b","title":"Review and tighten CORS configuration","description":"CORS in public-routes.ts allows any origin with Authorization header exposed. Consider: 1) Not exposing Authorization for public endpoints, 2) Using origin whitelist for authenticated endpoints, 3) Document security implications.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:05:04.814792-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:07:21.256734-06:00","closed_at":"2026-02-03T08:07:21.256734-06:00","close_reason":"Closed"}
{"id":"parquedb-s39z","title":"Fix: Missing input validation in defineView/defineCollection","description":"Public APIs don't validate field types in schema definition. Invalid types only caught at runtime. File: src/materialized-views/define.ts","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:15:27.927823-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:43:26.007874-06:00","closed_at":"2026-02-03T10:43:26.007874-06:00","close_reason":"Closed"}
{"id":"parquedb-s3o0","title":"Refactor: Storage - Type guard functions could be more robust","description":"The type guard functions in types/storage.ts use simple property checks:\\n\\n```typescript\\nexport function isStreamable(backend: StorageBackend): backend is StreamableBackend {\\n  return 'createReadStream' in backend \u0026\u0026 'createWriteStream' in backend\\n}\\n\\nexport function isMultipart(backend: StorageBackend): backend is MultipartBackend {\\n  return 'createMultipartUpload' in backend\\n}\\n\\nexport function isTransactional(backend: StorageBackend): backend is TransactionalBackend {\\n  return 'beginTransaction' in backend\\n}\\n```\\n\\nIssues:\\n- Doesn't verify the property is a function\\n- Could be fooled by objects with these property names but different types\\n\\nRefactor to:\\n```typescript\\nexport function isStreamable(backend: StorageBackend): backend is StreamableBackend {\\n  return typeof (backend as StreamableBackend).createReadStream === 'function'\\n    \u0026\u0026 typeof (backend as StreamableBackend).createWriteStream === 'function'\\n}\\n```\\n\\nFiles affected:\\n- /Users/nathanclevenger/projects/parquedb/src/types/storage.ts","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:34:26.867976-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:00:12.871191-06:00","closed_at":"2026-02-03T11:00:12.871191-06:00","close_reason":"Closed"}
{"id":"parquedb-s6ld","title":"Add MCP tool input validation","description":"MCP server missing input validation for tool parameters - could cause runtime errors or security issues","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:09:40.615673-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:15:30.28987-06:00","closed_at":"2026-02-03T09:15:30.28987-06:00","close_reason":"Fixed by parallel subagents"}
{"id":"parquedb-s8yv","title":"Implement StorageRouter interface and routing logic","description":"## Context\nPart of typed storage implementation (parquedb-k7jj design).\n\n## Requirements\n\n1. Create `src/storage/router.ts`:\n   - `StorageRouter` interface\n   - `StorageRouterImpl` class\n   - `getStorageMode(collection)` - returns 'typed' or 'flexible'\n   - `getStoragePath(collection)` - returns correct file path\n   - Route writes/reads based on schema presence\n\n2. Integration points:\n   - Extract flexible collections from DB() schema input\n   - Extract `$options` from collection definitions\n   - Wire into ParqueDB constructor\n\n3. Testing:\n   - Unit tests for routing logic\n   - Tests for path generation\n   - Tests for mode detection\n\n## Acceptance Criteria\n- [ ] StorageRouter interface defined\n- [ ] RouterImpl correctly routes typed vs flexible collections\n- [ ] Path generation follows spec: `data/{collection}.parquet` vs `data/{ns}/data.parquet`\n- [ ] Tests pass\n\n## References\n- Design: docs/architecture/typed-storage.md","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:41:10.595919-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:11:30.209147-06:00","closed_at":"2026-02-03T07:11:30.209147-06:00","close_reason":"Implemented"}
{"id":"parquedb-sbcl","title":"Rate limiting and backpressure for compaction under extreme load","description":"Under extreme load, compaction could overwhelm the system. Implement: 1) Queue consumer rate limiting - limit DO dispatches per second, 2) Backpressure signal when windows accumulate, 3) Adaptive batch sizing based on load, 4) Circuit breaker for workflow creation failures, 5) Graceful degradation - skip low-priority namespaces under load. Consider using Cloudflare's rate limiting primitives or custom token bucket in DO.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:57:10.648298-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:13:42.387769-06:00","closed_at":"2026-02-03T14:13:42.387769-06:00","close_reason":"Closed"}
{"id":"parquedb-se6e","title":"Missing CLI Command Tests","description":"## Problem\nbranch, conflicts, schema CLI commands have no tests.\n\n## Fix\nAdd the following test files:\n- tests/unit/cli/branch.test.ts\n- tests/unit/cli/conflicts.test.ts\n- tests/unit/cli/schema.test.ts\n\n## Priority\nP1 - HIGH","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:10:32.050246-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T18:22:30.632504-06:00","closed_at":"2026-02-03T18:22:30.632504-06:00","close_reason":"Fixed in commit ab28519"}
{"id":"parquedb-sexn","title":"Fix VectorIndex memory leak in evictedNodeIds","description":"The evictedNodeIds Set in src/indexes/vector/hnsw.ts grows unbounded when nodes are evicted but never cleaned up on persistence. In long-running processes this causes memory leak.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T16:27:40.916025-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:29:07.110897-06:00","closed_at":"2026-02-03T16:29:07.110897-06:00","close_reason":"Closed"}
{"id":"parquedb-skfg","title":"Replace sleep() timing in tests with deterministic alternatives","description":"74 sleep()/setTimeout calls across 20 test files create flaky test risk. Use monotonic counters, injectable clocks, or vitest fake timers instead of wall-clock timing.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T07:16:04.180504-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T03:54:50.495269-06:00","closed_at":"2026-02-03T03:54:50.495269-06:00","close_reason":"Closed"}
{"id":"parquedb-sks9","title":"Refactor: core.ts - Consolidate duplicate ID normalization pattern","description":"In src/ParqueDB/core.ts, the ID normalization pattern is duplicated across multiple methods:\n\n```typescript\nconst fullId = id.includes('/') ? id : `${namespace}/${id}`\n```\n\nThis appears in:\n- get() line 333\n- update() line 695\n- delete() line 1168\n- restore() line 1273\n- getHistory() line 1313\n- getAtVersion() line 2241\n- getRelated() line 492\n\nThis should be extracted to a utility function in src/ParqueDB/validation.ts:\n```typescript\nexport function normalizeEntityId(namespace: string, id: string): EntityId {\n  return (id.includes('/') ? id : `${namespace}/${id}`) as EntityId\n}\n```\n\nBenefits:\n1. Single source of truth for ID normalization logic\n2. Easier to add additional normalization rules (e.g., validation)\n3. Improves code readability","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:32:48.641316-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:40:05.535372-06:00","closed_at":"2026-02-03T11:40:05.535372-06:00","close_reason":"Closed","labels":["duplication","refactor"]}
{"id":"parquedb-sqg0","title":"Consolidate entity state management (3 sources of truth)","description":"ParqueDBDO has three sources of truth: entities table, events_wal, pending_row_groups. skipEntityTableWrites flag creates conditional codepaths. Choose event-sourcing as single source of truth, implement proper entity reconstruction with snapshot checkpoints.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:38:19.936272-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:47:16.17818-06:00","closed_at":"2026-02-03T08:47:16.17818-06:00","close_reason":"Closed"}
{"id":"parquedb-sqjq","title":"Time Bucket Sharding Partial Implementation","description":"**Issue:** Code exists but not wired into main queue consumer flow\n\n**Fix:** Either complete integration or remove dead code","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T14:34:27.412181-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:56:35.876161-06:00","closed_at":"2026-02-03T14:56:35.876161-06:00","close_reason":"Closed"}
{"id":"parquedb-stdm","title":"Refactor: Worker - Remove legacy events table code from ParqueDBDO","description":"The ParqueDBDO has both new events_wal batching and legacy events table support. The legacy events table (lines ~284-298) is explicitly kept for backward compatibility. Once WAL migration is complete, this dead code can be removed.\n\nFiles:\n- src/worker/ParqueDBDO.ts\n\nCleanup:\n- Remove CREATE TABLE events statement\n- Remove idx_events_unflushed and idx_events_ns indexes\n- Remove any references to the legacy events table\n\nImpact: Reduces code complexity and SQLite row usage","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:32:52.427764-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:37:14.500808-06:00","closed_at":"2026-02-03T11:37:14.500808-06:00","close_reason":"Closed"}
{"id":"parquedb-stf4","title":"Extract magic number 32 to MAX_HNSW_LEVEL constant","description":"In src/indexes/vector/hnsw.ts line 617-619, the magic number 32 for max HNSW level should be extracted to src/constants.ts as MAX_HNSW_LEVEL.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T16:27:50.827054-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:29:34.46082-06:00","closed_at":"2026-02-03T16:29:34.46082-06:00","close_reason":"Closed"}
{"id":"parquedb-stx","title":"Write: Schema Definition Guide","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T12:40:45.694171-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:35:25.733184-06:00","closed_at":"2026-02-01T14:35:25.733184-06:00","close_reason":"Closed"}
{"id":"parquedb-svh2","title":"Add aggregation framework","description":"Essential for analytical use cases. Implement MongoDB-style aggregation pipeline with $group, $unwind, $project stages. Consider supporting $lookup for joins.","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:43.813552-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T16:35:42.907985-06:00","closed_at":"2026-02-01T16:35:42.907985-06:00","close_reason":"Closed"}
{"id":"parquedb-sylm","title":"RED: Add e2e tests for deployed dataset endpoints","description":"## Problem\nProduction endpoint /datasets/onet-graph/occupations returns Error 1101 (File not found: onet-graph/occupations.parquet).\n\nThe e2e tests don't catch this because they:\n1. Only test basic R2/DO operations\n2. Only test the root endpoint (/)\n3. Never hit /datasets/* endpoints with real data\n\nUnit tests mock the worker so they don't verify data actually exists in R2.\n\n## Red Test Requirements\nAdd failing e2e tests that:\n1. Hit each configured dataset endpoint and verify 200 response\n2. Test at least one collection per dataset (onet-graph/occupations, imdb/titles, etc.)\n3. Verify response contains actual data (not just valid JSON structure)\n4. Test with and without filters\n5. Test pagination (limit, skip, cursor)\n\n## Files to modify\n- tests/e2e/parquedb.workers.test.ts - add dataset endpoint tests\n- OR create new tests/e2e/datasets.test.ts\n\n## Expected Result\nTests should FAIL currently, exposing the production bug.","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T02:54:17.437168-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T03:10:37.497553-06:00","closed_at":"2026-02-03T03:10:37.497553-06:00","close_reason":"Closed"}
{"id":"parquedb-szbx","title":"Add input validation for remote API responses","description":"API responses in remote.ts and sync.ts are cast to expected types without validation. A malicious server could return unexpected data. Add validation using zod or manual type guards.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:04:57.571801-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:07:19.003698-06:00","closed_at":"2026-02-03T08:07:19.003698-06:00","close_reason":"Closed"}
{"id":"parquedb-szz","title":"[GREEN] Pagination implementation","description":"Implement pagination to pass tests","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:32.829795-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:11:19.224011-06:00","closed_at":"2026-02-01T14:11:19.224011-06:00","close_reason":"Closed"}
{"id":"parquedb-t0ce","title":"P2: Extract magic numbers to constants","description":"Hardcoded values like JWKS_CACHE_TTL (3600*1000), JWKS_FETCH_TIMEOUT_MS (10000) in auth.ts and other files should be configurable constants. Audit codebase for magic numbers and extract to config.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:11:00.159664-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:36:57.191629-06:00","closed_at":"2026-02-03T15:36:57.191629-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-t2v","title":"[RED] Filter evaluation tests - string/array operators","description":"Write failing tests for $regex, $startsWith, $all, $elemMatch, $size","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:09.882799-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:03:47.557046-06:00","closed_at":"2026-02-01T14:03:47.557046-06:00","close_reason":"Closed"}
{"id":"parquedb-t35m","title":"Use safe-regex in ParqueDB.ts matchesOperator","description":"The matchesOperator method in ParqueDB.ts creates RegExp directly from user input without using the safe-regex utility. While createSafeRegex exists in src/utils/safe-regex.ts and is used in query/filter.ts, ParqueDB.ts has its own inline implementation that doesn't use it. This creates a ReDoS vulnerability.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T17:00:44.754732-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T17:02:14.858463-06:00","closed_at":"2026-02-01T17:02:14.858463-06:00","close_reason":"Closed"}
{"id":"parquedb-t4bk","title":"P1: Add LRU limit to JWKS cache","description":"src/integrations/payload/auth.ts:211 - JWKS cache has no size limit, could exhaust memory. Implement LRU cache with max entries.","status":"open","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:01.652633-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:01.652633-06:00"}
{"id":"parquedb-t4ee","title":"Testing: Add missing mutation/delete.test.ts and mutation/update.test.ts","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:49:42.224542-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:57:57.22803-06:00","closed_at":"2026-02-03T12:57:57.22803-06:00","close_reason":"Closed"}
{"id":"parquedb-t51t","title":"Missing Workflow Completion Notification in Compaction Migration","description":"**File:** src/workflows/compaction-migration.ts\n\n**Issue:** Workflow completes but never notifies CompactionStateDO via /workflow-complete endpoint\n\n**Impact:** Windows remain in 'dispatched' state indefinitely, causing memory leaks and incorrect metrics\n\n**Fix:** Add final step to notify DO of completion\n\n**Severity:** CRITICAL","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T14:33:56.335094-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:30:07.70507-06:00","closed_at":"2026-02-03T15:30:07.70507-06:00","close_reason":"Fixed in commit 6947aa0"}
{"id":"parquedb-t7aj","title":"Add automated compaction scheduling","description":"Compaction endpoints exist but scheduling is not automated. Implement background compaction: trigger when file count exceeds threshold, run during low-traffic periods, expose metrics.","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T16:28:08.178091-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T17:16:44.987316-06:00","closed_at":"2026-02-03T17:16:44.987316-06:00","close_reason":"Closed"}
{"id":"parquedb-tc2v","title":"Single Queue Bottleneck - Partitioned Queues Not Deployed","description":"**File:** wrangler.jsonc\n\n**Issue:** Queue partitioning infrastructure exists but not deployed\n\n**Impact:** Limited to single queue consumer throughput\n\n**Fix:** Deploy partitioned queues for production","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T14:34:18.884548-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:54:05.766197-06:00","closed_at":"2026-02-03T14:54:05.766197-06:00","close_reason":"Added comprehensive documentation for deploying partitioned queues in docs/guides/compaction-workflow.md including setup instructions, wrangler.jsonc configuration examples, and cost analysis. Also added production environment configuration with 4-partition queue setup in wrangler.jsonc."}
{"id":"parquedb-tc4c","title":"P2: Replace process.env with platform-agnostic approach","description":"src/errors/index.ts:211 - Direct process.env access may fail in non-Node environments. Make configurable.","status":"open","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:15.092778-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:15.092778-06:00"}
{"id":"parquedb-tczr","title":"Split DBProviderExtended into smaller interfaces","description":"Architecture: DBProviderExtended is too large (1300+ lines). Split into composable mixins: DBCrud, DBRelationships, DBEvents, DBActions, DBSearch","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:09:52.612056-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:18:51.628856-06:00","closed_at":"2026-02-03T10:18:51.628856-06:00","close_reason":"Closed"}
{"id":"parquedb-tdxr","title":"P2: Fix transaction rollback relationship cleanup","description":"src/ParqueDB/core.ts:740-776 - Rollback doesn't clean up relationship index changes. Add unindex/reindex operations.","status":"open","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:07.385999-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:07.385999-06:00"}
{"id":"parquedb-thw3","title":"Type safety: flushEvents rollback uses Variant type unsafely","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:41:33.30764-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:54:06.764034-06:00","closed_at":"2026-02-03T12:54:06.764034-06:00","close_reason":"Closed"}
{"id":"parquedb-tmu","title":"[RED] Real FsBackend tests","description":"Write failing tests for FsBackend that use real Node fs. No mocks. Test CRUD, list, atomic writes.","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T14:29:58.767533-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-30T14:31:29.326162-06:00","closed_at":"2026-01-30T14:31:29.326162-06:00","close_reason":"Closed"}
{"id":"parquedb-tn4","title":"Epic: Real Integration Tests (No Mocks)","description":"Replace all mock storage backends with real tests. Tests must exercise actual storage: FsBackend (Node fs), R2Backend (real R2), FsxBackend (Workers), DOSqliteBackend (Durable Objects). Use vitest-pool-workers with remote:true for real bindings.","status":"closed","priority":0,"issue_type":"epic","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T14:29:50.975228-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-30T14:46:18.284762-06:00","closed_at":"2026-01-30T14:46:18.284762-06:00","close_reason":"Closed"}
{"id":"parquedb-tp7v","title":"Create wiki.org.ai monorepo package structure","description":"Set up the monorepo structure for wiki.org.ai npm packages:\n\nPackages to create:\n- @wiki.org.ai/types - Shared TypeScript types for Wikidata entities\n- @wiki.org.ai/ingest - Data ingestion scripts for Wikidata dump processing\n- @wiki.org.ai/client - ParqueDB client SDK for querying Wikidata\n- @wiki.org.ai/worker - Cloudflare Worker API handlers\n\nDirectory structure:\n```\nwiki.org.ai/\n packages/\n    types/\n    ingest/\n    client/\n    worker/\n scripts/\n data/ (gitignored)\n pnpm-workspace.yaml\n tsconfig.json\n```\n\nUse pnpm workspaces. Configure TypeScript project references.","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:17:28.749416-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:33:11.542144-06:00","closed_at":"2026-02-03T13:33:11.542144-06:00","close_reason":"Created wiki.org.ai monorepo with packages: types, ingest, client, worker"}
{"id":"parquedb-tpoh","title":"Implement writeToTargetFormat in CompactionMigrationWorkflow","description":"The writeToTargetFormat method in src/workflows/compaction-migration.ts:398-449 is a placeholder. Implement actual Parquet writing using hyparquet-writer. Must handle native, iceberg, and delta formats with correct path structures.","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:56:02.013067-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:08:03.562733-06:00","closed_at":"2026-02-03T13:08:03.562733-06:00","close_reason":"Closed"}
{"id":"parquedb-trx8","title":"Fix: Memory leak in StreamingRefreshEngine","description":"pendingEvents array can grow unbounded under high load. Add max buffer size with backpressure or oldest-event eviction. File: src/materialized-views/streaming.ts","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:15:15.239013-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:20:18.02417-06:00","closed_at":"2026-02-03T10:20:18.02417-06:00","close_reason":"Closed"}
{"id":"parquedb-ttd","title":"[GREEN] Schema parsing implementation","description":"Implement schema parsing to pass tests","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:59.704655-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:16:30.938595-06:00","closed_at":"2026-02-01T13:16:30.938595-06:00","close_reason":"Closed"}
{"id":"parquedb-tw7o","title":"Critical: Sync route token signing uses insecure base64 encoding","description":"In src/worker/sync-routes.ts:663-717, the signUploadToken and signDownloadToken functions use simple base64 encoding (btoa) instead of HMAC with a secret. This allows anyone to forge valid upload/download tokens by simply encoding the JSON payload. The code has a TODO comment acknowledging this: 'In production, use HMAC with env.SYNC_SECRET'.\n\nFix: Implement proper HMAC-SHA256 signing using crypto.subtle.sign() with a secret from environment variables.\n\nFile: /Users/nathanclevenger/projects/parquedb/src/worker/sync-routes.ts\nLines: 663-717","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:06.768346-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:47:44.463294-06:00","closed_at":"2026-02-03T11:47:44.463294-06:00","close_reason":"Fixed: Token signing now uses HMAC-SHA256 via crypto.subtle.sign with SYNC_SECRET env var"}
{"id":"parquedb-twbx","title":"Refactor: Consolidate MVLineage types","description":"MVLineage defined differently in types.ts, staleness.ts, and incremental.ts (as IncrementalLineage). Unify into single type with extension points for backend-specific metadata.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:15:38.09669-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:44:34.138361-06:00","closed_at":"2026-02-03T10:44:34.138361-06:00","close_reason":"Closed"}
{"id":"parquedb-tz46","title":"Implement consistent error handling/logging","description":"Error handling inconsistent across codebase:\n- Silent error swallowing in index loading (hash.ts:73, sst.ts:73)\n- console.log in production code (11 files)\n- No consistent logging strategy\n\nImplement logger interface, add error context, use proper error types.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T12:42:43.839849-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:07:24.852836-06:00","closed_at":"2026-02-01T13:07:24.852836-06:00","close_reason":"Completed by parallel agents"}
{"id":"parquedb-u17m","title":"Add resource cleanup to MCP server","description":"createParqueDBMCPServer creates resources but has no cleanup/dispose mechanism. If recreated multiple times, could lead to resource leaks. Add dispose() method.","status":"open","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:19.791635-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:19.791635-06:00"}
{"id":"parquedb-u1k3","title":"P4: Add JSDoc to internal helper functions","description":"src/mutation/operators.ts:525-560 - Helper functions like sortArray, applySlice lack documentation.","status":"open","priority":4,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:35.665785-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:35.665785-06:00"}
{"id":"parquedb-u4sq","title":"TypeScript: Add stricter generics to ParqueDBDOStub interface","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:45:54.025767-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:18:41.607155-06:00","closed_at":"2026-02-03T13:18:41.607155-06:00","close_reason":"Closed"}
{"id":"parquedb-u4u9","title":"Architecture: Workers memory optimization for large queries","description":"Workers have 128MB memory limit and 30s CPU time limit. Large find() queries currently load all matching entities into memory before filtering/pagination. For queries returning 10K+ entities or complex aggregations, this risks hitting memory limits. Consider: (1) Streaming Parquet parsing with hyparquet, (2) Cursor-based pagination at storage level, (3) Moving large aggregations to D1 or scheduled workers.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:45:55.221456-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:57:41.174177-06:00","closed_at":"2026-02-03T14:57:41.174177-06:00","close_reason":"Closed"}
{"id":"parquedb-u5lp","title":"Add $options support to collection schema definitions","description":"## Context\nPart of typed storage implementation (parquedb-k7jj design).\n\n## Requirements\n\n1. Extend `src/db.ts` schema parsing:\n   - Extract `$options` from collection definitions\n   - Support: includeDataVariant, compression, rowGroupSize, sortBy\n   - Validate option values\n\n2. Type definitions:\n   - Add `CollectionOptions` interface to types\n   - Update `CollectionSchemaWithLayout` to include $options\n\n3. Integration:\n   - Pass options to StorageRouter\n   - Apply options during write operations\n   - Document in API docs\n\n4. Testing:\n   - Unit tests for options parsing\n   - Tests for default values\n   - Tests for invalid options\n\n## Configuration API:\n```typescript\nconst db = DB({\n  Occupation: {\n    $options: {\n      includeDataVariant: true,     // default\n      compression: 'lz4',           // default\n      rowGroupSize: 50000,          // default\n      sortBy: ['$id'],             // default\n    },\n    name: 'string!',\n    socCode: 'string!#',\n  },\n})\n```\n\n## Acceptance Criteria\n- [ ] $options parsed correctly from schema\n- [ ] Default values applied when not specified\n- [ ] Options passed through to writer\n- [ ] TypeScript types updated\n- [ ] Tests pass\n\n## References\n- Design: docs/architecture/typed-storage.md\n- Depends on: parquedb-s8yv (StorageRouter)","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:41:48.463784-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:11:30.213613-06:00","closed_at":"2026-02-03T07:11:30.213613-06:00","close_reason":"Implemented"}
{"id":"parquedb-u5oz","title":"TypeScript: Add type guards for R2Bucket workflow casts","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:45:55.659716-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:18:35.688151-06:00","closed_at":"2026-02-03T13:18:35.688151-06:00","close_reason":"Closed"}
{"id":"parquedb-u6j2","title":"Implement vacuum workflow for orphaned files","description":"Orphaned files from failed commits (manifests, data files) are logged but not cleaned up. Create a vacuum/cleanup workflow that: 1) Scans for unreferenced data files, 2) Removes orphaned manifests not in any snapshot, 3) Cleans up failed partial commits. Should run periodically or on-demand.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:33:20.425676-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:49:52.744611-06:00","closed_at":"2026-02-03T13:49:52.744611-06:00","close_reason":"Closed"}
{"id":"parquedb-u7nu","title":"Optimize tail worker: raw logging + hibernatable WebSockets","description":"Redesign tail worker to use existing WorkerLogsMV Parquet streaming.\n\n## Current Problems\n- Transforms/filters events (loses data)\n- Doesn't use existing WorkerLogsMV infrastructure\n- No connection reuse to DO\n\n## Existing Infrastructure\n- `src/streaming/worker-logs.ts` - WorkerLogsMV for Parquet storage\n- `ingestTailEvent(tailEvent)` - Already handles tail events\n- Parquet schema with proper types\n- Flush thresholds and intervals\n\n## Proposed Architecture\n\n```\nTail Worker (edge)\n Raw events (no transformation)\n Add metadata (instanceId, startTime)\n Debounce (100ms / 50 events)\n Persistent WebSocket connection\n         \nDurable Object (hibernatable WS)\n Receive via WS\n Call workerLogsMV.ingestTailEvent()\n Hibernate between messages\n MV handles flush to Parquet\n         \nWorkerLogsMV (existing)\n Buffer events\n Flush to Parquet on threshold\n R2 storage\n```\n\n## Tasks\n\n1. **Simplify tail worker to raw forwarding**\n   - Remove all transformation/filtering\n   - Add metadata envelope\n   - Forward raw events via WebSocket\n\n2. **DO with hibernatable WebSocket**\n   - Accept WS connections from tail workers\n   - On message: call workerLogsMV.ingestTailEvent()\n   - Hibernate between messages\n\n3. **Use existing WorkerLogsMV**\n   - Already has Parquet schema\n   - Already handles batching/flushing\n   - Just need to wire it up\n\n## Cost Savings\n- Hibernatable WS: DO only charged when processing\n- Batching: Fewer DO wakeups\n- Reuse existing MV infrastructure","notes":"Implemented TailDO with hibernatable WebSocket:\n\n## Files Created:\n- src/worker/TailDO.ts - Durable Object with hibernatable WebSocket API\n- src/worker/tail-streaming.ts - Simplified tail worker that streams to TailDO\n- examples/tail-worker/wrangler.toml - Example wrangler configuration\n- tests/unit/worker/tail-do.test.ts - Unit tests for message types and config\n\n## Architecture:\nTail events -\u003e Tail Worker (validate, batch) -\u003e WebSocket -\u003e TailDO -\u003e WorkerLogsMV -\u003e Parquet in R2\n\n## Features:\n- TailDO uses hibernatable WebSocket API for cost savings\n- Local batching in tail worker before sending to DO\n- WorkerLogsMV handles buffering and Parquet flush\n- Supports 'global' and 'hourly' DO ID strategies\n- Connection metadata tracking for debugging\n\n## Exports Added to src/worker/index.ts:\n- TailDO, TailDOEnv, TailWorkerMessage, TailAckMessage, TailErrorMessage\n- createStreamingTailHandler, DEFAULT_STREAMING_CONFIG, StreamingTailEnv, StreamingTailConfig, TailMessage","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:57:34.627073-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:06:01.48221-06:00","closed_at":"2026-02-03T11:06:01.48221-06:00","close_reason":"Closed"}
{"id":"parquedb-u9oc","title":"Consolidate duplicate StreamingRefreshEngine implementations","description":"Two implementations exist: materialized-views/streaming.ts (574 lines, full-featured) and streaming/engine.ts (307 lines, simpler). Different defaults, features, and patterns. Consolidate to single implementation and re-export.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:02:14.944995-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:06:59.348709-06:00","closed_at":"2026-02-03T11:06:59.348709-06:00","close_reason":"Consolidated StreamingRefreshEngine implementations: src/streaming/engine.ts and src/streaming/types.ts now re-export from src/materialized-views/streaming.ts (the canonical implementation)"}
{"id":"parquedb-uavt","title":"Implement actual grace period sleep in workflow","description":"The grace-period step in compaction-migration.ts:212-223 has a comment 'we just log and continue' instead of actually using step.sleep(). This defeats the purpose of waiting for late writers. Implement proper step.sleep() for the grace period.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:56:16.561041-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:58:18.214149-06:00","closed_at":"2026-02-03T12:58:18.214149-06:00","close_reason":"Closed"}
{"id":"parquedb-ucr8","title":"Edit: Update AI SDK docs with MV examples","description":"Update docs/integrations/ai/vercel-ai-sdk.md and related files to include examples of AIRequests/Generations stream collections and DailyAIUsage MV patterns","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:28:21.494317-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:29:23.353586-06:00","closed_at":"2026-02-03T09:29:23.353586-06:00","close_reason":"Closed"}
{"id":"parquedb-ud4j","title":"Standardize Transaction Patterns: Unified transaction abstraction","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:57:42.145633-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:13:18.696819-06:00","closed_at":"2026-02-03T09:13:18.696819-06:00","close_reason":"Closed"}
{"id":"parquedb-uf0n","title":"Add mutation operator test coverage","description":"Mutation operations have sparse test coverage for update/delete operators.\n\nAdd comprehensive tests for:\n- All update operators ($set, $unset, $inc, $push, $pull, $addToSet, $pop, $rename)\n- Operator combinations\n- Edge cases (empty arrays, null values, nested paths)\n- Error conditions (invalid paths, type mismatches)\n- Delete cascading behavior","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:31:59.961149-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:06:33.339803-06:00","closed_at":"2026-02-03T10:06:33.339803-06:00","close_reason":"Closed"}
{"id":"parquedb-uf0x","title":"DeltaSnapshotBackend uses generic Error instead of typed errors","description":"**High: Missing Typed Errors**\n\nIn /src/backends/delta.ts lines 1976-1993, DeltaSnapshotBackend throws generic Error:\n\n- Line 1948-1949: `throw new Error('Snapshot backend only supports namespace: ...')`\n- Line 1976-1993: All write operations throw `throw new Error('Snapshot backend is read-only')`\n\n**Impact**: Cannot programmatically distinguish error types.\n\n**Fix**: Use InvalidNamespaceError and ReadOnlyError from types.ts","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:50.847566-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:08:10.433363-06:00","closed_at":"2026-02-03T13:08:10.433363-06:00","close_reason":"Closed"}
{"id":"parquedb-ufux","title":"Implement Result\u003cT,E\u003e error handling pattern","description":"All errors are thrown rather than returned. Implement functional error handling:\n\ntype Result\u003cT, E = Error\u003e = { ok: true; value: T } | { ok: false; error: E }\n\nReplace throw-based patterns in critical paths with Result returns for better type safety.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T12:42:30.482725-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:07:24.793615-06:00","closed_at":"2026-02-01T13:07:24.793615-06:00","close_reason":"Completed by parallel agents"}
{"id":"parquedb-ug56","title":"Add Express/Fastify integration tests","description":"The new Express and Fastify adapters in src/integrations/express/ and src/integrations/fastify/ have no test coverage. Add comprehensive integration tests.","status":"open","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:13.483234-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:13.483234-06:00"}
{"id":"parquedb-ugdb","title":"Shard CompactionStateDO by namespace","description":"CompactionStateDO uses single global instance (idFromName('default')) which is a scalability bottleneck. Should shard by namespace: idFromName(namespace). Also consider per-window storage instead of single JSON blob for better memory characteristics.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:56:14.47825-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:13:50.764365-06:00","closed_at":"2026-02-03T13:13:50.764365-06:00","close_reason":"Closed"}
{"id":"parquedb-uhf9","title":"Add prototype pollution guards to query/update.ts and Collection.ts","description":"query/update.ts setField/unsetField and Collection.ts setNestedValue/deleteNestedValue lack prototype pollution checks (__proto__, constructor, prototype). mutation/operators.ts has validatePath/isUnsafePath - extract to shared utility and apply everywhere.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T07:16:01.304264-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T03:41:10.60434-06:00","closed_at":"2026-02-03T03:41:10.60434-06:00","close_reason":"Closed"}
{"id":"parquedb-ujk1","title":"Refactor: core.ts getEventLog() - Extract to dedicated class","description":"In src/ParqueDB/core.ts, getEventLog() (lines 2039-2155) returns an inline object literal with ~115 lines of implementation. Similar to the SnapshotManager, this should be extracted to a dedicated class.\n\nThe event log has several methods that deserve their own tests:\n- getEvents() - filters and sorts events by entity\n- getEventsByTimeRange() - complex boundary handling logic (lines 2073-2116)\n- getEventsByOp() - filters by operation type\n- archiveEvents() - rotation logic\n\nExtract to:\n```typescript\n// src/ParqueDB/EventLogImpl.ts\nexport class EventLogImpl implements EventLog {\n  constructor(\n    private events: Event[],\n    private archivedEvents: Event[],\n    private config: EventLogConfig\n  ) {}\n  // ...\n}\n```\n\nBenefits:\n1. Unit test event log logic in isolation\n2. Reduce core.ts complexity\n3. Cleaner interface boundaries","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:33:11.074013-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:22:11.856934-06:00","closed_at":"2026-02-03T11:22:11.856934-06:00","close_reason":"Extracted getEventLog() to dedicated EventLogImpl class. The inline object in core.ts has been replaced with a new EventLogImpl class in events.ts that implements the EventLog interface.","labels":["complexity","refactor"]}
{"id":"parquedb-ulvm","title":"Connect tail worker to search worker for CPU measurement","description":"Enable tail_consumers in snippets/worker/wrangler.toml to capture:\n- Real CPU time (not wall clock)\n- Subrequest count\n- Memory usage\n- Outcome (ok/exceededCpu/exceededMemory)\n\nThis is required to verify we're under Snippet limits before deploying as Snippet.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:16:12.54895-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:37:51.878187-06:00","closed_at":"2026-02-03T12:37:51.878187-06:00","close_reason":"Closed"}
{"id":"parquedb-uqe4","title":"GREEN: Restore/upload missing dataset parquet files to R2","description":"## Problem\nProduction error: File not found: onet-graph/occupations.parquet\n\nAll dataset collections are returning 1101 errors:\n- /datasets/onet-graph/occupations\n- /datasets/imdb/titles  \n- /datasets/unspsc/segments\n\nThe parquet files are either:\n1. Never uploaded to R2\n2. Accidentally deleted\n3. In wrong bucket/path\n\n## Investigation needed\n1. Check R2 bucket contents - what files exist?\n2. Check deployment history - when did files disappear?\n3. Check data pipeline - is there a script to generate/upload these?\n4. Check if data exists locally that needs uploading\n\n## Files involved\n- src/worker/datasets.ts - dataset config expects files at {prefix}/{collection}.parquet\n- R2 bucket: needs onet-graph/occupations.parquet, imdb/titles.parquet, etc.\n\n## Acceptance criteria\n- All configured datasets have their parquet files in R2\n- /datasets/onet-graph/occupations returns 200 with data\n- /datasets/imdb/titles returns 200 with data\n- E2e tests from RED issue pass","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T02:54:27.217494-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T03:17:21.044943-06:00","closed_at":"2026-02-03T03:17:21.044943-06:00","close_reason":"Restored onet-graph and unspsc datasets to R2. onet-graph 4/4 and unspsc 4/4 collections working. imdb 2/5 working (sample data). Remaining issues: imdb ratings/principals/crew, onet-optimized, wikidata need separate tickets.","dependencies":[{"issue_id":"parquedb-uqe4","depends_on_id":"parquedb-sylm","type":"blocks","created_at":"2026-02-03T02:54:47.090573-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-uqpb","title":"Refactor: Worker - Add error handling for missing R2 buckets","description":"In QueryExecutor and ParqueDBWorker, optional R2 buckets (CDN_BUCKET, CDN_R2_DEV_URL) are handled but primary BUCKET access lacks robust error handling.\n\nFiles:\n- src/worker/QueryExecutor.ts\n- src/worker/index.ts\n\nIssues identified:\n1. CdnR2StorageAdapter.loadWholeFile throws generic 'Object not found' error\n2. No distinction between network errors and missing files in some paths\n3. ParqueDBWorker.create/update/delete don't handle DO unavailable scenarios\n\nAdd:\n1. Specific error types for R2 connection issues vs missing files\n2. Graceful degradation when CDN bucket unavailable (fallback already exists but needs logging)\n3. Circuit breaker for repeated R2 failures\n\nImpact: Better operational debugging, more graceful failure modes","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:33:51.192728-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:08:57.119574-06:00","closed_at":"2026-02-03T11:08:57.119574-06:00","close_reason":"Closed"}
{"id":"parquedb-usdw","title":"Implement relationship indexes (forward/reverse) for wiki.org.ai","description":"Create relationship traversal indexes for Wikidata claims:\n\nForward indexes (subject -\u003e predicate -\u003e object):\n- P31 (instance of) - 113M rows\n- P279 (subclass of) - 10M rows\n- P17 (country) - 30M rows\n- P131 (admin territory) - 50M rows\n- P361 (part of) - 20M rows\n\nReverse indexes (object \u003c- predicate \u003c- subject):\n- Same properties, grouped by object\n\nUse cases:\n- 'Find all humans' (P31=Q5)\n- 'What is X an instance of?'\n- 'Find all items in country Y'\n\nTarget: ~10-15GB total, ~2,000 files","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:18:20.123878-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:56:52.785544-06:00","closed_at":"2026-02-03T13:56:52.785544-06:00","close_reason":"Implemented forward/reverse relationship indexes for P31, P279, P17, P131, and family properties"}
{"id":"parquedb-ut3r","title":"Add tests for src/studio/context.ts (0% coverage)","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:29.044503-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:06:41.59349-06:00","closed_at":"2026-02-03T09:06:41.59349-06:00","close_reason":"Closed"}
{"id":"parquedb-ut5m","title":"P0: Fix failing test suite","description":"Product review identified 568 failing tests. This is a critical blocker for beta release. Need to investigate and fix all failing tests to restore CI health.","status":"in_progress","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:10:16.233942-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T18:54:45.325205-06:00"}
{"id":"parquedb-uta6","title":"Add stricter typing to aggregation executor","description":"aggregation/executor.ts uses 'as any' cast and loose type guards with Record\u003cstring, unknown\u003e. Use discriminated unions or branded types for aggregation operators.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T17:01:03.633783-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T17:14:54.72836-06:00","closed_at":"2026-02-01T17:14:54.72836-06:00","close_reason":"Closed"}
{"id":"parquedb-uvie","title":"Fix 6 event sourcing test failures","description":"6 tests failing in event sourcing edge cases: version tracking in events, rapid sequential update ordering, relationship change event ordering. These block 1.0 release. Run npm test to identify and fix.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T17:00:50.549797-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T17:07:49.50068-06:00","closed_at":"2026-02-01T17:07:49.50068-06:00","close_reason":"Closed"}
{"id":"parquedb-uvpe","title":"ARCH: Add distributed locking for concurrent merges","description":"No locking if multiple processes try to merge simultaneously. Could cause data corruption. Need mutex/lock mechanism for merge operations.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:54:43.501512-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:07:47.050684-06:00","closed_at":"2026-02-03T09:07:47.050684-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-v0bg","title":"Add tests for query/bloom.ts integration","description":"src/query/bloom.ts (612 lines) handles bloom filter query planning and row group pruning but has no dedicated tests. The BloomFilter data structure is tested but the query-level integration is not.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T07:16:03.379899-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T07:23:52.360123-06:00","closed_at":"2026-02-02T07:23:52.360123-06:00","close_reason":"Closed"}
{"id":"parquedb-v5mo","title":"Add ParqueDBDO unit tests","description":"ParqueDBDO.ts is only tested through e2e tests. Create unit tests independent of full Worker environment to test: SQLite schema initialization, entity CRUD operations, event logging, flush pipeline, optimistic concurrency.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:28.566847-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T16:02:35.857328-06:00","closed_at":"2026-02-01T16:02:35.857328-06:00","close_reason":"Created comprehensive unit tests for ParqueDBDO covering: SQLite schema initialization, Entity CRUD operations, Event logging, Optimistic concurrency, and Relationship operations (link/unlink)"}
{"id":"parquedb-v5pd","title":"Implement Delta checkpoint files for faster reads","description":"Delta Lake checkpoint files (_delta_log/_last_checkpoint) speed up reads by avoiding scanning all log files. Implement: 1) Create checkpoint every N commits (e.g., 10), 2) Checkpoint contains aggregated state (all active add actions), 3) Update _last_checkpoint pointer, 4) Readers can start from checkpoint instead of version 0. This is an optimization - not blocking production but improves read performance significantly for tables with many commits.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:57:07.065284-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:19:37.932029-06:00","closed_at":"2026-02-03T14:19:37.932029-06:00","close_reason":"Closed"}
{"id":"parquedb-v7bv","title":"Deprecated Types Still in Use","description":"## Problem\nDOCreateOptions, DOUpdateOptions, DODeleteOptions are deprecated but still being used.\n\n## Location\n- src/worker/ParqueDBDO.ts (lines 87-132)\n\n## Fix\nComplete migration to canonical types.\n\n## Priority\nP1 - HIGH","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:10:27.443401-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T18:22:30.543981-06:00","closed_at":"2026-02-03T18:22:30.543981-06:00","close_reason":"Fixed in commit ab28519"}
{"id":"parquedb-v8eu","title":"[CRITICAL] Fix SQL injection vulnerability in search","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:24.858364-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:34:48.351359-06:00","closed_at":"2026-02-03T08:34:48.351359-06:00","close_reason":"Closed"}
{"id":"parquedb-vb08","title":"P0: Fix ReDoS risk in regex filter evaluation","description":"src/query/filter.ts:436-448 - createSafeRegex may not cover all edge cases. Add execution timeout for regex and audit safety.","status":"open","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:17:59.029221-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:17:59.029221-06:00"}
{"id":"parquedb-vf2t","title":"Race Condition in StreamingRefreshEngine Buffer Warning","description":"**File:** src/materialized-views/streaming.ts (lines 312-319)\n\n**Issue:** The _warningEmitted80 flag uses a check-then-act pattern without synchronization:\n```typescript\nif (!this._warningEmitted80) {\n  this._warningEmitted80 = true;\n  // emit warning\n}\n```\n\n**Impact:** In high-concurrency scenarios, the warning could be emitted multiple times or never due to the race condition between checking and setting the flag.\n\n**Fix:** Use atomic operations or a synchronized block to ensure thread-safe warning emission.\n\n**Priority:** P0 CRITICAL","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:10:11.890752-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T17:16:11.488482-06:00","closed_at":"2026-02-03T17:16:11.488482-06:00","close_reason":"Closed"}
{"id":"parquedb-vfo","title":"[GREEN] Populate implementation","description":"Implement populate to pass tests","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:42.858872-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:14:51.688443-06:00","closed_at":"2026-02-01T14:14:51.688443-06:00","close_reason":"Closed"}
{"id":"parquedb-vgur","title":"Add explicit return types to public functions","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:33.135957-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:00:13.808061-06:00","closed_at":"2026-02-03T11:00:13.808061-06:00","close_reason":"Closed"}
{"id":"parquedb-vhif","title":"Create snippets/ folder with Cloudflare Snippets examples","description":"Create a snippets/ folder with ParqueDB-powered Cloudflare Snippets.\n\n## Cloudflare Snippets Constraints\n- 32KB memory\n- 5ms CPU time\n- 5 subrequests\n- No Durable Objects\n- No KV/R2 bindings (use static assets or subrequests)\n\n## Folder Structure\n```\nsnippets/\n README.md                    # Documentation\n build.ts                     # Build script with minification\n package.json                 # Snippet-specific deps\n wrangler.toml               # Wrangler config for dry-run\n examples/\n    product-lookup/         # Simple ID lookup\n       snippet.ts\n       index.json          # Pre-built lookup index\n    category-filter/        # Filter by category\n       snippet.ts\n       partitions/\n    cascading-query/        # Multi-snippet fan-out\n       router.ts\n       partition.ts\n    static-mv/              # Pre-computed MV query\n        snippet.ts\n lib/\n     parquet-tiny.ts         # Minimal parquet reader\n     filter.ts               # Simple filter evaluation\n     types.ts                # Shared types\n```\n\n## Build \u0026 Test Process\n1. `bun run build` - Build and minify snippets\n2. `wrangler deploy --dry-run` - Check bundle size\n3. Upload to Cloudflare Snippets API\n4. Test CPU/subrequest limits\n\n## Success Criteria\n- [ ] Each snippet \u003c 1MB (Snippets limit)\n- [ ] CPU \u003c 5ms for simple queries\n- [ ] Memory \u003c 32KB during execution\n- [ ] Subrequests \u003c= 5 per request","status":"closed","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:29:16.256512-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:40:12.494882-06:00","closed_at":"2026-02-03T10:40:12.494882-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-vhif","depends_on_id":"parquedb-18nz","type":"blocks","created_at":"2026-02-03T10:29:25.365328-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-vlre","title":"Add tests for src/studio/database.ts (0% coverage)","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:29.77016-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:06:37.917721-06:00","closed_at":"2026-02-03T09:06:37.917721-06:00","close_reason":"Closed"}
{"id":"parquedb-vn8","title":"Write: Query API (filters, operators)","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T12:40:38.949433-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:35:37.568457-06:00","closed_at":"2026-02-01T14:35:37.568457-06:00","close_reason":"Closed"}
{"id":"parquedb-vnhb","title":"Refactor: Query - Improve null/undefined edge case handling","description":"## Summary\nWhile null/undefined handling is documented and mostly consistent, there are some edge cases and potential inconsistencies.\n\n### Current Behavior (as documented in filter.ts)\n- $eq: null and undefined are equivalent\n- $ne: null and undefined are equivalent  \n- $gt/$gte/$lt/$lte: Return false for null/undefined\n- $exists: Distinguishes undefined (missing) from null (present but null)\n- $type: Both null and undefined return 'null' type\n\n### Edge Cases to Verify/Fix\n\n1. **$elemMatch with null elements** (line 296-300):\n   - What happens when array contains null elements?\n   - Filter uses `matchesFilter(elem, subFilter)` which returns false for null\n\n2. **Nested path access through null** (comparison.ts line 161):\n   - `getNestedValue` returns undefined for path through null\n   - Should this distinguish 'not found' from 'found null'?\n\n3. **Array operators with null** (line 289-294):\n   - $all: Does it match if value is null? (Currently returns false)\n   - $size: Does it match if value is null? (Currently returns false)\n\n4. **Primitive row handling** (line 61-101):\n   - When row is a primitive, operators work but no null checks documented\n\n### Files to Update\n- `src/query/filter.ts` - Add edge case handling\n- `src/utils/comparison.ts` - Document behavior\n- Tests: Add explicit null edge case tests\n\n### Acceptance Criteria\n- [ ] Document all null/undefined behaviors in JSDoc\n- [ ] Add tests for $elemMatch with null array elements\n- [ ] Add tests for nested path through null\n- [ ] Verify array operator null handling matches MongoDB\n- [ ] Add primitive value null handling tests","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:33:14.699708-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:30:37.264383-06:00","closed_at":"2026-02-03T11:30:37.264383-06:00","close_reason":"Closed","labels":["refactor"]}
{"id":"parquedb-vpf","title":"Implement update operators","description":"MongoDB-style update operators ($set, $inc, $push, etc.)","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:19.196293-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:08:21.593295-06:00","closed_at":"2026-02-01T14:08:21.593295-06:00","close_reason":"Closed"}
{"id":"parquedb-vq7i","title":"DO WAL: Phase 2 - Bulk bypass to R2","description":"## TDD Task: Bulk operations bypass SQLite, stream to R2\n\n### Context\nFor 5+ entities, it's cheaper to write directly to R2 than buffer in SQLite. Bulk creates/updates should stream to pending Parquet files.\n\n### Red (Write failing tests first)\n1. Test bulk create (5+ entities) writes to R2, not SQLite\n2. Test pending row group metadata recorded (1 SQLite row)\n3. Test reads include pending row groups\n4. Test flush promotes pending to committed\n\n### Green (Implement)\n1. Add threshold check in create/update/upsertMany\n2. Stream to `data/{ns}/pending/{ulid}.parquet`\n3. Add `pending_row_groups` SQLite table (metadata only)\n4. Update QueryExecutor to merge pending files\n\n### Files\n- `src/worker/ParqueDBDO.ts` - bulk path\n- `src/worker/QueryExecutor.ts` - merge logic\n- `src/parquet/writer.ts` - streaming writes","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:07:31.186988-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T06:37:00.090414-06:00","closed_at":"2026-02-03T06:37:00.090414-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-vq7i","depends_on_id":"parquedb-w48p","type":"blocks","created_at":"2026-02-03T06:07:44.77406-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-vqik","title":"Missing Storage Failure Tests for Compaction Workflows","description":"**Issue:** No tests for R2 read/write failures, network timeouts, partial writes\n\n**Impact:** Unknown behavior under storage failures\n\n**Fix:** Add tests/unit/workflows/storage-failure.test.ts covering:\n- R2 read failures\n- R2 write failures\n- Network timeouts\n- Partial writes","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T14:34:08.55268-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:41:44.566584-06:00","closed_at":"2026-02-03T14:41:44.566584-06:00","close_reason":"Closed"}
{"id":"parquedb-vr4n","title":"Fix phantom generics on Entity\u003cTData\u003e - TData is unused","description":"Entity\u003cTData\u003e, CreateInput\u003cT\u003e, FindOptions\u003cT\u003e declare generic params that are never used in the interface body due to [key: string]: unknown index signature. Entity\u003cPost\u003e provides no more type safety than Entity\u003cany\u003e. Either put data under a data: TData property or remove phantom generics.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T07:16:02.427737-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T03:37:52.365554-06:00","closed_at":"2026-02-03T03:37:52.365554-06:00","close_reason":"Closed"}
{"id":"parquedb-vtuw","title":"[FEATURE] Enforce schema validation at runtime","description":"Schema inference exists in src/schema/parser.ts but validation is not enforced. Need to:\n\n1. Add validateOnWrite option to Collection\n2. Implement runtime validation against inferred/declared schema\n3. Add schema evolution/migration support\n4. Consider strict vs permissive modes\n5. Add helpful error messages for validation failures","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T13:36:05.138775-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:46:29.741121-06:00","closed_at":"2026-02-01T14:46:29.741121-06:00","close_reason":"Closed"}
{"id":"parquedb-vx9d","title":"Fix MV define.test.ts validation tests","description":"tests/unit/materialized-views/define.test.ts has 17 failing tests for validation: INVALID_FROM, INVALID_EXPAND, INVALID_FILTER, INVALID_GROUP_BY, INVALID_COMPUTE, INVALID_TYPE, INVALID_FIELDS, INVALID_FIELD_TYPE, INVALID_INGEST_SOURCE, and CollectionDefinitionError.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:56:43.03484-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:38:21.8208-06:00","closed_at":"2026-02-03T13:38:21.8208-06:00","close_reason":"Closed"}
{"id":"parquedb-vzou","title":"Concurrent Write Tests: Test OCC under load","description":"Test OCC (Optimistic Concurrency Control) under load:\n- Multiple writers to same table\n- Conflict detection and retry behavior\n- Performance under contention","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:57:47.950859-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:09:06.148994-06:00","closed_at":"2026-02-03T09:09:06.148994-06:00","close_reason":"Closed"}
{"id":"parquedb-w17b","title":"Consolidate duplicate CORS handling","description":"CORS handling is duplicated across worker routes. Extract into shared middleware/utility.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:15:10.535453-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T17:30:44.475485-06:00","closed_at":"2026-02-03T17:30:44.475485-06:00","close_reason":"Created shared CORS utility at src/worker/cors.ts with corsHeaders constants (BASIC_CORS_HEADERS, PUBLIC_CORS_HEADERS, AUTHENTICATED_CORS_HEADERS, SYNC_CORS_HEADERS, PREFLIGHT_CORS_HEADERS) and handleCors middleware. The utility is ready for use by other routes."}
{"id":"parquedb-w2wz","title":"TypeScript: Replace Function type with specific signature","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:45:54.784906-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:50:47.590754-06:00","closed_at":"2026-02-03T16:50:47.590754-06:00","close_reason":"Closed"}
{"id":"parquedb-w3bc","title":"P2: Fix cookie parsing for RFC 6265 compliance","description":"src/integrations/payload/auth.ts:192-204 - Simple cookie parsing doesn't handle URL-encoded values or quoted strings properly.","status":"open","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:05.253006-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:05.253006-06:00"}
{"id":"parquedb-w48p","title":"DO WAL: Phase 1 - Batched event storage","description":"## TDD Task: Implement batched event storage in DO\n\n### Context\nCurrently each event = 1 SQLite row. Use the existing `SqliteWal` pattern from `src/events/sqlite-wal.ts` to batch events into blobs.\n\n### Red (Write failing tests first)\n1. Test that events are batched (not 1 row per event)\n2. Test batch threshold (e.g., 100 events or 64KB)\n3. Test reading back batched events\n4. Test flush triggers batch write\n\n### Green (Implement)\n1. Add `events_wal` table with blob storage\n2. Buffer events in memory until threshold\n3. Write batch as single blob row\n4. Update `appendEvent()` to use batching\n\n### Refactor\n- Remove old per-event `events` table usage\n- Ensure backward compat during migration\n\n### Files\n- `src/worker/ParqueDBDO.ts` - main changes\n- `src/events/sqlite-wal.ts` - reuse pattern\n- `tests/unit/worker/ParqueDBDO.test.ts` - new tests","notes":"## Updated Requirements\n\n### Counter for Sqid IDs\nStore counter in event batch metadata (zero extra writes):\n```sql\nCREATE TABLE events_wal (\n  id INTEGER PRIMARY KEY,\n  ns TEXT,\n  first_seq INTEGER,  -- first counter in batch\n  last_seq INTEGER,   -- last counter in batch  \n  events BLOB,\n  created_at TEXT\n)\n```\n\nOn startup: `SELECT MAX(last_seq) FROM events_wal WHERE ns = ?`\nID generation: `sqids.encode([counter])`  'Uk', '86u', 'RHEA'\n\n### Testing\nUse vitest-pool-workers with REAL DO SQLite, NOT mocks.\nTests go in `tests/e2e/worker/` to run in Workers runtime.\n\n### Files\n- Remove/ignore `tests/unit/worker/ParqueDBDO-wal.test.ts` (has mocks)\n- Create `tests/e2e/worker/do-wal.test.ts` instead","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:07:26.511173-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T06:27:04.288283-06:00","closed_at":"2026-02-03T06:27:04.288283-06:00","close_reason":"Implemented: Sqids IDs, events_wal batching, counter persistence, 12 e2e tests"}
{"id":"parquedb-w7wz","title":"Studio: Implement full keyboard navigation","status":"closed","priority":3,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:37.471931-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:56:25.938493-06:00","closed_at":"2026-02-03T16:56:25.938493-06:00","close_reason":"Closed"}
{"id":"parquedb-w865","title":"Fix in-place entity mutation in ParqueDBImpl.update()","description":"ParqueDB/core.ts update() mutates entity objects directly in the shared entities Map. Concurrent async updates will overwrite each other (lost update). Clone entity before mutation like query/update.ts does.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T07:16:01.508567-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T07:22:09.84826-06:00","closed_at":"2026-02-02T07:22:09.84826-06:00","close_reason":"Closed"}
{"id":"parquedb-w8d1","title":"Bug: Transaction rollback uses wrong event target format","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:36.430245-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:06:53.161017-06:00","closed_at":"2026-02-03T12:06:53.161017-06:00","close_reason":"Fixed: Added comprehensive tests verifying event target format validation. The validation in EventWalManager.appendEvent() was already in place to reject events with wrong format (ns/id instead of ns:id). Created tests/unit/transaction-rollback-target-format.test.ts with 13 tests covering entityTarget, entityId, parseEntityTarget, CRUD operations, and format conversion."}
{"id":"parquedb-w9zj","title":"Missing Corruption Recovery Tests","description":"## Problem\nNo tests for corrupted Parquet files or partial writes.\n\n## Fix\nAdd tests/unit/storage/corruption-recovery.test.ts with tests for:\n- Corrupted Parquet file handling\n- Partial write recovery\n- Data integrity verification\n\n## Priority\nP1 - HIGH","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:10:33.402449-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T18:22:30.652044-06:00","closed_at":"2026-02-03T18:22:30.652044-06:00","close_reason":"Fixed in commit ab28519"}
{"id":"parquedb-wao8","title":"Fix db-provider tests","description":"tests/unit/integrations/db-provider.test.ts ParqueDBAdapter tests failing for CRUD operations (create, get).","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:17:09.592815-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:22:21.968316-06:00","closed_at":"2026-02-03T12:22:21.968316-06:00","close_reason":"Closed"}
{"id":"parquedb-wczs","title":"Load testing compaction with realistic data volumes","description":"Need to validate compaction system under realistic production load. Test scenarios: 1) 1M+ files per hour across 100+ namespaces, 2) Large files (100MB+) that stress memory during merge-sort, 3) High writer concurrency (50+ simultaneous writers), 4) Sustained load over 24+ hours. Measure: latency percentiles, memory usage, workflow step counts, OCC retry rates. Use wrangler dev with simulated R2 events or deploy to staging.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:57:00.79456-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:11:54.088017-06:00","closed_at":"2026-02-03T14:11:54.088017-06:00","close_reason":"Closed"}
{"id":"parquedb-we5b","title":"Fix: Add missing @dotdo/cli dependency for CLI utils","description":"The parquedb CLI utils module (src/cli/utils.ts) imports from @dotdo/cli/output, @dotdo/cli/highlight, @dotdo/cli/spinner, @dotdo/cli/progress, and @dotdo/cli/colors. However, the @dotdo/cli package was not listed in package.json dependencies, causing CLI tests to fail with 'Cannot find package @dotdo/cli/output' error. Fixed by adding @dotdo/cli ^0.2.5 to dependencies.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:55:20.795683-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:55:54.246002-06:00","closed_at":"2026-02-03T15:55:54.246002-06:00","close_reason":"Closed"}
{"id":"parquedb-wexp","title":"Fix 54 TypeScript errors","description":"TypeScript check shows 54 errors: 14 possibly undefined (TS18048), 19 unused declarations (TS6133), 3 missing modules in git/index.ts (TS2307), 7 type incompatibilities, 5 type constraint violations. Top files: log.ts (11), merge.ts (7), parquet-merge.ts (6), event-merge.ts (4), delta.ts (3).","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:38:05.936351-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:47:39.347979-06:00","closed_at":"2026-02-03T08:47:39.347979-06:00","close_reason":"Closed"}
{"id":"parquedb-wglp","title":"Upload missing IMDB collections (ratings, principals, crew)","description":"The imdb dataset has 5 collections but only 2 have data uploaded:\n- titles \n- names \n- ratings  missing\n- principals  missing\n- crew  missing\n\nNeed to generate sample data or source real IMDB data for these collections.\n\nThe sample data generator (scripts/generate-imdb-sample.ts) creates titles, people, cast but\nthe dataset config expects different collection names.\n\nEither:\n1. Update dataset config to match what generator produces\n2. Update generator to produce expected collections\n3. Source real IMDB data and upload","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T03:24:40.19218-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T03:34:51.964859-06:00","closed_at":"2026-02-03T03:34:51.964859-06:00","close_reason":"Closed"}
{"id":"parquedb-wj2c","title":"Large Context Objects Creating Tight Coupling","description":"**File:** src/ParqueDB/core.ts (lines 196-260)\n\n**Issue:** EntityOperationsContext bundles many dependencies together, creating tight coupling\n\n**Fix:** Break into smaller, focused contexts (e.g., StorageContext, QueryContext, MutationContext)\n\n**Impact:** Improved testability and maintainability","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:10:35.238275-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T18:22:33.478802-06:00","closed_at":"2026-02-03T18:22:33.478802-06:00","close_reason":"Fixed in commit ab28519"}
{"id":"parquedb-wlrc","title":"Integration: Enhance CLI with cli.do utilities","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:40.984027-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:43:47.863725-06:00","closed_at":"2026-02-03T10:43:47.863725-06:00","close_reason":"Closed"}
{"id":"parquedb-wnn","title":"[RED] IMDB example using ParqueDB","description":"Rewrite IMDB loader to use db.collection().createMany() and $link for relationships. Movies, actors, ratings with proper relationships.","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T14:31:40.815894-06:00","created_by":"Nathan Clevenger","updated_at":"2026-01-30T14:46:18.31241-06:00","closed_at":"2026-01-30T14:46:18.31241-06:00","close_reason":"Closed"}
{"id":"parquedb-wnvu","title":"Fix Iceberg hard delete tests","description":"tests/unit/backends/iceberg-hard-delete.test.ts has 8 failing tests. Tests for hardDelete() functionality including single entity, multiple entities, delete snapshots, equality delete files, delete manifests, and time travel.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:04:18.113438-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:08:12.522872-06:00","closed_at":"2026-02-03T12:08:12.522872-06:00","close_reason":"Closed"}
{"id":"parquedb-wq9e","title":"Enable exactOptionalPropertyTypes in tsconfig","description":"TypeScript review recommends enabling exactOptionalPropertyTypes and noUnusedParameters for stricter typing. Current config is already very strict but these would improve further.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T16:28:10.376266-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:29:52.135231-06:00","closed_at":"2026-02-03T16:29:52.135231-06:00","close_reason":"Cannot enable these settings yet - produces 1108 TypeScript errors. Created follow-up issue parquedb-wq9e.1 with details. Settings remain disabled pending a larger refactoring effort."}
{"id":"parquedb-wq9e.1","title":"Fix 1108 TypeScript errors for stricter tsconfig","description":"Enabling exactOptionalPropertyTypes and noUnusedParameters in tsconfig.json produces 1108 TypeScript errors.\n\nKey error categories:\n\n1. **exactOptionalPropertyTypes errors** - The codebase frequently assigns `undefined` to optional properties, but this flag requires distinguishing between `prop?: T` (property may be missing) and `prop?: T | undefined` (property exists but may be undefined).\n\n2. **noUnusedParameters errors** - Many callback functions and interface implementations have unused parameters that need to be prefixed with `_` or removed.\n\n3. **Generic type mismatches** - Several backend classes have generic type issues where `Entity\u003cEntityData\u003e` is not assignable to `Entity\u003cT\u003e` under strict optional property checking.\n\nAffected areas:\n- src/backends/*.ts - EntityBackend implementations\n- src/integrations/*.ts - SQL integration code\n- src/workflows/*.ts - Cloudflare Workflows\n- src/worker/*.ts - Durable Object implementations\n- Many other files throughout the codebase\n\nRecommended approach:\n1. First enable `noUnusedParameters` alone and fix those errors (simpler)\n2. Then tackle `exactOptionalPropertyTypes` in a separate PR\n\nEstimated effort: 4-8 hours","status":"open","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T16:29:46.47041-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:29:46.47041-06:00","dependencies":[{"issue_id":"parquedb-wq9e.1","depends_on_id":"parquedb-wq9e","type":"parent-child","created_at":"2026-02-03T16:29:46.471433-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-wqi0","title":"Fix: Unsafe parseSchema type assertions","description":"Double assertion 'as unknown as MVDefinition' bypasses type checking. Use type guard or validation function instead. File: src/materialized-views/define.ts lines 520-531","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:16:01.539259-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:42:59.28322-06:00","closed_at":"2026-02-03T10:42:59.28322-06:00","close_reason":"Closed"}
{"id":"parquedb-wqry","title":"Vector: Add query-time embedding generation","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:25:42.499759-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:53:28.969542-06:00","closed_at":"2026-02-03T08:53:28.969542-06:00","close_reason":"Closed"}
{"id":"parquedb-ws0p","title":"Create Hono type extensions","description":"Hono context access uses (c as any).var?.user pattern. Create proper Hono context type extensions: declare module 'hono' { interface ContextVariableMap { user: AuthUser | null; databaseContext: DatabaseContext } }","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:38:44.281442-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:44:34.304816-06:00","closed_at":"2026-02-03T08:44:34.304816-06:00","close_reason":"Closed"}
{"id":"parquedb-wvxk","title":"P3: Add schema evolution tests","description":"Basic coverage in tests/unit/schema.test.ts. Add tests for complex schema evolution and validation edge cases.","status":"open","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:34.691686-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:34.691686-06:00"}
{"id":"parquedb-wz9g","title":"Implement concordance indexes between wiki.org.ai and places.org.ai","description":"Create cross-reference indexes linking datasets:\n\nWikidata \u003c-\u003e GeoNames:\n- P1566 (GeoNames ID) links ~10M Wikidata entities\n- Build bidirectional lookup table\n\nWikidata \u003c-\u003e Overture:\n- Match by coordinates (P625) + name similarity\n- Fuzzy matching for POI alignment\n\nIndex structure:\n```\nconcordances/\n wikidata-geonames.parquet   # QID \u003c-\u003e geonames_id\n wikidata-overture.parquet   # QID \u003c-\u003e overture_id\n geonames-overture.parquet   # geonames_id \u003c-\u003e overture_id\n```\n\nTarget: ~500MB total, ~20 files","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:18:23.585205-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:56:52.922865-06:00","closed_at":"2026-02-03T13:56:52.922865-06:00","close_reason":"Implemented concordance indexes linking Wikidata-GeoNames, Wikidata-Overture, GeoNames-Overture, and Wikidata-OSM"}
{"id":"parquedb-wzi1","title":"Fix EvalScoresMV date key generation tests","description":"EvalScoresMV.test.ts date key generation tests are failing. mockDb.__aggregates returns empty array after mv.refresh() calls. Tests affected: group by month, week. The refresh logic is not populating aggregates correctly.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:22:10.89612-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:23:55.281181-06:00","closed_at":"2026-02-03T11:23:55.281181-06:00","close_reason":"Closed"}
{"id":"parquedb-x0ot","title":"Remove hardcoded relationship configurations","description":"Collection.ts has hardcoded relationship configs that limit extensibility:\n\n1. reverseMap (lines 587-592):\n```typescript\nconst reverseMap: Record\u003cstring, Record\u003cstring, string\u003e\u003e = {\n  'posts': { 'author': 'posts', 'categories': 'posts' },\n  'comments': { 'author': 'comments', 'post': 'comments' },\n}\n```\n\n2. knownPredicates whitelist (lines 636-637):\n```typescript\nconst knownPredicates = ['author', 'categories', 'post', 'comments', 'posts', 'manager']\n```\n\nThese should be:\n- Derived from schema metadata\n- Configurable at runtime\n- Not hardcoded to specific test cases","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:31:38.95496-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:11:32.472269-06:00","closed_at":"2026-02-03T10:11:32.472269-06:00","close_reason":"Closed"}
{"id":"parquedb-x3r9","title":"Refactor: core.ts - Add JSDoc comments to private methods","description":"In src/ParqueDB/core.ts, several private methods lack documentation:\n\nMethods with good JSDoc:\n- recordEvent() - lines 1847-1853 have comprehensive docs\n- validateAgainstSchema() - lines 1317-1323\n\nMethods missing documentation:\n- hydrateEntity() - lines 1535-1648, only has one-line comment\n- reconstructEntityAtTime() - lines 1654-1751, only brief comment\n- flushEvents() - line 1756, single line\n- scheduleFlush() - lines 1829-1841, brief\n- maybeRotateEventLog() - lines 1919-1959, no docs\n- isFieldRequired() - lines 1395-1405, no docs\n- hasDefault() - lines 1410-1418, no docs\n- validateFieldType() - lines 1423-1486, no docs\n- applySchemaDefaults() - lines 1491-1529, no docs\n- legacyValidateAgainstSchema() - lines 1366-1390, no docs\n- archiveEvents() - lines 1967-2027, no docs\n\nAdding JSDoc would help developers understand:\n1. What each method does\n2. When/why it's called\n3. What parameters mean\n4. What side effects occur\n\nPriority on public-facing utility methods and complex logic like reconstructEntityAtTime.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:34:42.226416-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:57:19.318305-06:00","closed_at":"2026-02-03T10:57:19.318305-06:00","close_reason":"Closed","labels":["documentation","refactor"]}
{"id":"parquedb-x53","title":"Write: API Reference - Collection class","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T12:40:35.234239-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:37:46.795057-06:00","closed_at":"2026-02-01T14:37:46.795057-06:00","close_reason":"Closed"}
{"id":"parquedb-x58","title":"[REFACTOR] FTS optimization","description":"Optimize tokenization and BM25 scoring","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:56.499715-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:27:18.834807-06:00","closed_at":"2026-02-01T14:27:18.834807-06:00","close_reason":"Closed"}
{"id":"parquedb-x5vd","title":"High: Missing rate limiting on /ns and write endpoints","description":"The main worker fetch handler in src/worker/index.ts does not apply rate limiting to:\n\n1. /ns/:namespace routes (handleNsRoute) - includes CREATE, UPDATE, DELETE operations\n2. Debug endpoints (/debug/*)\n3. Benchmark endpoints (/benchmark*)\n4. Migration endpoints (/migrate)\n5. Sync routes (handled separately in sync-routes.ts)\n\nOnly public-routes.ts applies rate limiting. While CSRF protection is present on /ns mutations, there's no protection against:\n- Brute force attacks\n- Resource exhaustion via repeated write operations\n- DoS via heavy debug queries\n\nRate limiting should be applied at the main fetch handler level for all routes, not just public routes.\n\nFile: /Users/nathanclevenger/projects/parquedb/src/worker/index.ts","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:41:29.275901-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:53:22.697651-06:00","closed_at":"2026-02-03T12:53:22.697651-06:00","close_reason":"Closed"}
{"id":"parquedb-x6m","title":"[RED] Bloom filter tests","description":"Write failing tests for bloom filter creation and lookup","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:45.846772-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:24:19.291861-06:00","closed_at":"2026-02-01T14:24:19.291861-06:00","close_reason":"Closed"}
{"id":"parquedb-x9ie","title":"Split ParqueDB.ts into smaller modules","description":"ParqueDB.ts is 2000+ lines containing multiple classes, validation, filter matching, events, snapshots, relationships. Split into: src/ParqueDB/core.ts, src/ParqueDB/validation.ts, src/ParqueDB/relationships.ts, src/ParqueDB/snapshots.ts","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T17:01:01.572979-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T04:45:51.664924-06:00","closed_at":"2026-02-02T04:45:51.664924-06:00","close_reason":"Split into 6 modules: core.ts, collection.ts, types.ts, validation.ts, store.ts, index.ts. All 5062 tests pass."}
{"id":"parquedb-xazg","title":"Critical: TOCTOU race in lock extend method","description":"The Lock.extend() method in lock.ts (lines 444-475) has a TOCTOU race condition. It reads the lock state, verifies ownership, then writes with a regular storage.write() instead of using writeConditional. Between the read and write, another process could modify the lock, leading to lost updates or lock corruption.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:00.261947-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:36:18.668446-06:00","closed_at":"2026-02-03T11:36:18.668446-06:00","close_reason":"Fixed by using writeConditional with etag in the extend method to prevent TOCTOU race"}
{"id":"parquedb-xbfj","title":"Refactor readEntitiesFromSnapshot (178 lines)","description":"**File:** src/backends/iceberg.ts - readEntitiesFromSnapshot function\n\n**Issue:** Function is 178 lines long, too complex, hard to test in isolation\n\n**Fix:** Break into smaller, focused helper functions:\n- parseSnapshotMetadata()\n- resolveManifestFiles()\n- filterByPartition()\n- readDataFiles()\n- mergeResults()\n\n**Impact:** Improved testability and maintainability","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:10:43.463953-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T18:22:33.052314-06:00","closed_at":"2026-02-03T18:22:33.052314-06:00","close_reason":"Fixed in commit ab28519"}
{"id":"parquedb-xi44","title":"Extend ParquetWriter for typed mode with $data column","description":"## Context\nPart of typed storage implementation (parquedb-k7jj design).\n\n## Requirements\n\n1. Extend `src/parquet/writer.ts`:\n   - Add typed mode write path\n   - Convert entities to native columns using generated schema\n   - Encode $data Variant column when enabled\n\n2. $data column encoding:\n   - Use Parquet Variant encoding if available\n   - Fall back to JSON encoding\n   - Include complete entity for fast full-row reads\n\n3. Integration with StorageRouter:\n   - Writer receives schema from router\n   - Respects per-collection options (compression, rowGroupSize)\n\n4. Testing:\n   - Unit tests for typed mode writing\n   - Tests for $data encoding/decoding roundtrip\n   - Tests with includeDataVariant: true/false\n   - Verify DuckDB can read generated files\n\n## Acceptance Criteria\n- [ ] Typed mode writes native columns per schema\n- [ ] $data column encoded correctly when enabled\n- [ ] Files readable by hyparquet and DuckDB\n- [ ] Per-collection options respected\n- [ ] Tests pass\n\n## References\n- Design: docs/architecture/typed-storage.md\n- Depends on: parquedb-37c3 (ParquetSchemaGenerator)","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:41:29.278642-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:22:10.17593-06:00","closed_at":"2026-02-03T07:22:10.17593-06:00","close_reason":"Implemented with full test coverage"}
{"id":"parquedb-xi8","title":"[REFACTOR] Collection class cleanup","description":"Refactor Collection class","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:46.846235-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:02:25.580152-06:00","closed_at":"2026-02-01T14:02:25.580152-06:00","close_reason":"Closed"}
{"id":"parquedb-xixf","title":"Add e2e tests for search workers","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:53:11.688281-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:55:56.375421-06:00","closed_at":"2026-02-03T13:55:56.375421-06:00","close_reason":"Added search-e2e.test.ts with comprehensive test suite"}
{"id":"parquedb-xjcl","title":"P1: Fix inconsistent error handling in Collection.exists()","description":"In src/collection.ts:1132-1140, all errors are caught and treated the same way, masking real issues. Check for specific error types (NotFoundError) rather than catching all exceptions.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:10:34.474939-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:24:28.916205-06:00","closed_at":"2026-02-03T15:24:28.916205-06:00","close_reason":"Fixed by parallel agents"}
{"id":"parquedb-xjjk","title":"Overuse of Record\u003cstring, unknown\u003e","description":"## Problem\n30+ locations use Record\u003cstring, unknown\u003e which loses type information and won't catch typos at compile time.\n\n## Location\n30+ locations throughout codebase\n\n## Fix\nDefine specific interfaces for each use case.\n\n## Priority\nP1 - HIGH","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:10:30.594588-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T18:22:30.595902-06:00","closed_at":"2026-02-03T18:22:30.595902-06:00","close_reason":"Fixed in commit ab28519"}
{"id":"parquedb-xlpg","title":"Fix null as unknown as Record pattern in migrations","description":"Migration files (mongodb.ts:697,718, csv.ts:474, json.ts:478,494,596) use 'null as unknown as Record\u003cstring,unknown\u003e' pattern. Replace with proper optional types or undefined.","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T05:32:47.491335-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T06:45:38.321621-06:00","closed_at":"2026-02-02T06:45:38.321621-06:00","close_reason":"Closed"}
{"id":"parquedb-xnm4","title":"Add: Runtime validation for tail handler input","description":"createTailHandler trusts incoming TraceItem[] matches interface. Add runtime validation for production safety. File: src/integrations/tail/index.ts lines 363-412","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:16:03.693695-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:23:23.359132-06:00","closed_at":"2026-02-03T10:23:23.359132-06:00","close_reason":"Closed"}
{"id":"parquedb-xopt","title":"Implement external ID indexes for wiki.org.ai","description":"Create lookup indexes for external identifiers:\n\nHigh priority properties:\n- P345 (IMDb ID) - 2M items\n- P356 (DOI) - 40M items\n- P212/P957 (ISBN) - 5M items\n- P1566 (GeoNames ID) - 10M items (concordance with places.org.ai)\n- P646 (Freebase ID) - 5M items\n- P214 (VIAF ID) - 3M items\n- P2002 (Twitter) - 1M items\n- P2013 (Facebook) - 500K items\n\nIndex format: property_value -\u003e QID\nTarget: ~2-3GB total, ~500 files","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:18:12.437907-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:56:52.643729-06:00","closed_at":"2026-02-03T13:56:52.643729-06:00","close_reason":"Implemented external ID indexes for IMDb, DOI, ISBN, GeoNames, social media, and more"}
{"id":"parquedb-xsdj","title":"Fix benchmarks percentile test","description":"tests/unit/benchmarks/percentile.test.ts calculateStats empty array handling test failing - should return all zeros for empty array.","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:17:11.099584-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:18:00.780197-06:00","closed_at":"2026-02-03T12:18:00.780197-06:00","close_reason":"Closed"}
{"id":"parquedb-xssl","title":"CompactionStateDO Storage Limits - 128KB Risk","description":"**Issue:** All window state stored in single key, could exceed 128KB DO limit\n\n**Fix:** Use per-window storage keys with transactions","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T14:34:23.258104-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:54:33.764595-06:00","closed_at":"2026-02-03T15:54:33.764595-06:00","close_reason":"Closed"}
{"id":"parquedb-xtps","title":"Add performance benchmarks","description":"No published benchmarks to back efficiency claims. Create benchmarks comparing: compression ratios vs JSON/SQLite, query latency by data size, storage costs, predicate pushdown effectiveness.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:44.725848-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T16:15:27.716037-06:00","closed_at":"2026-02-01T16:15:27.716037-06:00","close_reason":"Closed"}
{"id":"parquedb-xxu2","title":"Epic: Local AI Observability MVs","description":"Auto-materialize AI analytics from AI SDK and evalite to local filesystem. Streaming MVs that work in Node.js for AI SDK logging, evalite traces, and generated content capture.","status":"open","priority":1,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:09:03.485017-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:09:03.485017-06:00"}
{"id":"parquedb-xxu2.1","title":"createStreamAdapter() for Node.js","description":"EventEmitter, async iterator, and polling adapters for local streaming","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:09:07.784025-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:23:38.177857-06:00","closed_at":"2026-02-03T10:23:38.177857-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-xxu2.1","depends_on_id":"parquedb-xxu2","type":"parent-child","created_at":"2026-02-03T09:09:07.784651-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-xxu2.10","title":"Add percentile latency tracking to AIUsageMV","description":"AIUsageMV has p50/p95/p99 latency fields defined in the types but they are never populated during aggregation. The updateAggregateFromLog() function only tracks min/max/avg latency. For production AI monitoring, percentile latencies are critical for SLA tracking and outlier detection.\n\nImplementation needed:\n- Store latency samples in working aggregate (with reservoir sampling for memory efficiency)\n- Calculate percentiles during final aggregate computation\n- Consider T-digest or similar streaming algorithm for large-scale deployments","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:15:11.245268-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:53:11.502815-06:00","closed_at":"2026-02-03T12:53:11.502815-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-xxu2.10","depends_on_id":"parquedb-xxu2","type":"parent-child","created_at":"2026-02-03T12:15:11.246087-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-xxu2.11","title":"Add token/cost rate limiting metrics","description":"Production AI workloads need rate limiting awareness. Add metrics for:\n- Tokens per minute/hour by model\n- Cost burn rate (USD per hour)\n- Request rate per model/provider\n- Alerting thresholds configuration\n\nThis enables setting up alerts when approaching API rate limits or budget caps.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:15:28.853054-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:09:11.694425-06:00","closed_at":"2026-02-03T15:09:11.694425-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-xxu2.11","depends_on_id":"parquedb-xxu2","type":"parent-child","created_at":"2026-02-03T12:15:28.853806-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-xxu2.12","title":"Implement data retention/compaction for observability MVs","description":"Current MVs have maxAgeMs configuration but no automatic cleanup/compaction:\n\nIssues:\n1. AIRequestsMV.cleanup() deletes records one-by-one (O(n) deletes)\n2. No background cleanup scheduler - must be called manually\n3. No compaction for Parquet files - old files accumulate\n4. GeneratedContentMV.cleanup() same issue\n\nNeeded for production:\n- Background cleanup job with configurable schedule\n- Batch delete operations for efficiency\n- Parquet file compaction (merge old files, remove deleted rows)\n- Retention policies by granularity (keep hourly for 7d, daily for 90d, monthly forever)","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:15:37.85152-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:46:23.435597-06:00","closed_at":"2026-02-03T12:46:23.435597-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-xxu2.12","depends_on_id":"parquedb-xxu2","type":"parent-child","created_at":"2026-02-03T12:15:37.85337-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-xxu2.13","title":"Add streaming MV persistence for StreamProcessor crashes","description":"StreamProcessor has a dead-letter queue for failed writes, but:\n\n1. DLQ is in-memory only - lost on crash\n2. No WAL (write-ahead log) for in-flight records\n3. No checkpointing for stream position\n\nFor production reliability:\n- Persist DLQ to disk for recovery\n- Add WAL for in-flight batches before Parquet write\n- Checkpoint mechanism for incremental refresh position\n- Recovery protocol on startup","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:15:46.159939-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:44:49.164401-06:00","closed_at":"2026-02-03T12:44:49.164401-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-xxu2.13","depends_on_id":"parquedb-xxu2","type":"parent-child","created_at":"2026-02-03T12:15:46.160728-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-xxu2.14","title":"Add prompt/completion content sampling to AIRequestsMV","description":"For AI debugging and quality analysis, production systems need prompt/completion samples:\n\nCurrent state:\n- AIRequestsMV tracks tokens, latency, cost but NOT content\n- GeneratedContentMV tracks content separately but requires explicit call\n\nNeeded:\n- Optional sampling of prompts/completions (configurable % or first N chars)\n- Sampling policy (e.g., sample 1% of requests, or all errors)\n- Integration with GeneratedContentMV for correlation\n- PII detection/redaction hooks before storage\n- Content fingerprinting for deduplication","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:15:56.216797-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:10:13.050426-06:00","closed_at":"2026-02-03T15:10:13.050426-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-xxu2.14","depends_on_id":"parquedb-xxu2","type":"parent-child","created_at":"2026-02-03T12:15:56.217652-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-xxu2.15","title":"Add real-time dashboard/export APIs for observability data","description":"Production monitoring needs easy access to metrics:\n\nCurrent state:\n- MVs provide query methods but no standard export format\n- No real-time streaming of metrics\n- No OpenTelemetry/Prometheus integration\n\nNeeded:\n- Prometheus metrics endpoint (/metrics)\n- OpenTelemetry trace/metric export\n- WebSocket/SSE streaming for dashboards\n- JSON export API for custom integrations\n- Grafana-compatible query interface","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:16:04.965829-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:12:14.730643-06:00","closed_at":"2026-02-03T15:12:14.730643-06:00","close_reason":"Implemented comprehensive observability export APIs: Prometheus metrics, OpenTelemetry OTLP, Grafana SimpleJSON datasource, SSE/WebSocket streaming, and JSON/CSV exports. All 90 tests passing.","dependencies":[{"issue_id":"parquedb-xxu2.15","depends_on_id":"parquedb-xxu2","type":"parent-child","created_at":"2026-02-03T12:16:04.966876-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-xxu2.16","title":"Fix AIUsageMV incremental refresh losing existing aggregates","description":"Bug in AIUsageMV.refresh():\n\nWhen processing logs incrementally, the code loads existing aggregates from DB but creates a fresh WorkingAggregate that loses the existing data:\n\n```typescript\n// Line 305-324 in AIUsageMV.ts\nif (existingAggregates.length \u003e 0) {\n  aggregate = existingAggregates[0] as unknown as AIUsageAggregate\n} else {\n  aggregate = createEmptyAggregate(...)\n}\n```\n\nThe existing aggregate is loaded but then immediately overwritten because:\n1. scores array is initialized empty\n2. Working aggregate doesn't merge existing counts\n3. On save, only new values are written (not merged with existing)\n\nImpact: Incremental refresh overwrites historical data instead of adding to it.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:16:13.447148-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:41:46.941261-06:00","closed_at":"2026-02-03T12:41:46.941261-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-xxu2.16","depends_on_id":"parquedb-xxu2","type":"parent-child","created_at":"2026-02-03T12:16:13.448217-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-xxu2.17","title":"Add model pricing auto-update mechanism","description":"DEFAULT_MODEL_PRICING is hardcoded and will become stale:\n\nCurrent state:\n- Pricing defined as static const in types.ts\n- Comment says 'as of early 2026' but prices change frequently\n- No way to fetch current prices\n\nNeeded for production:\n- API endpoint to fetch current model pricing (provider APIs or aggregator)\n- Periodic refresh of pricing data\n- Version/timestamp tracking for pricing data\n- Fallback to cached pricing when API unavailable\n- User override mechanism for enterprise pricing","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:16:21.233035-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:02:26.93161-06:00","closed_at":"2026-02-03T15:02:26.93161-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-xxu2.17","depends_on_id":"parquedb-xxu2","type":"parent-child","created_at":"2026-02-03T12:16:21.233845-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-xxu2.18","title":"Add multi-tenancy support to observability MVs","description":"Current MVs have optional appId/userId fields but no proper multi-tenant isolation:\n\nMissing features:\n- Tenant-scoped storage paths\n- Per-tenant data isolation guarantees\n- Tenant-level quota tracking\n- Cross-tenant analytics for platform operators\n- Tenant ID propagation through streaming pipeline\n\nNeeded for SaaS deployments where multiple customers share infrastructure.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:16:28.69245-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:10:08.047967-06:00","closed_at":"2026-02-03T15:10:08.047967-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-xxu2.18","depends_on_id":"parquedb-xxu2","type":"parent-child","created_at":"2026-02-03T12:16:28.69325-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-xxu2.19","title":"Add anomaly detection for AI workload metrics","description":"Production AI monitoring needs anomaly detection:\n\nUse cases:\n- Latency spike detection\n- Cost anomaly alerts\n- Error rate threshold violations\n- Token usage outliers\n- Model performance degradation\n\nImplementation:\n- Rolling window statistics for baseline\n- Standard deviation based thresholds\n- Configurable alert callbacks\n- Integration with existing webhook/notification systems","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:16:35.56857-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:57:50.4028-06:00","closed_at":"2026-02-03T14:57:50.4028-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-xxu2.19","depends_on_id":"parquedb-xxu2","type":"parent-child","created_at":"2026-02-03T12:16:35.569472-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-xxu2.2","title":"StreamProcessor for local MV processing","description":"Batching, flushing, backpressure for Node.js environments","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:09:08.734354-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:40:12.924416-06:00","closed_at":"2026-02-03T10:40:12.924416-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-xxu2.2","depends_on_id":"parquedb-xxu2","type":"parent-child","created_at":"2026-02-03T09:09:08.734931-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-xxu2.3","title":"AIRequestsMV","description":"Materialize all AI requests with latency, tokens, cost tracking","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:09:09.544246-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:32:55.912474-06:00","closed_at":"2026-02-03T10:32:55.912474-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-xxu2.3","depends_on_id":"parquedb-xxu2","type":"parent-child","created_at":"2026-02-03T09:09:09.544813-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-xxu2.4","title":"AIUsageMV","description":"Aggregated usage by model/provider/day with cost estimates","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:09:10.27155-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:24:27.650906-06:00","closed_at":"2026-02-03T10:24:27.650906-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-xxu2.4","depends_on_id":"parquedb-xxu2","type":"parent-child","created_at":"2026-02-03T09:09:10.272162-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-xxu2.5","title":"GeneratedContentMV","description":"Capture all AI-generated text and structured objects","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:09:12.281171-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:23:40.99723-06:00","closed_at":"2026-02-03T10:23:40.99723-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-xxu2.5","depends_on_id":"parquedb-xxu2","type":"parent-child","created_at":"2026-02-03T09:09:12.281694-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-xxu2.6","title":"EvalScoresMV","description":"Stream evalite scores to MV for analytics","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:09:13.411219-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:24:43.650442-06:00","closed_at":"2026-02-03T10:24:43.650442-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-xxu2.6","depends_on_id":"parquedb-xxu2","type":"parent-child","created_at":"2026-02-03T09:09:13.411764-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-xxu2.7","title":"AI SDK middleware MV integration","description":"Wire middleware to stream to registered MVs","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:09:14.710454-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:10:03.343446-06:00","closed_at":"2026-02-03T10:10:03.343446-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-xxu2.7","depends_on_id":"parquedb-xxu2","type":"parent-child","created_at":"2026-02-03T09:09:14.711016-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-xxu2.8","title":"Evalite adapter MV integration","description":"Wire evalite adapter to stream to MVs","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:09:15.483422-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:18:30.609297-06:00","closed_at":"2026-02-03T10:18:30.609297-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-xxu2.8","depends_on_id":"parquedb-xxu2","type":"parent-child","created_at":"2026-02-03T09:09:15.484007-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-xxu2.9","title":"Local AI MV tests","description":"Tests with FsBackend and mock AI events","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T09:09:17.130512-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:32:32.267003-06:00","closed_at":"2026-02-03T10:32:32.267003-06:00","close_reason":"Closed","dependencies":[{"issue_id":"parquedb-xxu2.9","depends_on_id":"parquedb-xxu2","type":"parent-child","created_at":"2026-02-03T09:09:17.131304-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-xy4c","title":"[TEST] Add E2E tests to CI pipeline","description":"E2E tests exist in scripts/e2e-benchmark.mjs but are not run in CI. Need to:\n\n1. Add GitHub Actions workflow for E2E tests\n2. Configure test environment (Wrangler dev or miniflare)\n3. Add E2E test badge to README\n4. Consider nightly E2E runs against deployed worker","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T13:35:54.275202-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:43:29.471991-06:00","closed_at":"2026-02-01T13:43:29.471991-06:00","close_reason":"Closed"}
{"id":"parquedb-y073","title":"Implement TTLCache utility","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:19:38.589604-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:21:37.696905-06:00","closed_at":"2026-02-03T15:21:37.696905-06:00","close_reason":"Closed"}
{"id":"parquedb-y66t","title":"Replace Buffer.from with Worker-safe encoding","description":"QueryExecutor.ts line 1618 uses Buffer.from() for cursor encoding. Buffer is Node.js global - Workers support it but prefer btoa() or Uint8Array for proper edge compatibility.","status":"closed","priority":2,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:21.658511-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T16:20:04.074079-06:00","closed_at":"2026-02-01T16:20:04.074079-06:00","close_reason":"Closed"}
{"id":"parquedb-y7xm","title":"Refactor: Query - Strengthen Filter type safety","description":"## Summary\nThe Filter interface in `src/types/filter.ts` allows too much flexibility, making it easy to construct invalid filters.\n\n### Current Issues\n\n1. **Loose field index signature** (line 262-264):\n   ```typescript\n   export interface Filter {\n     [field: string]: FieldFilter | undefined\n     // ... logical operators\n   }\n   ```\n   This allows any key including ones that look like operators but aren't recognized.\n\n2. **No validation of operator combinations**:\n   - Can combine incompatible operators (e.g., $gt on arrays)\n   - String operators can be used on non-string fields\n   - No type narrowing based on operator usage\n\n3. **Type guards don't cover all cases**:\n   - `isComparisonOperator` only checks for single-key objects (line 296)\n   - Combined operators like `{ $gt: 5, $lt: 10 }` fail the check\n\n### Proposed Improvements\n- Use branded types or template literals for field paths\n- Add strict mode filter type with better inference\n- Improve type guards to handle multi-operator conditions\n- Consider generic Filter\u003cT\u003e that validates against entity type\n\n### Files to Update\n- `src/types/filter.ts` - Improve type definitions\n\n### Acceptance Criteria\n- [ ] Create StrictFilter\u003cT\u003e generic type\n- [ ] Add field path validation type\n- [ ] Fix type guards for multi-operator conditions\n- [ ] Add JSDoc examples for common patterns","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:32:51.09074-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:33:15.207326-06:00","closed_at":"2026-02-03T11:33:15.207326-06:00","close_reason":"Closed","labels":["refactor"]}
{"id":"parquedb-y9aw","title":"[ARCH] Address async-and-forget write semantics","description":"Some write operations don't await completion, leading to:\n- Fire-and-forget patterns that may lose data\n- No confirmation that writes succeeded\n- Potential for silent failures\n\nAudit all write paths and ensure proper await/error handling. Consider adding write acknowledgment options.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T13:35:51.840178-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:44:07.212622-06:00","closed_at":"2026-02-01T13:44:07.212622-06:00","close_reason":"Closed"}
{"id":"parquedb-yba1","title":"Refactor: Worker - QueryExecutor has placeholder parsing methods","description":"QueryExecutor has several placeholder implementations that return empty/dummy data:\n\n1. parseMetadata() - returns empty ParquetMetadata (lines 1889-1897)\n2. parseRowGroup() - returns empty EntityRecord[] (lines 1903-1910)  \n3. parseBloomFilter() - returns a bloom filter that always returns true (lines 1916-1924)\n\nThese are used by the legacy code path when ParquetReader is not available.\n\nFiles:\n- src/worker/QueryExecutor.ts\n\nOptions:\n1. Complete the implementations using hyparquet\n2. Remove the legacy path since ParquetReader is now the primary reader\n3. Add clear error messages if these paths are hit unexpectedly\n\nImpact: Cleaner code, better error handling for edge cases","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:33:43.116612-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:11:29.910228-06:00","closed_at":"2026-02-03T11:11:29.910228-06:00","close_reason":"Closed"}
{"id":"parquedb-ycie","title":"Refactor: Storage - Extract duplicate globToRegex function","description":"The globToRegex function is duplicated in multiple files:\\n\\n1. MemoryBackend.ts (lines 97-105) - matchPattern function\\n2. FsBackend.ts (lines 306-312) - globToRegex method\\n3. DOSqliteBackend.ts (lines 602-608) - globToRegex function\\n\\nAll three implementations are nearly identical, converting glob patterns (* and ?) to regex.\\n\\nRefactor to:\\n- Add globToRegex to src/storage/validation.ts or create src/storage/utils.ts\\n- Update all backends to use the shared implementation\\n\\nFiles affected:\\n- /Users/nathanclevenger/projects/parquedb/src/storage/MemoryBackend.ts\\n- /Users/nathanclevenger/projects/parquedb/src/storage/FsBackend.ts\\n- /Users/nathanclevenger/projects/parquedb/src/storage/DOSqliteBackend.ts\\n- /Users/nathanclevenger/projects/parquedb/src/storage/validation.ts","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:32:33.21887-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:22:10.181778-06:00","closed_at":"2026-02-03T07:22:10.181778-06:00","close_reason":"Implemented with full test coverage"}
{"id":"parquedb-yd24","title":"Add: Concurrent refresh race condition tests","description":"No tests for what happens when two refresh operations run simultaneously on same MV. Test that concurrent incrementalRefresh() calls don't corrupt data or produce duplicate rows.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T10:15:52.110339-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:42:06.071521-06:00","closed_at":"2026-02-03T10:42:06.071521-06:00","close_reason":"Closed"}
{"id":"parquedb-yeho","title":"TransactionalBackend commit is not atomic - partial failures possible","description":"**Critical: Data Corruption Risk**\n\nIn /src/storage/TransactionalBackend.ts lines 150-196, the commit() method explicitly states in comments that it is NOT truly atomic across files:\n\n- Lines 12-13 state: 'Commit is not truly atomic - partial failures are possible'\n- The implementation applies deletes and writes sequentially in a loop\n- If a write fails mid-way, some changes are persisted while others are not\n\n**Impact**: \n- Transactions can leave the storage in an inconsistent state\n- No rollback of successfully applied operations when later operations fail\n- Data integrity cannot be guaranteed for multi-file operations\n\n**Location**: src/storage/TransactionalBackend.ts:150-196\n\n**Recommendation**: \n1. For critical data, document the limitation prominently\n2. Consider implementing two-phase commit with a journal\n3. Or use the underlying storage's transaction support if available (e.g., SQLite for DO)","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:20.994555-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:48:58.798636-06:00","closed_at":"2026-02-03T11:48:58.798636-06:00","close_reason":"Closed"}
{"id":"parquedb-yf8p","title":"Add tests for untested worker handlers","description":"14 of 15 worker handlers lack tests. Add tests for: benchmark, compaction, csrf-validation, datasets, debug, entity, health, migration, ns, relationships, root, vacuum handlers.","status":"open","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:01.996804-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:01.996804-06:00"}
{"id":"parquedb-ygh8","title":"Add concurrency tests","description":"No tests for parallel operations on same entities. Add tests for: concurrent reads, concurrent writes, read-write races, optimistic locking under contention.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T15:47:31.847513-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T16:22:48.916818-06:00","closed_at":"2026-02-01T16:22:48.916818-06:00","close_reason":"Closed"}
{"id":"parquedb-yhce","title":"Hardcoded Timeout Values in Compaction Queue Consumer","description":"**File:** src/workflows/compaction-queue-consumer.ts (lines 183-196)\n\n**Issue:** Multiple timeout constants hardcoded with no configuration option\n\n**Fix:** Add to CompactionConsumerConfig interface","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T14:34:14.678497-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T15:04:18.424835-06:00","closed_at":"2026-02-03T15:04:18.424835-06:00","close_reason":"Closed"}
{"id":"parquedb-yixn","title":"No Prometheus Metrics Endpoint","description":"**Issue:** ParqueDB has no standard monitoring integration. There is no /metrics endpoint providing Prometheus-compatible metrics.\n\n**Impact:**\n- Cannot integrate with industry-standard monitoring tools (Prometheus, Grafana, etc.)\n- No visibility into database performance in production\n- Cannot set up alerts for critical metrics\n- Operational blindspot for production deployments\n\n**Fix:**\n1. Add /metrics endpoint exposing Prometheus format metrics\n2. Include key metrics:\n   - Query latency (p50, p95, p99)\n   - Operations per second (reads, writes, deletes)\n   - Active connections\n   - Storage size\n   - Compaction status\n   - Error rates\n3. Support metric labels for namespace/collection filtering\n4. Document available metrics\n\n**Priority:** P0 CRITICAL","status":"closed","priority":0,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T17:10:23.957165-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T17:32:46.656053-06:00","closed_at":"2026-02-03T17:32:46.656053-06:00","close_reason":"Closed"}
{"id":"parquedb-yldv","title":"Extract BaseEntityBackend: Abstract base class with shared functionality","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:57:41.025729-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:17:23.744602-06:00","closed_at":"2026-02-03T09:17:23.744602-06:00","close_reason":"Closed"}
{"id":"parquedb-yluh","title":"P2: Migrate remaining type casts to cast.ts","description":"104 'as unknown as' assertions across 31 files. Migrate to centralized cast utilities in src/types/cast.ts.","status":"open","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:22.514189-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:22.514189-06:00"}
{"id":"parquedb-ympz","title":"Add tests for vector index components","description":"Missing test coverage for: hnsw.ts (HNSW index), distance.ts (vector distance calculations). Vector search is a key feature that needs test coverage.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-02T05:32:42.624783-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-02T06:34:55.02098-06:00","closed_at":"2026-02-02T06:34:55.02098-06:00","close_reason":"Closed"}
{"id":"parquedb-yn82","title":"Remove deprecated global filter config functions","description":"setFilterConfig() and getFilterConfig() in src/query/filter.ts are deprecated but still exported. They only log warnings. Should throw errors or be removed entirely.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T16:27:46.181065-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:29:44.750355-06:00","closed_at":"2026-02-03T16:29:44.750355-06:00","close_reason":"Closed"}
{"id":"parquedb-yppk","title":"Create deployment runbook for search worker","description":"Missing production deployment documentation. Need runbook covering:\n- Environment setup\n- R2 bucket creation\n- Data upload\n- Worker deployment\n- Health verification\n- Rollback procedure","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:27:51.578243-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:17:32.833611-06:00","closed_at":"2026-02-03T13:17:32.833611-06:00","close_reason":"Closed"}
{"id":"parquedb-yr2","title":"[REFACTOR] Filter evaluation cleanup","description":"Refactor filter evaluation for performance","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:11.791039-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:03:47.622261-06:00","closed_at":"2026-02-01T14:03:47.622261-06:00","close_reason":"Closed"}
{"id":"parquedb-yray","title":"Add timeout to JWKS fetch in payload/auth.ts","description":"## Issue\nIn `src/integrations/payload/auth.ts`, the `createRemoteJWKSet` function from jose is called without a timeout. The JWKS cache stores the result for 1 hour, but the initial fetch has no timeout protection.\n\n## Location\n`src/integrations/payload/auth.ts:219-221`\n\n## Risk\nHigh - Network calls to JWKS endpoint could hang indefinitely if the endpoint is slow or unresponsive.\n\n## Suggested Fix\nPass a timeout option to jose's `createRemoteJWKSet` or wrap the JWKS fetch with a timeout.","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:35:06.468219-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:41:21.857252-06:00","closed_at":"2026-02-03T11:41:21.857252-06:00","close_reason":"Fixed: Added JWKS_FETCH_TIMEOUT_MS constant (10 seconds) and passed it to createRemoteJWKSet via timeoutDuration option."}
{"id":"parquedb-ysgx","title":"Refactor: Storage - ObservedBackend missing MultipartBackend support","description":"ObservedBackend wraps StorageBackend but does not pass through MultipartBackend methods.\\n\\nR2Backend implements MultipartBackend:\\n```typescript\\nexport class R2Backend implements StorageBackend, MultipartBackend {\\n  createMultipartUpload(path: string, options?: WriteOptions): Promise\u003cMultipartUpload\u003e\\n  startMultipartUpload(path: string): Promise\u003cstring\u003e\\n  uploadPart(...): Promise\u003c{ etag: string }\u003e\\n  completeMultipartUpload(...): Promise\u003cvoid\u003e\\n  abortMultipartUpload(...): Promise\u003cvoid\u003e\\n  writeStreaming(...): Promise\u003cWriteResult\u003e\\n}\\n```\\n\\nBut wrapping R2Backend with ObservedBackend loses access to these methods:\\n```typescript\\nconst observed = withObservability(new R2Backend(bucket))\\nobserved.createMultipartUpload // undefined!\\n```\\n\\nRefactor to:\\n- Add conditional multipart method forwarding in ObservedBackend\\n- Use isMultipart() type guard to check and forward calls\\n- Add observability hooks for multipart operations\\n\\nFiles affected:\\n- /Users/nathanclevenger/projects/parquedb/src/storage/ObservedBackend.ts","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:34:19.51593-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:56:45.824768-06:00","closed_at":"2026-02-03T10:56:45.824768-06:00","close_reason":"Closed"}
{"id":"parquedb-ytzs","title":"Implement Overture Maps Places loader for places.org.ai","description":"Create Overture Maps Places ingestion pipeline:\n\nData source:\n- s3://overturemaps-us-west-2/release/2026-01-21.0/theme=places/\n- Already in GeoParquet format (64M POIs)\n- Size: ~15-25GB raw, ~8GB optimized\n\nPipeline steps:\n1. Download from AWS S3 (no auth required)\n2. Re-partition by geohash prefix (2-3 chars)\n3. Optimize compression (zstd level 15)\n4. Split to 25MB chunks\n5. Generate spatial indexes\n\nTarget: ~8GB total, ~400 files\nLicense: CDLA-Permissive-2.0 / Apache-2.0 (with attribution)","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:17:51.343883-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:43:10.996539-06:00","closed_at":"2026-02-03T13:43:10.996539-06:00","close_reason":"Implemented Overture Maps Places loader with S3 download, geohash partitioning, indexes"}
{"id":"parquedb-yus7","title":"Add tests for public-routes.ts","description":"public-routes.ts lacks tests. Need tests for: CORS handling, database metadata endpoint, collection query endpoint, raw file access with Range headers, visibility checks.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:05:29.061958-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T10:10:33.191753-06:00","closed_at":"2026-02-03T10:10:33.191753-06:00","close_reason":"Tests already exist in tests/unit/worker/public-routes.test.ts with 50 passing tests covering: CORS handling, rate limiting, database metadata endpoint, collection query endpoint, raw file access with Range headers, visibility checks, JWT token verification, and error handling."}
{"id":"parquedb-yvzi","title":"Fix remaining mixed logging in compaction code","description":"compaction-queue-consumer.ts still uses console.log in places (lines 188-273) while also using logger.info (lines 310-331). Standardize all logging to use the logger utility. Convert remaining console.log to logger.debug or logger.info as appropriate.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T13:33:25.374362-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:35:56.935011-06:00","closed_at":"2026-02-03T13:35:56.935011-06:00","close_reason":"Closed"}
{"id":"parquedb-yykh","title":"Complete parseMetadata() placeholder in QueryExecutor","description":"QueryExecutor.parseMetadata() at line ~1898-1906 returns empty placeholder metadata (numRows: 0, rowGroups: [], etc.). This could cause incorrect query execution for certain code paths.","status":"closed","priority":0,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:38:07.764725-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:46:01.738044-06:00","closed_at":"2026-02-03T08:46:01.738044-06:00","close_reason":"Closed"}
{"id":"parquedb-z22c","title":"Time-Travel Query Tests: Comprehensive snapshot query tests","description":"Comprehensive snapshot query tests:\n- Query at specific version\n- Query at timestamp\n- Schema evolution across versions","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:57:48.823106-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:09:32.397993-06:00","closed_at":"2026-02-03T09:09:32.397993-06:00","close_reason":"Closed"}
{"id":"parquedb-z2iy","title":"Add tail integration tests","description":"src/integrations/tail/index.ts is a new integration without dedicated test coverage. Add comprehensive tests.","status":"open","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T19:18:38.503057-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T19:18:38.503057-06:00"}
{"id":"parquedb-z7jn","title":"DeltaBackend.compact does not use OCC - potential data loss","description":"**High: Data Corruption Risk**\n\nIn /src/backends/delta.ts lines 768-783, the performCompaction() method writes the commit file without OCC:\n\n```typescript\ntry {\n  await this.storage.write(\n    commitPath,\n    new TextEncoder().encode(commitContent),\n    { ifNoneMatch: '*' }\n  )\n  this.versionCache.set(ns, newVersion)\n} catch (error) {\n  await this.storage.delete(newDataFilePath).catch(() =\u003e {})\n  throw error\n}\n```\n\nWhile it does use ifNoneMatch: '*', it doesn't handle conflicts - it just throws and leaks the new data file if another write happened.\n\n**Impact**: \n- Compaction can fail silently leaving orphaned data files\n- No retry logic for concurrent modifications\n\n**Location**: src/backends/delta.ts:768-783\n\n**Fix**: Apply the same OCC retry pattern used in appendEntities()","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T11:36:48.380748-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T12:18:19.156608-06:00","closed_at":"2026-02-03T12:18:19.156608-06:00","close_reason":"Closed"}
{"id":"parquedb-z9kf","title":"Add cache TTL and invalidation for dataCache","description":"dataCache Map has no TTL or invalidation. Data could be stale indefinitely. No versioning strategy. Worker restart loses cache with no warm-up.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:27:35.604998-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:23:54.923716-06:00","closed_at":"2026-02-03T13:23:54.923716-06:00","close_reason":"Closed"}
{"id":"parquedb-zag","title":"[GREEN] Link/unlink implementation","description":"Implement $link and $unlink to pass tests","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-01-30T11:51:37.213952-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T14:06:34.658476-06:00","closed_at":"2026-02-01T14:06:34.658476-06:00","close_reason":"Closed"}
{"id":"parquedb-zcjq","title":"ARCH: Consolidate CLI/Worker merge code paths","description":"RESOLVED: Analysis shows merge code is already well-centralized in /src/sync/ with minimal duplication. CLI orchestrates, sync module implements. Worker is intentionally read-optimized. Minor gaps: 1) Add missing exports (merge-state, parquet-merge) to sync/index.ts, 2) Complete placeholder event loading in merge.ts","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:54:42.402155-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T09:07:59.245141-06:00","closed_at":"2026-02-03T09:07:59.245141-06:00","close_reason":"Analysis shows code already well-centralized - no refactoring needed"}
{"id":"parquedb-zh19","title":"P2: Complete namespace sharding infrastructure","description":"Sharding schema defined but routing logic incomplete. Complete type-based sharding first with automatic shard routing in StorageRouter. See docs/architecture/NAMESPACE_SHARDED_ARCHITECTURE.md.","status":"closed","priority":2,"issue_type":"feature","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T15:10:57.957155-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T16:00:06.063825-06:00","closed_at":"2026-02-03T16:00:06.063825-06:00","close_reason":"Closed"}
{"id":"parquedb-zidx","title":"Unbounded Memory in Metrics Store - No eviction policy for old namespaces","description":"**File:** src/observability/compaction/metrics.ts (lines 26-59)\n\n**Issue:** No eviction policy for old namespaces, only data points per namespace are capped\n\n**Impact:** Memory grows unboundedly in multi-tenant environments\n\n**Fix:** Add LRU eviction or TTL-based cleanup for inactive namespaces","status":"closed","priority":1,"issue_type":"bug","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T14:34:05.718049-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T14:39:00.335744-06:00","closed_at":"2026-02-03T14:39:00.335744-06:00","close_reason":"Closed"}
{"id":"parquedb-zkar","title":"Add graph traversal tests","description":"src/relationships/traverse.ts has limited test coverage for complex graph queries. Add tests for: multi-hop traversal, cycle detection, path finding, breadth-first vs depth-first traversal.","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T17:01:07.565418-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T17:11:57.367181-06:00","closed_at":"2026-02-01T17:11:57.367181-06:00","close_reason":"Closed"}
{"id":"parquedb-zlwa","title":"Performance: core.ts find() - Avoid full entity scan for indexed queries","description":"In src/ParqueDB/core.ts, find() (lines 196-320) always performs a full scan of all entities:\n\n```typescript\nthis.entities.forEach((entity, id) =\u003e {\n  if (id.startsWith(`${namespace}/`)) {\n    // filter matching\n  }\n})\n```\n\nThis has O(n) complexity where n is total entities across ALL namespaces, not just the target namespace.\n\nPotential optimizations:\n1. **Namespace index**: Store entities keyed by namespace -\u003e Map\u003cnamespace, Map\u003cid, Entity\u003e\u003e\n2. **Use IndexManager**: The class has an indexManager but find() doesn't use it\n3. **Bloom filter pre-check**: Use bloom filters to skip non-matching row groups\n\nThe IndexManager exists (line 113, 129) but is never consulted during find() queries. The createIndex/listIndexes APIs exist (lines 2838-2906) but indexes aren't used for query optimization.\n\nConsider:\n```typescript\nasync find(namespace: string, filter?: Filter): Promise\u003c...\u003e {\n  // Check if any index can accelerate this query\n  const applicableIndex = await this.indexManager.findApplicableIndex(namespace, filter)\n  if (applicableIndex) {\n    return this.indexManager.executeIndexedQuery(namespace, filter, applicableIndex)\n  }\n  // Fall back to full scan\n}\n```","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:34:51.276414-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T07:32:27.140437-06:00","closed_at":"2026-02-03T07:32:27.140437-06:00","close_reason":"Closed","labels":["optimization","performance"]}
{"id":"parquedb-zmh6","title":"Set up CI/CD pipeline with GitHub Actions","description":"No CI/CD pipeline detected. Create .github/workflows/ with:\n- test:unit (15 second target)\n- test:integration (30 second target)\n- test:e2e (60 second target)\n- lint and type check\n- Performance regression detection\n- Coverage reporting","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T12:42:26.190832-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:07:24.659465-06:00","closed_at":"2026-02-01T13:07:24.659465-06:00","close_reason":"Completed by parallel agents"}
{"id":"parquedb-zmny","title":"Add tests for src/studio/ (10% coverage)","description":"src/studio/ has only 1 test (xss-prevention.test.ts) for 10 source files. Missing tests for: api.ts, collections.ts, context.ts, database.ts, discovery.ts, metadata.ts, payload-config.ts, server.ts.","status":"closed","priority":1,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T08:38:30.186847-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:47:18.905963-06:00","closed_at":"2026-02-03T08:47:18.905963-06:00","close_reason":"Closed"}
{"id":"parquedb-zpoh","title":"[TEST] Enforce code coverage thresholds","description":"Add code coverage enforcement to CI:\n\n1. Configure vitest/jest coverage reporting\n2. Set minimum thresholds (suggest 80% line, 70% branch)\n3. Add coverage badge to README\n4. Fail CI if coverage drops below threshold\n5. Generate coverage reports as artifacts","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T13:35:56.897188-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T13:38:43.700518-06:00","closed_at":"2026-02-01T13:38:43.700518-06:00","close_reason":"Closed"}
{"id":"parquedb-zqk","title":"Implement Event writer with memory buffer","description":"Create EventWriter class that buffers events in memory before flushing:\n\n- Buffer events in memory\n- Configurable flush triggers (size threshold, time interval, explicit flush)\n- Track min/max timestamps for the buffer\n- Emit batches to flush handlers\n\nFile: src/events/writer.ts","status":"closed","priority":2,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-01T06:37:48.522193-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-01T07:03:58.82561-06:00","closed_at":"2026-02-01T07:03:58.82561-06:00","close_reason":"Implemented EventWriter class with memory buffering, configurable thresholds (size, count, time), flush handlers, statistics tracking, and timed auto-flush. Added 19 passing tests.","dependencies":[{"issue_id":"parquedb-zqk","depends_on_id":"parquedb-ov5","type":"blocks","created_at":"2026-02-01T06:38:06.397637-06:00","created_by":"Nathan Clevenger"}]}
{"id":"parquedb-zrf1","title":"Implement Iceberg metadata commits during compaction","description":"When writing to Iceberg format, must update Iceberg metadata (manifest files, snapshot). Currently marked as TODO at src/workflows/compaction-migration.ts:421. Required for Iceberg compatibility with DuckDB/Spark/Snowflake.","status":"closed","priority":0,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T12:56:04.745431-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T13:11:52.991373-06:00","closed_at":"2026-02-03T13:11:52.991373-06:00","close_reason":"Closed"}
{"id":"parquedb-zs1v","title":"Refactor: core.ts - Improve error messages with context","description":"In src/ParqueDB/core.ts, some error messages lack sufficient context for debugging:\n\n1. Line 117: `throw new Error('Storage backend is required')`\n   - OK but could mention ParqueDBConfig\n\n2. Line 762: `throw new Error(\\`Unsafe path detected: \\\"${key}\\\" contains a prototype pollution attempt\\`)`\n   - Good, includes the key\n\n3. Line 799: `throw new Error(\\`Cannot apply $inc to non-numeric field: ${key}\\`)`\n   - Good, includes field name\n\n4. Line 971: `throw new Error(\\`Relationship '${key}' is not defined in schema for type '${typeName}'\\`)`\n   - Good, includes field and type\n\n5. Line 991: `throw new Error(\\`Target entity '${targetId}' does not exist\\`)`\n   - OK but could include source entity and relationship name\n\nAreas for improvement:\n- Line 2377: `throw new Error(\\`Entity not found: ${entityId}\\`)`\n  - Could specify this is for snapshot creation\n- Line 2761: `throw new Error('Entity did not exist at the target time')`\n  - Should include entityId and targetTime\n- Line 2131: `throw new Error(\\`Event not found: ${id}\\`)`\n  - OK\n\nConsider creating an errors.ts module with factory functions:\n```typescript\nexport const Errors = {\n  entityNotFound: (id: string, operation: string) =\u003e\n    new Error(\\`Entity '${id}' not found during ${operation}\\`),\n  // ...\n}\n```","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:34:20.920836-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T08:00:05.76411-06:00","closed_at":"2026-02-03T08:00:05.76411-06:00","close_reason":"Closed","labels":["developer-experience","error-handling","refactor"]}
{"id":"parquedb-zzj0","title":"Refactor: Worker - Relationship fromType/fromName always empty in ParqueDBDO","description":"ParqueDBDO.toRelationship() returns empty strings for fromType, fromName, toType, and toName:\n\n```typescript\nreturn {\n  fromNs: stored.from_ns as Namespace,\n  fromId: stored.from_id as Id,\n  fromType: '', // Would need to look up from entity\n  fromName: '', // Would need to look up from entity\n  ...\n}\n```\n\nFiles:\n- src/worker/ParqueDBDO.ts (toRelationship method, lines 1377-1396)\n\nThis makes relationship responses less useful for clients who want display names.\n\nOptions:\n1. Add entity lookups to populate these fields\n2. Denormalize name/type into the relationships table (cost: data duplication)\n3. Document that these are intentionally empty and let clients hydrate\n\nImpact: More complete relationship data, better client UX","status":"closed","priority":3,"issue_type":"task","owner":"4130910+nathanclevenger@users.noreply.github.com","created_at":"2026-02-03T06:33:59.945822-06:00","created_by":"Nathan Clevenger","updated_at":"2026-02-03T11:09:47.763467-06:00","closed_at":"2026-02-03T11:09:47.763467-06:00","close_reason":"Closed"}
