#!/usr/bin/env node
/**
 * Load Example Datasets for ParqueDB
 *
 * This script loads and validates example datasets for testing and development.
 * It can use existing data in data-v3/ or generate sample data if needed.
 *
 * Available datasets:
 * - imdb-1m: 1 million titles, 500K people, 2M cast relationships
 * - onet: Occupational data with skills, abilities, knowledge
 * - unspsc: Product/service classification taxonomy
 *
 * Usage:
 *   node scripts/load-examples.mjs                 # Load all datasets
 *   node scripts/load-examples.mjs --dataset imdb  # Load specific dataset
 *   node scripts/load-examples.mjs --generate      # Regenerate data
 *   node scripts/load-examples.mjs --verify        # Verify existing data
 *   node scripts/load-examples.mjs --list          # List available datasets
 *   node scripts/load-examples.mjs --help          # Show help
 */

import { promises as fs } from 'node:fs';
import { join, basename } from 'node:path';
import { parquetRead, parquetMetadata } from 'hyparquet';
import { parquetWriteBuffer } from 'hyparquet-writer';

// =============================================================================
// Configuration
// =============================================================================

const DATA_DIR = './data-v3';

const DATASETS = {
  'imdb-1m': {
    name: 'IMDB 1M',
    description: '1 million movie/TV titles with cast relationships',
    path: 'imdb-1m',
    files: ['titles.parquet', 'people.parquet', 'cast.parquet'],
    expectedCounts: {
      titles: 1000000,
      people: 500000,
      cast: 2000000,
    },
    generator: generateImdbData,
  },
  'imdb': {
    name: 'IMDB Sample',
    description: '100K sample of movie/TV data',
    path: 'imdb',
    files: ['titles.parquet', 'people.parquet', 'cast.parquet'],
    expectedCounts: {
      titles: 100000,
      people: 50000,
      cast: 200000,
    },
    generator: null, // Use imdb-1m generator with lower count
  },
  'onet': {
    name: 'O*NET',
    description: 'Occupational data with skills, abilities, knowledge',
    path: 'onet',
    files: ['occupations.parquet', 'skills.parquet', 'occupation-skills.parquet'],
    expectedCounts: {
      occupations: 100,
      skills: 10,
      'occupation-skills': 1000,
    },
    generator: generateOnetData,
  },
  'onet-full': {
    name: 'O*NET Full',
    description: 'Complete O*NET database with all occupations',
    path: 'onet-full',
    files: ['occupations.parquet', 'skills.parquet', 'abilities.parquet', 'knowledge.parquet'],
    expectedCounts: {
      occupations: 1000,
      skills: 35,
      abilities: 52,
      knowledge: 33,
    },
    generator: null, // Generated by scale-onet.mjs
  },
  'unspsc': {
    name: 'UNSPSC',
    description: 'Product/service classification taxonomy (sample)',
    path: 'unspsc',
    files: ['segments.parquet', 'families.parquet', 'classes.parquet', 'commodities.parquet'],
    expectedCounts: {
      segments: 6,
      families: 4,
      classes: 20,
      commodities: 200,
    },
    generator: generateUnspscData,
  },
  'unspsc-full': {
    name: 'UNSPSC Full',
    description: 'Complete UNSPSC taxonomy',
    path: 'unspsc-full',
    files: ['segments.parquet', 'families.parquet', 'classes.parquet', 'commodities.parquet'],
    expectedCounts: {
      segments: 55,
      families: 400,
      classes: 4000,
      commodities: 70000,
    },
    generator: null, // Generated by scale-unspsc.mjs
  },
};

// =============================================================================
// Utilities
// =============================================================================

function formatBytes(bytes) {
  if (bytes < 1024) return `${bytes} B`;
  if (bytes < 1024 * 1024) return `${(bytes / 1024).toFixed(1)} KB`;
  if (bytes < 1024 * 1024 * 1024) return `${(bytes / (1024 * 1024)).toFixed(1)} MB`;
  return `${(bytes / (1024 * 1024 * 1024)).toFixed(2)} GB`;
}

function formatNumber(n) {
  return n.toLocaleString();
}

function formatDuration(ms) {
  if (ms < 1000) return `${ms}ms`;
  if (ms < 60000) return `${(ms / 1000).toFixed(1)}s`;
  return `${Math.floor(ms / 60000)}m ${Math.floor((ms % 60000) / 1000)}s`;
}

async function writeParquet(path, columnData, rowGroupSize = 10000) {
  const buffer = parquetWriteBuffer({ columnData, rowGroupSize });
  await fs.mkdir(join(DATA_DIR, path.split('/').slice(0, -1).join('/')), { recursive: true });
  await fs.writeFile(join(DATA_DIR, path), Buffer.from(buffer));
  return buffer.byteLength;
}

async function getParquetInfo(filePath) {
  try {
    const buffer = await fs.readFile(filePath);
    const arrayBuffer = buffer.buffer.slice(buffer.byteOffset, buffer.byteOffset + buffer.byteLength);
    const metadata = parquetMetadata(arrayBuffer);

    return {
      exists: true,
      rowCount: Number(metadata.num_rows),
      rowGroups: metadata.row_groups.length,
      columns: metadata.schema.filter(s => s.name !== 'schema').map(s => s.name),
      fileSize: buffer.length,
    };
  } catch (err) {
    return { exists: false, error: err.message };
  }
}

// =============================================================================
// Data Generators
// =============================================================================

// Seeded random for reproducibility
let seed = 42;
function random() {
  seed = (seed * 1103515245 + 12345) & 0x7fffffff;
  return seed / 0x7fffffff;
}

function weightedRandom(items) {
  const r = random();
  let cumulative = 0;
  for (const item of items) {
    cumulative += item.weight;
    if (r < cumulative) {
      return item.name || item.type || item.value;
    }
  }
  return items[items.length - 1].name || items[items.length - 1].type || items[items.length - 1].value;
}

async function generateImdbData(config) {
  console.log(`Generating ${config.name} data...`);
  const startTime = Date.now();

  const outputDir = join(DATA_DIR, config.path);
  await fs.mkdir(outputDir, { recursive: true });

  const titleCount = config.expectedCounts.titles;
  const peopleCount = config.expectedCounts.people;
  const castCount = config.expectedCounts.cast;

  // Title type distribution
  const titleTypes = [
    { type: 'movie', weight: 0.35 },
    { type: 'tvSeries', weight: 0.25 },
    { type: 'tvEpisode', weight: 0.30 },
    { type: 'short', weight: 0.05 },
    { type: 'tvMovie', weight: 0.03 },
    { type: 'video', weight: 0.02 },
  ];

  const genres = ['Drama', 'Comedy', 'Action', 'Thriller', 'Horror', 'Romance', 'Sci-Fi', 'Documentary'];

  // Generate titles
  console.log(`  Generating ${formatNumber(titleCount)} titles...`);
  const titles = [];
  for (let i = 0; i < titleCount; i++) {
    const type = weightedRandom(titleTypes);
    const year = 1920 + Math.floor(random() * 105);
    const rating = 5 + (random() * 5);
    const votes = Math.floor(random() * 1000000) + 100;

    titles.push({
      tconst: `tt${String(i).padStart(7, '0')}`,
      titleType: type,
      primaryTitle: `Title ${i}`,
      startYear: year,
      genres: [genres[i % genres.length], genres[(i + 3) % genres.length]],
      averageRating: Math.round(rating * 10) / 10,
      numVotes: votes,
    });
  }

  // Sort by titleType for row-group statistics
  titles.sort((a, b) => a.titleType.localeCompare(b.titleType));

  await writeParquet(`${config.path}/titles.parquet`, [
    { name: '$id', type: 'STRING', data: titles.map(t => `title:${t.tconst}`) },
    { name: '$index_tconst', type: 'STRING', data: titles.map(t => t.tconst) },
    { name: '$index_titleType', type: 'STRING', data: titles.map(t => t.titleType) },
    { name: '$index_startYear', type: 'INT32', data: titles.map(t => t.startYear) },
    { name: '$index_averageRating', type: 'DOUBLE', data: titles.map(t => t.averageRating) },
    { name: '$index_numVotes', type: 'INT32', data: titles.map(t => t.numVotes) },
    { name: 'name', type: 'STRING', data: titles.map(t => t.primaryTitle) },
    { name: '$data', type: 'STRING', data: titles.map(t => JSON.stringify(t)) },
  ], 5000);

  // Generate people
  console.log(`  Generating ${formatNumber(peopleCount)} people...`);
  const people = [];
  const professions = ['actor', 'actress', 'director', 'writer', 'producer'];
  for (let i = 0; i < peopleCount; i++) {
    people.push({
      nconst: `nm${String(i).padStart(7, '0')}`,
      primaryName: `Person ${i}`,
      birthYear: 1950 + (i % 50),
      primaryProfession: professions[i % professions.length],
    });
  }

  // Sort by nconst
  people.sort((a, b) => a.nconst.localeCompare(b.nconst));

  await writeParquet(`${config.path}/people.parquet`, [
    { name: '$id', type: 'STRING', data: people.map(p => `person:${p.nconst}`) },
    { name: '$index_nconst', type: 'STRING', data: people.map(p => p.nconst) },
    { name: '$index_birthYear', type: 'INT32', data: people.map(p => p.birthYear) },
    { name: '$index_primaryProfession', type: 'STRING', data: people.map(p => p.primaryProfession) },
    { name: 'name', type: 'STRING', data: people.map(p => p.primaryName) },
    { name: '$data', type: 'STRING', data: people.map(p => JSON.stringify(p)) },
  ], 5000);

  // Generate cast
  console.log(`  Generating ${formatNumber(castCount)} cast entries...`);
  const cast = [];
  const categories = ['actor', 'actress', 'director', 'writer', 'producer'];
  for (let i = 0; i < castCount; i++) {
    cast.push({
      tconst: `tt${String(i % titleCount).padStart(7, '0')}`,
      nconst: `nm${String(i % peopleCount).padStart(7, '0')}`,
      category: categories[i % categories.length],
      ordering: (i % 10) + 1,
    });
  }

  // Sort by tconst
  cast.sort((a, b) => a.tconst.localeCompare(b.tconst));

  await writeParquet(`${config.path}/cast.parquet`, [
    { name: '$id', type: 'STRING', data: cast.map((c, i) => `cast:${i}`) },
    { name: '$index_tconst', type: 'STRING', data: cast.map(c => c.tconst) },
    { name: '$index_nconst', type: 'STRING', data: cast.map(c => c.nconst) },
    { name: '$index_category', type: 'STRING', data: cast.map(c => c.category) },
    { name: '$index_ordering', type: 'INT32', data: cast.map(c => c.ordering) },
    { name: '$data', type: 'STRING', data: cast.map(c => JSON.stringify(c)) },
  ], 10000);

  console.log(`  Completed in ${formatDuration(Date.now() - startTime)}`);
}

async function generateOnetData(config) {
  console.log(`Generating ${config.name} data...`);
  const startTime = Date.now();

  const outputDir = join(DATA_DIR, config.path);
  await fs.mkdir(outputDir, { recursive: true });

  // Generate occupations
  console.log(`  Generating occupations...`);
  const occupations = [];
  const sampleOccupations = [
    { socCode: '15-1252.00', title: 'Software Developers', jobZone: 4 },
    { socCode: '15-1253.00', title: 'Software QA Analysts', jobZone: 4 },
    { socCode: '15-1254.00', title: 'Web Developers', jobZone: 3 },
    { socCode: '15-1211.00', title: 'Computer Systems Analysts', jobZone: 4 },
    { socCode: '15-1212.00', title: 'Information Security Analysts', jobZone: 4 },
  ];

  for (let i = 0; i < 100; i++) {
    const base = sampleOccupations[i % sampleOccupations.length];
    occupations.push({
      socCode: `${15 + Math.floor(i / 20)}-${1200 + i}.00`,
      title: `${base.title} ${i > 9 ? i : ''}`.trim(),
      jobZone: base.jobZone,
    });
  }

  occupations.sort((a, b) => a.socCode.localeCompare(b.socCode));

  await writeParquet(`${config.path}/occupations.parquet`, [
    { name: '$id', type: 'STRING', data: occupations.map(o => `occupation:${o.socCode}`) },
    { name: '$index_socCode', type: 'STRING', data: occupations.map(o => o.socCode) },
    { name: '$index_jobZone', type: 'INT32', data: occupations.map(o => o.jobZone) },
    { name: 'name', type: 'STRING', data: occupations.map(o => o.title) },
    { name: '$data', type: 'STRING', data: occupations.map(o => JSON.stringify(o)) },
  ]);

  // Generate skills
  console.log(`  Generating skills...`);
  const skills = [
    { elementId: '2.A.1.a', name: 'Reading Comprehension' },
    { elementId: '2.A.1.b', name: 'Active Listening' },
    { elementId: '2.A.1.c', name: 'Writing' },
    { elementId: '2.A.1.d', name: 'Speaking' },
    { elementId: '2.A.2.a', name: 'Critical Thinking' },
    { elementId: '2.A.2.b', name: 'Active Learning' },
    { elementId: '2.B.1.a', name: 'Complex Problem Solving' },
    { elementId: '2.B.2.i', name: 'Programming' },
    { elementId: '2.B.3.a', name: 'Operations Analysis' },
    { elementId: '2.B.4.a', name: 'Technology Design' },
  ];

  await writeParquet(`${config.path}/skills.parquet`, [
    { name: '$id', type: 'STRING', data: skills.map(s => `skill:${s.elementId}`) },
    { name: '$index_elementId', type: 'STRING', data: skills.map(s => s.elementId) },
    { name: 'name', type: 'STRING', data: skills.map(s => s.name) },
    { name: '$data', type: 'STRING', data: skills.map(s => JSON.stringify(s)) },
  ]);

  // Generate occupation-skills
  console.log(`  Generating occupation-skills relationships...`);
  const occupationSkills = [];
  for (const occ of occupations) {
    for (const skill of skills) {
      occupationSkills.push({
        id: `${occ.socCode}:${skill.elementId}`,
        socCode: occ.socCode,
        elementId: skill.elementId,
        dataValue: 2 + random() * 3,
        scaleId: random() > 0.5 ? 'IM' : 'LV',
      });
    }
  }

  occupationSkills.sort((a, b) => a.socCode.localeCompare(b.socCode));

  await writeParquet(`${config.path}/occupation-skills.parquet`, [
    { name: '$id', type: 'STRING', data: occupationSkills.map(os => `os:${os.id}`) },
    { name: '$index_socCode', type: 'STRING', data: occupationSkills.map(os => os.socCode) },
    { name: '$index_elementId', type: 'STRING', data: occupationSkills.map(os => os.elementId) },
    { name: '$index_dataValue', type: 'DOUBLE', data: occupationSkills.map(os => os.dataValue) },
    { name: '$index_scaleId', type: 'STRING', data: occupationSkills.map(os => os.scaleId) },
    { name: '$data', type: 'STRING', data: occupationSkills.map(os => JSON.stringify(os)) },
  ], 100);

  console.log(`  Completed in ${formatDuration(Date.now() - startTime)}`);
}

async function generateUnspscData(config) {
  console.log(`Generating ${config.name} data...`);
  const startTime = Date.now();

  const outputDir = join(DATA_DIR, config.path);
  await fs.mkdir(outputDir, { recursive: true });

  const segments = [];
  const families = [];
  const classes = [];
  const commodities = [];

  // IT Segment (43)
  segments.push({ code: '43', title: 'Information Technology Broadcasting and Telecommunications' });

  const itFamilies = [
    { code: '4310', title: 'Computer Equipment and Accessories' },
    { code: '4320', title: 'Computer Components' },
    { code: '4321', title: 'Software' },
    { code: '4322', title: 'Networking Equipment' },
  ];

  for (const fam of itFamilies) {
    families.push({ ...fam, segmentCode: '43' });

    for (let i = 0; i < 5; i++) {
      const classCode = fam.code + String(i + 10).padStart(2, '0');
      classes.push({
        code: classCode,
        title: `${fam.title} Class ${i + 1}`,
        familyCode: fam.code,
        segmentCode: '43'
      });

      for (let j = 0; j < 10; j++) {
        commodities.push({
          code: classCode + String(j + 1).padStart(2, '0'),
          title: `${fam.title} Item ${i * 10 + j + 1}`,
          classCode,
          familyCode: fam.code,
          segmentCode: '43'
        });
      }
    }
  }

  // Add more segments
  const otherSegments = [
    { code: '10', title: 'Live Plant and Animal Material' },
    { code: '20', title: 'Mining and Well Drilling Machinery' },
    { code: '30', title: 'Structures and Building and Construction' },
    { code: '40', title: 'Distribution and Conditioning Systems' },
    { code: '50', title: 'Food Beverage and Tobacco Products' },
  ];
  segments.push(...otherSegments);

  // Write segments
  await writeParquet(`${config.path}/segments.parquet`, [
    { name: '$id', type: 'STRING', data: segments.map(s => `segment:${s.code}`) },
    { name: '$index_code', type: 'STRING', data: segments.map(s => s.code) },
    { name: '$index_level', type: 'INT32', data: segments.map(() => 1) },
    { name: 'name', type: 'STRING', data: segments.map(s => s.title) },
    { name: '$data', type: 'STRING', data: segments.map(s => JSON.stringify(s)) },
  ]);

  // Write families
  await writeParquet(`${config.path}/families.parquet`, [
    { name: '$id', type: 'STRING', data: families.map(f => `family:${f.code}`) },
    { name: '$index_code', type: 'STRING', data: families.map(f => f.code) },
    { name: '$index_segmentCode', type: 'STRING', data: families.map(f => f.segmentCode) },
    { name: '$index_level', type: 'INT32', data: families.map(() => 2) },
    { name: 'name', type: 'STRING', data: families.map(f => f.title) },
    { name: '$data', type: 'STRING', data: families.map(f => JSON.stringify(f)) },
  ]);

  // Write classes
  await writeParquet(`${config.path}/classes.parquet`, [
    { name: '$id', type: 'STRING', data: classes.map(c => `class:${c.code}`) },
    { name: '$index_code', type: 'STRING', data: classes.map(c => c.code) },
    { name: '$index_familyCode', type: 'STRING', data: classes.map(c => c.familyCode) },
    { name: '$index_segmentCode', type: 'STRING', data: classes.map(c => c.segmentCode) },
    { name: '$index_level', type: 'INT32', data: classes.map(() => 3) },
    { name: 'name', type: 'STRING', data: classes.map(c => c.title) },
    { name: '$data', type: 'STRING', data: classes.map(c => JSON.stringify(c)) },
  ]);

  // Write commodities
  commodities.sort((a, b) => a.code.localeCompare(b.code));
  await writeParquet(`${config.path}/commodities.parquet`, [
    { name: '$id', type: 'STRING', data: commodities.map(c => `commodity:${c.code}`) },
    { name: '$index_code', type: 'STRING', data: commodities.map(c => c.code) },
    { name: '$index_classCode', type: 'STRING', data: commodities.map(c => c.classCode) },
    { name: '$index_familyCode', type: 'STRING', data: commodities.map(c => c.familyCode) },
    { name: '$index_segmentCode', type: 'STRING', data: commodities.map(c => c.segmentCode) },
    { name: '$index_level', type: 'INT32', data: commodities.map(() => 4) },
    { name: 'name', type: 'STRING', data: commodities.map(c => c.title) },
    { name: '$data', type: 'STRING', data: commodities.map(c => JSON.stringify(c)) },
  ], 50);

  console.log(`  Total: ${segments.length} segments, ${families.length} families, ${classes.length} classes, ${commodities.length} commodities`);
  console.log(`  Completed in ${formatDuration(Date.now() - startTime)}`);
}

// =============================================================================
// Commands
// =============================================================================

async function listDatasets() {
  console.log('\nAvailable Datasets:\n');
  console.log('Name              Description');
  console.log('-'.repeat(70));

  for (const [key, config] of Object.entries(DATASETS)) {
    const status = await checkDatasetStatus(config);
    const statusIcon = status.exists ? '[OK]' : '[--]';
    console.log(`${statusIcon} ${key.padEnd(15)} ${config.description}`);
  }

  console.log('\n[OK] = Data exists, [--] = Not loaded');
}

async function checkDatasetStatus(config) {
  const path = join(DATA_DIR, config.path);
  try {
    const stat = await fs.stat(path);
    if (!stat.isDirectory()) return { exists: false };

    // Check if all files exist
    for (const file of config.files) {
      try {
        await fs.stat(join(path, file));
      } catch {
        return { exists: false, partial: true };
      }
    }

    return { exists: true };
  } catch {
    return { exists: false };
  }
}

async function verifyDataset(datasetKey) {
  const config = DATASETS[datasetKey];
  if (!config) {
    console.error(`Unknown dataset: ${datasetKey}`);
    return false;
  }

  console.log(`\nVerifying ${config.name}...`);
  console.log('='.repeat(60));

  const datasetPath = join(DATA_DIR, config.path);
  let allValid = true;
  let totalSize = 0;
  let totalRows = 0;

  for (const file of config.files) {
    const filePath = join(datasetPath, file);
    const info = await getParquetInfo(filePath);

    if (!info.exists) {
      console.log(`  [MISSING] ${file}`);
      allValid = false;
      continue;
    }

    const baseName = basename(file, '.parquet');
    const expectedCount = config.expectedCounts[baseName];
    const countMatch = !expectedCount || Math.abs(info.rowCount - expectedCount) < expectedCount * 0.1;

    totalSize += info.fileSize;
    totalRows += info.rowCount;

    console.log(`  [${countMatch ? 'OK' : 'WARN'}] ${file}`);
    console.log(`        Rows: ${formatNumber(info.rowCount)}${expectedCount ? ` (expected ~${formatNumber(expectedCount)})` : ''}`);
    console.log(`        Size: ${formatBytes(info.fileSize)}`);
    console.log(`        Row Groups: ${info.rowGroups}`);
    console.log(`        Columns: ${info.columns.join(', ')}`);
  }

  console.log('='.repeat(60));
  console.log(`Total: ${formatNumber(totalRows)} rows, ${formatBytes(totalSize)}`);
  console.log(`Status: ${allValid ? 'VALID' : 'INCOMPLETE'}`);

  return allValid;
}

async function loadDataset(datasetKey, options = {}) {
  const config = DATASETS[datasetKey];
  if (!config) {
    console.error(`Unknown dataset: ${datasetKey}`);
    return false;
  }

  // Check if data already exists
  const status = await checkDatasetStatus(config);
  if (status.exists && !options.force) {
    console.log(`Dataset ${config.name} already exists. Use --force to regenerate.`);
    return true;
  }

  // Check if generator exists
  if (!config.generator) {
    console.log(`Dataset ${config.name} has no generator.`);
    if (datasetKey.includes('full')) {
      console.log(`Use the corresponding scale script: node scripts/scale-${datasetKey.split('-')[0]}.mjs`);
    }
    return false;
  }

  // Generate the data
  console.log(`\nLoading ${config.name}...`);
  console.log('='.repeat(60));

  try {
    await config.generator(config);
    console.log('='.repeat(60));
    console.log(`Dataset ${config.name} loaded successfully.`);
    return true;
  } catch (err) {
    console.error(`Error loading ${config.name}:`, err.message);
    return false;
  }
}

async function loadAllDatasets(options = {}) {
  console.log('\nLoading All Example Datasets\n');

  const results = [];

  // Only load the non-full datasets by default
  const datasetsToLoad = Object.keys(DATASETS).filter(k => !k.includes('full'));

  for (const key of datasetsToLoad) {
    const config = DATASETS[key];
    const status = await checkDatasetStatus(config);

    if (status.exists && !options.force) {
      console.log(`[SKIP] ${config.name} (already exists)`);
      results.push({ name: key, status: 'skipped' });
      continue;
    }

    if (!config.generator) {
      console.log(`[SKIP] ${config.name} (no generator)`);
      results.push({ name: key, status: 'skipped' });
      continue;
    }

    console.log(`[LOAD] ${config.name}`);
    const success = await loadDataset(key, options);
    results.push({ name: key, status: success ? 'loaded' : 'failed' });
  }

  console.log('\n' + '='.repeat(60));
  console.log('Summary:');
  for (const result of results) {
    console.log(`  ${result.name.padEnd(15)} ${result.status.toUpperCase()}`);
  }
}

async function showHelp() {
  console.log(`
Load Example Datasets for ParqueDB

Usage:
  node scripts/load-examples.mjs [command] [options]

Commands:
  (default)         Load all example datasets
  --dataset <name>  Load a specific dataset
  --verify          Verify existing datasets
  --list            List available datasets
  --help            Show this help

Options:
  --force           Force regeneration of existing data
  --generate        Alias for default (load all)

Datasets:
  imdb-1m           1 million IMDB titles with relationships
  imdb              100K sample IMDB data
  onet              O*NET occupational data (sample)
  onet-full         Complete O*NET database
  unspsc            UNSPSC taxonomy (sample)
  unspsc-full       Complete UNSPSC taxonomy

Examples:
  # Load all sample datasets
  node scripts/load-examples.mjs

  # Load specific dataset
  node scripts/load-examples.mjs --dataset imdb

  # Verify existing data
  node scripts/load-examples.mjs --verify

  # Force regeneration
  node scripts/load-examples.mjs --dataset imdb --force

Data is stored in: ${DATA_DIR}/
`);
}

// =============================================================================
// Main
// =============================================================================

async function main() {
  const args = process.argv.slice(2);

  // Parse arguments
  let command = 'load';
  let dataset = null;
  let force = false;

  for (let i = 0; i < args.length; i++) {
    const arg = args[i];
    switch (arg) {
      case '--help':
      case '-h':
        command = 'help';
        break;
      case '--list':
      case '-l':
        command = 'list';
        break;
      case '--verify':
      case '-v':
        command = 'verify';
        break;
      case '--generate':
      case '-g':
        command = 'load';
        break;
      case '--dataset':
      case '-d':
        dataset = args[++i];
        break;
      case '--force':
      case '-f':
        force = true;
        break;
      default:
        if (!arg.startsWith('-')) {
          dataset = arg;
        }
    }
  }

  console.log('='.repeat(60));
  console.log('ParqueDB Example Dataset Loader');
  console.log('='.repeat(60));

  switch (command) {
    case 'help':
      await showHelp();
      break;
    case 'list':
      await listDatasets();
      break;
    case 'verify':
      if (dataset) {
        await verifyDataset(dataset);
      } else {
        for (const key of Object.keys(DATASETS)) {
          await verifyDataset(key);
          console.log('');
        }
      }
      break;
    case 'load':
    default:
      if (dataset) {
        await loadDataset(dataset, { force });
      } else {
        await loadAllDatasets({ force });
      }
      break;
  }
}

main().catch(console.error);
