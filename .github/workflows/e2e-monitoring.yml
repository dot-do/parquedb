name: E2E Production Monitoring

on:
  # Health checks every 15 minutes
  schedule:
    - cron: '*/15 * * * *'  # Every 15 minutes (health checks)
    - cron: '5 * * * *'     # Hourly at :05 (full benchmarks, offset to avoid collision)
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      url:
        description: 'Worker URL to monitor'
        required: false
        default: 'https://parquedb.workers.do'
      run_full_benchmark:
        description: 'Run full benchmark suite (not just health check)'
        type: boolean
        required: false
        default: false

env:
  WORKER_URL: ${{ github.event.inputs.url || 'https://parquedb.workers.do' }}

jobs:
  health-check:
    name: Health Check
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      health_status: ${{ steps.health.outputs.status }}

    steps:
      - name: Check production health
        id: health
        run: |
          echo "Checking health at ${{ env.WORKER_URL }}/health?deep=true"

          # Make health check request
          HTTP_CODE=$(curl -s -o response.json -w "%{http_code}" \
            "${{ env.WORKER_URL }}/health?deep=true" \
            --max-time 30)

          echo "HTTP Status: $HTTP_CODE"
          cat response.json

          # Parse status from response
          STATUS=$(cat response.json | jq -r '.api.status // "unknown"')
          echo "Health Status: $STATUS"
          echo "status=$STATUS" >> $GITHUB_OUTPUT
          echo "http_code=$HTTP_CODE" >> $GITHUB_OUTPUT

          # Check for failures
          if [ "$HTTP_CODE" != "200" ]; then
            echo "::error::Health check returned HTTP $HTTP_CODE"
            exit 1
          fi

          if [ "$STATUS" == "unhealthy" ]; then
            echo "::error::Worker health is unhealthy"
            exit 1
          fi

          if [ "$STATUS" == "degraded" ]; then
            echo "::warning::Worker health is degraded"
          fi

          echo "Health check passed: $STATUS"

      - name: Alert on failure (Slack)
        if: failure()
        continue-on-error: true
        run: |
          if [ -n "${{ secrets.E2E_SLACK_WEBHOOK_URL }}" ]; then
            curl -X POST -H 'Content-type: application/json' \
              --data '{
                "text": ":broken_heart: Production Health Check Failed",
                "blocks": [
                  {
                    "type": "section",
                    "text": {
                      "type": "mrkdwn",
                      "text": ":broken_heart: *Production Health Check Failed*\n\n*Worker URL:* ${{ env.WORKER_URL }}\n*Time:* '"$(date -u +"%Y-%m-%d %H:%M:%S UTC")"'\n\n<${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Run>"
                    }
                  }
                ]
              }' \
              ${{ secrets.E2E_SLACK_WEBHOOK_URL }}
          fi

      - name: Alert on failure (PagerDuty)
        if: failure()
        continue-on-error: true
        run: |
          if [ -n "${{ secrets.E2E_PAGERDUTY_ROUTING_KEY }}" ]; then
            curl -X POST https://events.pagerduty.com/v2/enqueue \
              -H 'Content-Type: application/json' \
              -d '{
                "routing_key": "${{ secrets.E2E_PAGERDUTY_ROUTING_KEY }}",
                "event_action": "trigger",
                "dedup_key": "parquedb-e2e:production:health_check_failed",
                "payload": {
                  "summary": "[ParqueDB] Production health check failed",
                  "severity": "critical",
                  "source": "${{ env.WORKER_URL }}",
                  "component": "parquedb-e2e",
                  "group": "e2e-monitoring",
                  "class": "health_check_failed",
                  "custom_details": {
                    "worker_url": "${{ env.WORKER_URL }}",
                    "run_id": "${{ github.run_id }}",
                    "run_url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                  }
                }
              }'
          fi

  # Full benchmark runs hourly (on the hour)
  full-benchmark:
    name: Full Benchmark (Hourly)
    runs-on: ubuntu-latest
    timeout-minutes: 30
    # Only run on hourly schedule (5 * * * *) OR when manually triggered with run_full_benchmark=true
    if: >-
      github.event.inputs.run_full_benchmark == 'true' ||
      github.event.schedule == '5 * * * *'
    needs: health-check

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20.x'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Install Wrangler
        run: npm install -g wrangler

      - name: Download baseline from R2
        id: baseline
        continue-on-error: true
        run: |
          wrangler r2 object get parquedb-benchmarks/benchmarks/baselines/production/latest.json \
            --file=baseline.json 2>/dev/null || echo "No baseline found"
          if [ -f baseline.json ]; then
            echo "baseline_exists=true" >> $GITHUB_OUTPUT
          else
            echo "baseline_exists=false" >> $GITHUB_OUTPUT
          fi
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}

      - name: Run E2E benchmark
        id: benchmark
        run: |
          BASELINE_FLAG=""
          if [ "${{ steps.baseline.outputs.baseline_exists }}" == "true" ]; then
            BASELINE_FLAG="--baseline-url=./baseline.json"
          fi

          npx tsx tests/e2e/benchmarks/runner.ts \
            --url="${{ env.WORKER_URL }}" \
            --iterations=5 \
            --output=json \
            --save-baseline=./results.json \
            --environment=production \
            --fail-on-regression \
            $BASELINE_FLAG \
            > benchmark-output.json || true

          # Check exit code
          if [ $? -eq 0 ]; then
            echo "benchmark_status=passed" >> $GITHUB_OUTPUT
          else
            echo "benchmark_status=failed" >> $GITHUB_OUTPUT
          fi
        env:
          CI: true
          GITHUB_SHA: ${{ github.sha }}
          GITHUB_REF_NAME: ${{ github.ref_name }}

      - name: Upload results to R2
        if: always()
        continue-on-error: true
        run: |
          if [ -f results.json ]; then
            DATE_PATH=$(date -u +"%Y/%m/%d")
            RUN_ID="monitoring-$(date -u +"%s")-${{ github.run_id }}"
            RESULT_PATH="benchmarks/results/production/${DATE_PATH}/${RUN_ID}.json"

            wrangler r2 object put parquedb-benchmarks/${RESULT_PATH} \
              --file=results.json \
              --content-type="application/json"
          fi
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: monitoring-results-${{ github.run_id }}
          path: |
            results.json
            benchmark-output.json
          retention-days: 7

      - name: Display summary
        if: always()
        run: |
          echo "## E2E Monitoring Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Time:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")" >> $GITHUB_STEP_SUMMARY
          echo "**Worker URL:** ${{ env.WORKER_URL }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f results.json ]; then
            node -e "
              const fs = require('fs');
              const results = JSON.parse(fs.readFileSync('results.json', 'utf8'));
              const r = results.results || results;

              if (r.summary) {
                console.log('### Summary');
                console.log('| Metric | Value |');
                console.log('|--------|-------|');
                console.log('| Total Tests | ' + r.summary.totalTests + ' |');
                console.log('| Passed | ' + r.summary.passedTests + ' |');
                console.log('| Failed | ' + r.summary.failedTests + ' |');
                console.log('| Avg Latency | ' + r.summary.avgLatencyMs?.toFixed(1) + 'ms |');
                console.log('');
              }

              if (r.regression) {
                const status = r.regression.hasRegression ? ':warning: Regression' : ':white_check_mark: OK';
                console.log('### Regression: ' + status);
                console.log('Severity: ' + r.regression.severity);
              }
            " >> $GITHUB_STEP_SUMMARY
          fi

      - name: Alert on regression (Slack)
        if: steps.benchmark.outputs.benchmark_status == 'failed'
        continue-on-error: true
        run: |
          if [ -n "${{ secrets.E2E_SLACK_WEBHOOK_URL }}" ]; then
            curl -X POST -H 'Content-type: application/json' \
              --data '{
                "text": ":chart_with_downwards_trend: E2E Monitoring: Regression Detected",
                "blocks": [
                  {
                    "type": "section",
                    "text": {
                      "type": "mrkdwn",
                      "text": ":chart_with_downwards_trend: *E2E Monitoring: Regression Detected*\n\n*Worker URL:* ${{ env.WORKER_URL }}\n*Time:* '"$(date -u +"%Y-%m-%d %H:%M:%S UTC")"'\n\n<${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}|View Run>"
                    }
                  }
                ]
              }' \
              ${{ secrets.E2E_SLACK_WEBHOOK_URL }}
          fi

  # Recovery alert when health check recovers after failure
  recovery-alert:
    name: Recovery Alert
    runs-on: ubuntu-latest
    needs: health-check
    if: needs.health-check.outputs.health_status == 'healthy'
    # This job only runs if the previous run failed

    steps:
      - name: Check if recovering from failure
        id: check-recovery
        run: |
          # Use GitHub API to check if the last run of this workflow failed
          # This is a simplified check - in production you'd use a more robust mechanism
          echo "Health check passed. Checking for recovery scenario..."

          # For now, we just output that health is OK
          # A more sophisticated implementation would track state in R2 or a database
          echo "recovery_needed=false" >> $GITHUB_OUTPUT

      - name: Send recovery alert (Slack)
        if: steps.check-recovery.outputs.recovery_needed == 'true'
        continue-on-error: true
        run: |
          if [ -n "${{ secrets.E2E_SLACK_WEBHOOK_URL }}" ]; then
            curl -X POST -H 'Content-type: application/json' \
              --data '{
                "text": ":white_check_mark: Production Health Recovered",
                "blocks": [
                  {
                    "type": "section",
                    "text": {
                      "type": "mrkdwn",
                      "text": ":white_check_mark: *Production Health Recovered*\n\n*Worker URL:* ${{ env.WORKER_URL }}\n*Time:* '"$(date -u +"%Y-%m-%d %H:%M:%S UTC")"'"
                    }
                  }
                ]
              }' \
              ${{ secrets.E2E_SLACK_WEBHOOK_URL }}
          fi

      - name: Resolve PagerDuty incident
        if: steps.check-recovery.outputs.recovery_needed == 'true'
        continue-on-error: true
        run: |
          if [ -n "${{ secrets.E2E_PAGERDUTY_ROUTING_KEY }}" ]; then
            curl -X POST https://events.pagerduty.com/v2/enqueue \
              -H 'Content-Type: application/json' \
              -d '{
                "routing_key": "${{ secrets.E2E_PAGERDUTY_ROUTING_KEY }}",
                "event_action": "resolve",
                "dedup_key": "parquedb-e2e:production:health_check_failed"
              }'
          fi
